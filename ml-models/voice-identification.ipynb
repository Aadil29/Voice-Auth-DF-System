{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bc3dd0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485c06",
   "metadata": {},
   "source": [
    "Sample of Vox1celeb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset split\n",
    "base_path = os.path.join(os.getcwd(), 'datasets', 'vox1_dev')\n",
    "print(base_path)\n",
    "\n",
    "#Each folder contains a different speaker\n",
    "\n",
    "speaker_count = sum(os.path.isdir(os.path.join(base_path, entry)) for entry in os.listdir(base_path))\n",
    "print(\"Number of original Speakers : \",speaker_count)\n",
    "#Select a random selection of speakers, as inital dataset of 1211 speakers is roughly 30GB. will take a sample of 150 speakers(12.3%), aroudn 3.7GB\n",
    "\n",
    "\n",
    "# Destination path for subset\n",
    "train_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Set seed and select random 150 speakers\n",
    "random.seed(42)\n",
    "all_speakers = [entry for entry in os.listdir(base_path)\n",
    "                if os.path.isdir(os.path.join(base_path, entry))]\n",
    "selected_speakers = random.sample(all_speakers, 150)\n",
    "\n",
    "# Copy selected speakers and their full contents to the subset folder\n",
    "for speaker in selected_speakers:\n",
    "    src_speaker_path = os.path.join(base_path, speaker)\n",
    "    dest_speaker_path = os.path.join(train_path, speaker)\n",
    "    shutil.copytree(src_speaker_path, dest_speaker_path)\n",
    "\n",
    "#print(f\"Subset dataset created at: {train_path}\")\n",
    "\n",
    "\n",
    "# Print the selected speaker folders\n",
    "count = 0\n",
    "#print(\"Selected speakers:\")\n",
    "for speaker in selected_speakers:\n",
    "    count +=1\n",
    "    #print(speaker)\n",
    "\n",
    "print(\"Number of train Speakers : \",count)\n",
    "\n",
    "\n",
    "\n",
    "test_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_test')\n",
    "\n",
    "test_speaker_count = sum(os.path.isdir(os.path.join(test_path, entry)) for entry in os.listdir(test_path))\n",
    "print(\"Number of test Speakers : \",test_speaker_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da789",
   "metadata": {},
   "source": [
    "Splitting that sample into train and val (already a pre split test folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "\n",
    "train_folder = os.path.join(source_path, 'train_data')\n",
    "val_folder = os.path.join(source_path, 'val_data')\n",
    "\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "speaker_folders = [folder for folder in os.listdir(source_path)\n",
    "                   if os.path.isdir(os.path.join(source_path, folder)) and folder not in ['train_data', 'val_data']]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(speaker_folders)\n",
    "split_index = int(0.8 * len(speaker_folders))\n",
    "train_speakers = speaker_folders[:split_index]\n",
    "val_speakers = speaker_folders[split_index:]\n",
    "\n",
    "for speaker in train_speakers:\n",
    "    src = os.path.join(source_path, speaker)\n",
    "    dst = os.path.join(train_folder, speaker)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "for speaker in val_speakers:\n",
    "    src = os.path.join(source_path, speaker)\n",
    "    dst = os.path.join(val_folder, speaker)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "print(f\"{len(train_speakers)} speakers moved to train_data, {len(val_speakers)} to val_data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078666c",
   "metadata": {},
   "source": [
    "Preprocessing and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06780e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    \n",
    "    # Trim leading/trailing silence\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Apply noise reduction / dereverberation\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    # Apply pre-emphasis filter\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Normalisation method\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Duration control: pad or truncate\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "def preprocess_folder(input_folder, output_folder, sr=22050, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    \n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    wav_files = list(input_folder.rglob(\"*.wav\"))\n",
    "    \n",
    "    print(f\"Found {len(wav_files)} audio files in {input_folder}\")\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_file, sr=sr)\n",
    "            y = preprocess_audio(\n",
    "                y, sr, \n",
    "                target_duration=target_duration,\n",
    "                apply_preemphasis=apply_preemphasis,\n",
    "                apply_reduction=apply_reduction,\n",
    "                coef=coef,\n",
    "                normalise=normalise\n",
    "            )\n",
    "\n",
    "            # Save processed file to the same structure\n",
    "            relative_path = wav_file.relative_to(input_folder)\n",
    "            out_path = output_folder / relative_path\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            sf.write(out_path, y, sr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {wav_file} â†’ {e}\")\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.feature_dirs = {\n",
    "            \"mel\": \"mel_spectrogram\",\n",
    "            \"mfcc\": \"mfcc\",\n",
    "            \"chroma\": \"chroma\",\n",
    "            \"tonnetz\": \"tonnetz\",\n",
    "            \"contrast\": \"spectral_contrast\"\n",
    "        }\n",
    "\n",
    "        # Collect relative paths from one feature folder (\n",
    "        mel_folder = os.path.join(base_path, self.feature_dirs['mel'])\n",
    "        rel_paths = glob(os.path.join(mel_folder, \"*\", \"*\", \"*.npy\"))\n",
    "        self.valid_paths = [\n",
    "            os.path.relpath(path, mel_folder)\n",
    "            for path in rel_paths\n",
    "        ]\n",
    "\n",
    "        # Filter to only keep files that exist in all 5 folders\n",
    "        self.valid_paths = [\n",
    "            rel_path for rel_path in self.valid_paths\n",
    "            if all(\n",
    "                os.path.exists(os.path.join(base_path, folder, rel_path))\n",
    "                for folder in self.feature_dirs.values()\n",
    "            )\n",
    "        ]\n",
    "\n",
    "      \n",
    "        speaker_ids = sorted(set(p.split(os.sep)[0] for p in self.valid_paths))\n",
    "        self.label_map = {speaker: idx for idx, speaker in enumerate(speaker_ids)}\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.valid_paths)} aligned samples\")\n",
    "        print(f\"[INFO] Number of speaker classes: {len(self.label_map)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.valid_paths[idx]\n",
    "        speaker_id = rel_path.split(os.sep)[0]\n",
    "        label = self.label_map[speaker_id]\n",
    "\n",
    "        def load(feature):\n",
    "            path = os.path.join(self.base_path, self.feature_dirs[feature], rel_path)\n",
    "            return np.load(path)\n",
    "\n",
    "        mfcc = torch.tensor(load(\"mfcc\"), dtype=torch.float32).mean(dim=1)     \n",
    "        chroma = torch.tensor(load(\"chroma\"), dtype=torch.float32).mean(dim=1)  \n",
    "        tonnetz = torch.tensor(load(\"tonnetz\"), dtype=torch.float32).mean(dim=1) \n",
    "        contrast = torch.tensor(load(\"contrast\"), dtype=torch.float32).mean(dim=1)  \n",
    "        mel = torch.tensor(load(\"mel\"), dtype=torch.float32).unsqueeze(0)  \n",
    "\n",
    "\n",
    "        return mel, mfcc, chroma, tonnetz, contrast, torch.tensor(label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df4817",
   "metadata": {},
   "source": [
    "Cleanier version of pre and featrie exztatoin/ saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aa277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    \n",
    "    # Trim leading/trailing silence\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Apply noise reduction / dereverberation\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    # Apply pre-emphasis filter\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Normalisation method\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Duration control: pad or truncate\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "#Run the feature extraction\n",
    "\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n",
    "\n",
    "\n",
    "#same coide but this tiem test, as its in a different location\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"test\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "#Run the feature extraction\n",
    "\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503851eb",
   "metadata": {},
   "source": [
    "Model Architecture (CNN+LSTM and DenseNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ConvolutionLSTMBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.AvgPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((32, 8))  \n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=8 * 256, hidden_size=512,\n",
    "                            num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(x.size(0), 32, -1)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        return out[:, -1, :]  \n",
    "\n",
    "\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class AudioFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mel_branch = ConvolutionLSTMBlock()\n",
    "        self.mfcc_branch = DenseNeuralNetwork(input_dim=40)\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=12)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=6)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=7)\n",
    "\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(1024 + 4 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        self.embedding_out = nn.Linear(256, 128)  \n",
    "\n",
    "\n",
    "    def forward(self, mel, mfcc, chroma, tonnetz, contrast):\n",
    "        mel_out = self.mel_branch(mel)\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(chroma)\n",
    "        tonnetz_out = self.tonnetz_branch(tonnetz)\n",
    "        contrast_out = self.contrast_branch(contrast)\n",
    "\n",
    "        combined = torch.cat([mel_out, mfcc_out, chroma_out, tonnetz_out, contrast_out], dim=1)\n",
    "        fusion = self.fc_fusion(combined)\n",
    "        embedding = self.embedding_out(fusion)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c44a1",
   "metadata": {},
   "source": [
    "Tuned Model Architecture\n",
    "To test replace the code aboev with this version:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e09246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TUNED MODEL ARCHITECTURE\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ConvolutionLSTMBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((32, 8))  # Fixed output shape\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=8 * 256,  # 8 = width after pooling, 256 = channels\n",
    "            hidden_size=256,     # reduced hidden size for generalisation\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(x.size(0), 32, -1)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        return out[:, -1, :]  # final time step output\n",
    "\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)  # no activation here\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mel_branch = ConvolutionLSTMBlock()\n",
    "        self.mfcc_branch = DenseNeuralNetwork(input_dim=40)\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=12)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=6)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=7)\n",
    "\n",
    "        # Mel output: 512 (from 2x 256 LSTM), others: 4 * 128\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 4 * 128, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.embedding_out = nn.Linear(256, 128)  # Final speaker embedding\n",
    "\n",
    "    def forward(self, mel, mfcc, chroma, tonnetz, contrast):\n",
    "        mel_out = self.mel_branch(mel)\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(chroma)\n",
    "        tonnetz_out = self.tonnetz_branch(tonnetz)\n",
    "        contrast_out = self.contrast_branch(contrast)\n",
    "\n",
    "        combined = torch.cat([mel_out, mfcc_out, chroma_out, tonnetz_out, contrast_out], dim=1)\n",
    "        fusion = self.fc_fusion(combined)\n",
    "        embedding = self.embedding_out(fusion)  # No activation here\n",
    "        return embedding\n",
    "\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b159432",
   "metadata": {},
   "source": [
    "Triplet loss dataset creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TripletAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.label_to_indices = self._build_label_index()\n",
    "\n",
    "\n",
    "    def _build_label_index(self):\n",
    "        label_to_indices = {}\n",
    "        print(\"[INFO] Building label index...\")\n",
    "        for idx in tqdm(range(len(self.base_dataset)), desc=\"Indexing labels\"):\n",
    "            _, _, _, _, _, label = self.base_dataset[idx]\n",
    "            label = label.item()\n",
    "            if label not in label_to_indices:\n",
    "                label_to_indices[label] = []\n",
    "            label_to_indices[label].append(idx)\n",
    "        return label_to_indices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, anchor_idx):\n",
    "        anchor = self.base_dataset[anchor_idx]\n",
    "        anchor_label = anchor[-1].item()\n",
    "\n",
    "        positive_idx = anchor_idx\n",
    "        while positive_idx == anchor_idx:\n",
    "            positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive = self.base_dataset[positive_idx]\n",
    "\n",
    "        negative_label = random.choice([label for label in self.label_to_indices.keys() if label != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative = self.base_dataset[negative_idx]\n",
    "\n",
    "        return (\n",
    "            (anchor[:-1], positive[:-1], negative[:-1])\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef1236",
   "metadata": {},
   "source": [
    "Training loop (using the triplet loss as the learning loss function )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_triplet(model, triplet_dataset, epochs=20, batch_size=32, lr=1e-4, device='cuda', save_csv_path=None):\n",
    "    model.to(device)\n",
    "    dataloader = DataLoader(triplet_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            anchor, positive, negative = batch\n",
    "\n",
    "            def move_to_device(x):\n",
    "                return [t.to(device) for t in x]\n",
    "\n",
    "            anchor = move_to_device(anchor)\n",
    "            positive = move_to_device(positive)\n",
    "            negative = move_to_device(negative)\n",
    "\n",
    "            emb_anchor = model(*anchor)\n",
    "            emb_positive = model(*positive)\n",
    "            emb_negative = model(*negative)\n",
    "\n",
    "            loss = criterion(emb_anchor, emb_positive, emb_negative)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']:.6f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"voice_model.pth\")\n",
    "    print(\"\\nModel saved to 'model.pth'\")\n",
    "\n",
    "    # Plotting loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs+1), epoch_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Triplet Loss\")\n",
    "    plt.title(\"Triplet Loss Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df = pd.DataFrame({\n",
    "            'epoch': list(range(1, epochs + 1)),\n",
    "            'avg_loss': epoch_losses\n",
    "        })\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "        print(f\"Loss data saved to: {save_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc6a03",
   "metadata": {},
   "source": [
    "Running Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917424e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = AudioFeatureDataset(\"datasets/vox1_subset_150/val_features\")\n",
    "triplet_dataset = TripletAudioDataset(base_dataset)\n",
    "\n",
    "model = AudioFusionModel()  \n",
    "train_triplet(model, triplet_dataset, epochs=100, batch_size=32, lr=0.001, device='cuda')    # Save model\n",
    "\n",
    "torch.save(model.state_dict(), \"voice_model.pth\")\n",
    "print(\"\\nModel saved to 'val_model.pth'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
