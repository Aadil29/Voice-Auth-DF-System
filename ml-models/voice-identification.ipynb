{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9078666c",
   "metadata": {},
   "source": [
    "Data spillting, pre processign and feartire extraciotgn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06780e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    \n",
    "    # Trim leading/trailing silence\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Apply noise reduction / dereverberation\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    # Apply pre-emphasis filter\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Normalisation method\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Duration control: pad or truncate\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "def preprocess_folder(input_folder, output_folder, sr=22050, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    \n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    wav_files = list(input_folder.rglob(\"*.wav\"))\n",
    "    \n",
    "    print(f\"Found {len(wav_files)} audio files in {input_folder}\")\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_file, sr=sr)\n",
    "            y = preprocess_audio(\n",
    "                y, sr, \n",
    "                target_duration=target_duration,\n",
    "                apply_preemphasis=apply_preemphasis,\n",
    "                apply_reduction=apply_reduction,\n",
    "                coef=coef,\n",
    "                normalise=normalise\n",
    "            )\n",
    "\n",
    "            # Save processed file to the same structure\n",
    "            relative_path = wav_file.relative_to(input_folder)\n",
    "            out_path = output_folder / relative_path\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            sf.write(out_path, y, sr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {wav_file} â†’ {e}\")\n",
    "\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.feature_dirs = {\n",
    "            \"mel\": \"mel_spectrogram\",\n",
    "            \"mfcc\": \"mfcc\",\n",
    "            \"chroma\": \"chroma\",\n",
    "            \"tonnetz\": \"tonnetz\",\n",
    "            \"contrast\": \"spectral_contrast\"\n",
    "        }\n",
    "\n",
    "        # Collect relative paths from one feature folder (\n",
    "        mel_folder = os.path.join(base_path, self.feature_dirs['mel'])\n",
    "        rel_paths = glob(os.path.join(mel_folder, \"*\", \"*\", \"*.npy\"))\n",
    "        self.valid_paths = [\n",
    "            os.path.relpath(path, mel_folder)\n",
    "            for path in rel_paths\n",
    "        ]\n",
    "\n",
    "        # Filter to only keep files that exist in all 5 folders\n",
    "        self.valid_paths = [\n",
    "            rel_path for rel_path in self.valid_paths\n",
    "            if all(\n",
    "                os.path.exists(os.path.join(base_path, folder, rel_path))\n",
    "                for folder in self.feature_dirs.values()\n",
    "            )\n",
    "        ]\n",
    "\n",
    "      \n",
    "        speaker_ids = sorted(set(p.split(os.sep)[0] for p in self.valid_paths))\n",
    "        self.label_map = {speaker: idx for idx, speaker in enumerate(speaker_ids)}\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.valid_paths)} aligned samples\")\n",
    "        print(f\"[INFO] Number of speaker classes: {len(self.label_map)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.valid_paths[idx]\n",
    "        speaker_id = rel_path.split(os.sep)[0]\n",
    "        label = self.label_map[speaker_id]\n",
    "\n",
    "        def load(feature):\n",
    "            path = os.path.join(self.base_path, self.feature_dirs[feature], rel_path)\n",
    "            return np.load(path)\n",
    "\n",
    "        mfcc = torch.tensor(load(\"mfcc\"), dtype=torch.float32).mean(dim=1)     \n",
    "        chroma = torch.tensor(load(\"chroma\"), dtype=torch.float32).mean(dim=1)  \n",
    "        tonnetz = torch.tensor(load(\"tonnetz\"), dtype=torch.float32).mean(dim=1) \n",
    "        contrast = torch.tensor(load(\"contrast\"), dtype=torch.float32).mean(dim=1)  \n",
    "        mel = torch.tensor(load(\"mel\"), dtype=torch.float32).unsqueeze(0)  \n",
    "\n",
    "\n",
    "        return mel, mfcc, chroma, tonnetz, contrast, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "feature_root = \"datasets/vox1_subset_150/features_aligned_ready\"\n",
    "dataset = AudioFeatureDataset(feature_root)\n",
    "\n",
    "\n",
    "\n",
    "def check_feature_shapes(dataset, idx=0):\n",
    "    print(f\"\\Checking sample index: {idx}\\n{'='*50}\")\n",
    "    \n",
    "    rel_path = dataset.valid_paths[idx]\n",
    "    print(f\"Sample path: {rel_path}\")\n",
    "\n",
    "    for feature in dataset.feature_dirs.keys():\n",
    "      \n",
    "        path = os.path.join(dataset.base_path, dataset.feature_dirs[feature], rel_path)\n",
    "        arr = np.load(path)\n",
    "\n",
    "        print(f\"\\n{feature.upper()}\")\n",
    "        print(f\"Raw .npy shape: {arr.shape}\")\n",
    "\n",
    "       \n",
    "        tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "        if feature == \"mel\":\n",
    "            model_input = tensor.unsqueeze(0)  \n",
    "            print(f\"Model input shape (after unsqueeze): {model_input.shape}\")\n",
    "        else:\n",
    "            try:\n",
    "                reduced = tensor.mean(dim=1)\n",
    "                print(f\"Model input shape (after mean over time): {reduced.shape}\")\n",
    "            except:\n",
    "                print(\" Shape error in mean(dim=1). Try mean(dim=0)?\")\n",
    "\n",
    "    print(\"\\nDone shape check.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "check_feature_shapes(dataset, idx=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class ConvolutionLSTMBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.AvgPool2d(2)\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((32, 8))  \n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=8 * 256, hidden_size=512,\n",
    "                            num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(x.size(0), 32, -1)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        return out[:, -1, :]  \n",
    "\n",
    "\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class AudioFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mel_branch = ConvolutionLSTMBlock()\n",
    "        self.mfcc_branch = DenseNeuralNetwork(input_dim=40)\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=12)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=6)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=7)\n",
    "\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(1024 + 4 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.embedding_out = nn.Linear(256, 128)  \n",
    "\n",
    "\n",
    "    def forward(self, mel, mfcc, chroma, tonnetz, contrast):\n",
    "        mel_out = self.mel_branch(mel)\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(chroma)\n",
    "        tonnetz_out = self.tonnetz_branch(tonnetz)\n",
    "        contrast_out = self.contrast_branch(contrast)\n",
    "\n",
    "        combined = torch.cat([mel_out, mfcc_out, chroma_out, tonnetz_out, contrast_out], dim=1)\n",
    "        fusion = self.fc_fusion(combined)\n",
    "        embedding = self.embedding_out(fusion)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c44a1",
   "metadata": {},
   "source": [
    "Tuned Model Architecture\n",
    "To test replace the code aboev with this version:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e09246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TUNED MODEL ARCHITECTURE\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvolutionLSTMBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.AvgPool2d(2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((32, 8))  # Fixed output shape\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=8 * 256,  # 8 = width after pooling, 256 = channels\n",
    "            hidden_size=256,     # reduced hidden size for generalisation\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(x.size(0), 32, -1)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        return out[:, -1, :]  # final time step output\n",
    "\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)  # no activation here\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mel_branch = ConvolutionLSTMBlock()\n",
    "        self.mfcc_branch = DenseNeuralNetwork(input_dim=40)\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=12)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=6)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=7)\n",
    "\n",
    "        # Mel output: 512 (from 2x 256 LSTM), others: 4 * 128\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 4 * 128, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.embedding_out = nn.Linear(256, 128)  # Final speaker embedding\n",
    "\n",
    "    def forward(self, mel, mfcc, chroma, tonnetz, contrast):\n",
    "        mel_out = self.mel_branch(mel)\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(chroma)\n",
    "        tonnetz_out = self.tonnetz_branch(tonnetz)\n",
    "        contrast_out = self.contrast_branch(contrast)\n",
    "\n",
    "        combined = torch.cat([mel_out, mfcc_out, chroma_out, tonnetz_out, contrast_out], dim=1)\n",
    "        fusion = self.fc_fusion(combined)\n",
    "        embedding = self.embedding_out(fusion)  # No activation here\n",
    "        return embedding\n",
    "\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TripletAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.label_to_indices = self._build_label_index()\n",
    "\n",
    "\n",
    "    def _build_label_index(self):\n",
    "        label_to_indices = {}\n",
    "        print(\"[INFO] Building label index...\")\n",
    "        for idx in tqdm(range(len(self.base_dataset)), desc=\"Indexing labels\"):\n",
    "            _, _, _, _, _, label = self.base_dataset[idx]\n",
    "            label = label.item()\n",
    "            if label not in label_to_indices:\n",
    "                label_to_indices[label] = []\n",
    "            label_to_indices[label].append(idx)\n",
    "        return label_to_indices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, anchor_idx):\n",
    "        anchor = self.base_dataset[anchor_idx]\n",
    "        anchor_label = anchor[-1].item()\n",
    "\n",
    "        positive_idx = anchor_idx\n",
    "        while positive_idx == anchor_idx:\n",
    "            positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive = self.base_dataset[positive_idx]\n",
    "\n",
    "        negative_label = random.choice([label for label in self.label_to_indices.keys() if label != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative = self.base_dataset[negative_idx]\n",
    "\n",
    "        return (\n",
    "            (anchor[:-1], positive[:-1], negative[:-1])\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_triplet(model, triplet_dataset, epochs=20, batch_size=32, lr=1e-4, device='cuda', save_csv_path=None):\n",
    "    model.to(device)\n",
    "    dataloader = DataLoader(triplet_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            anchor, positive, negative = batch\n",
    "\n",
    "            def move_to_device(x):\n",
    "                return [t.to(device) for t in x]\n",
    "\n",
    "            anchor = move_to_device(anchor)\n",
    "            positive = move_to_device(positive)\n",
    "            negative = move_to_device(negative)\n",
    "\n",
    "            emb_anchor = model(*anchor)\n",
    "            emb_positive = model(*positive)\n",
    "            emb_negative = model(*negative)\n",
    "\n",
    "            loss = criterion(emb_anchor, emb_positive, emb_negative)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']:.6f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"voice_model.pth\")\n",
    "    print(\"\\nModel saved to 'model.pth'\")\n",
    "\n",
    "    # Plotting loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs+1), epoch_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Triplet Loss\")\n",
    "    plt.title(\"Triplet Loss Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df = pd.DataFrame({\n",
    "            'epoch': list(range(1, epochs + 1)),\n",
    "            'avg_loss': epoch_losses\n",
    "        })\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "        print(f\"Loss data saved to: {save_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917424e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = AudioFeatureDataset(\"datasets/vox1_subset_150/val_features\")\n",
    "triplet_dataset = TripletAudioDataset(base_dataset)\n",
    "\n",
    "model = AudioFusionModel()  \n",
    "train_triplet(model, triplet_dataset, epochs=100, batch_size=32, lr=0.001, device='cuda')    # Save model\n",
    "\n",
    "torch.save(model.state_dict(), \"voice_model.pth\")\n",
    "print(\"\\nModel saved to 'val_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ac949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
