{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         # should return True\n",
    "print(torch.cuda.get_device_name(0))     # should say \"NVIDIA GeForce RTX 3060\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "BASE_DIR = \"datasets/release_in_the_wild\"\n",
    "META_CSV = os.path.join(BASE_DIR, \"meta.csv\")\n",
    "WAV_DIR = BASE_DIR\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"train\": os.path.join(BASE_DIR, \"train\"),\n",
    "    \"val\": os.path.join(BASE_DIR, \"val\"),\n",
    "    \"test\": os.path.join(BASE_DIR, \"test\")\n",
    "}\n",
    "\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df[\"file\"] = df[\"file\"].str.strip()\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"speaker\"] = df[\"speaker\"].str.strip()\n",
    "\n",
    "df_40_percent, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=0.4,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_40_percent,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_40_percent[\"label\"]\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "spoof_train = train_df[train_df[\"label\"] == \"spoof\"]\n",
    "bonafide_train = train_df[train_df[\"label\"] == \"bona-fide\"]\n",
    "\n",
    "spoof_upsampled = resample(\n",
    "    spoof_train,\n",
    "    replace=True,\n",
    "    n_samples=len(bonafide_train),\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = pd.concat([bonafide_train, spoof_upsampled]).sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "#copy not move \n",
    "def move_files(subset_df, split_name):\n",
    "    for _, row in subset_df.iterrows():\n",
    "        src = os.path.join(WAV_DIR, row[\"file\"])\n",
    "        dst = os.path.join(OUTPUT_DIRS[split_name], row[\"file\"])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# Move files\n",
    "#move_files(train_df, \"train\")\n",
    "#move_files(val_df, \"val\")\n",
    "#move_files(test_df, \"test\")\n",
    "\n",
    "# Save metadata\n",
    "train_df.to_csv(os.path.join(BASE_DIR, \"train_meta.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(BASE_DIR, \"val_meta.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(BASE_DIR, \"test_meta.csv\"), index=False)\n",
    "\n",
    "# Summary\n",
    "def count_files(folder_path):\n",
    "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "print(\"✔ 40% of dataset sampled and split into train/val/test (70/15/15).\")\n",
    "print(\"✔ Spoof samples oversampled in train set only.\")\n",
    "print(f\"Train files: {count_files(OUTPUT_DIRS['train'])}\")\n",
    "print(f\"Val files: {count_files(OUTPUT_DIRS['val'])}\")\n",
    "print(f\"Test files: {count_files(OUTPUT_DIRS['test'])}\")\n",
    "\n",
    "# Show label distribution in each set\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# Preprocess raw audio\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Main extraction + saving\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "# Run the feature extraction\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0,\n",
    "                         apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    splits = [\"test\"] \n",
    "\n",
    "    for split in splits:\n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"\\nLooking in: {input_folder}\")\n",
    "        wav_files = sorted(input_folder.glob(\"*.wav\"))\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "                \n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio(path, sr=22050):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "def show_waveform(y, sr, title=\"Waveform\"):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mel_spectrogram(y, sr, title=\"Mel Spectrogram\"):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_audio(original_path, fake_path, apply_preprocessing=False, sr=22050):\n",
    "    print(f\" Original: {original_path}\")\n",
    "    y_real = load_audio(original_path, sr=sr)\n",
    "    y_fake = load_audio(fake_path, sr=sr)\n",
    "\n",
    "    if apply_preprocessing:\n",
    "        y_real = preprocess_audio(y_real, sr, apply_preemphasis=True, normalise='rms')\n",
    "        y_fake = preprocess_audio(y_fake, sr, apply_preemphasis=True, normalise='rms')\n",
    "\n",
    "    # Waveform comparison\n",
    "    show_waveform(y_real, sr, title=\"Real - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "\n",
    "    show_waveform(y_fake, sr, title=\"Fake - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_fake, rate=sr))\n",
    "\n",
    "    # Mel spectrogram comparison\n",
    "    show_mel_spectrogram(y_real, sr, title=\"Real - Mel Spectrogram\")\n",
    "    show_mel_spectrogram(y_fake, sr, title=\"Fake - Mel Spectrogram\")\n",
    "\n",
    "real_audio_path = \"datasets/release_in_the_wild/remaining_files/7.wav\"\n",
    "\n",
    "fake_audio_path = \"datasets/release_in_the_wild/remaining_files/5.wav\"\n",
    "\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=False)\n",
    "\n",
    "# compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):  \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].str.strip().str.lower()\n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {'bona-fide': 1, 'spoof': 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            tensor = tensor[:target_shape[0], :target_shape[1]] \n",
    "        else:\n",
    "            tensor = F.pad(tensor, (0, pad_w, 0, pad_h))  \n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, feat, file_id) \n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "            feature = np.load(path)\n",
    "            tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self.pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "feature_root = \"datasets/release_in_the_wild/preprocessed_train\"\n",
    "features = ['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "            'onset_strength', 'pitch', 'spectral_centroid',\n",
    "            'spectral_contrast', 'tonnetz', 'zcr']\n",
    "target_shape = (128, 259)  \n",
    "\n",
    "def pad_or_resize(tensor, target_shape):\n",
    "    h, w = tensor.shape\n",
    "    pad_h = target_shape[0] - h\n",
    "    pad_w = target_shape[1] - w\n",
    "\n",
    "    if pad_h < 0 or pad_w < 0:\n",
    "        tensor = tensor[:target_shape[0], :target_shape[1]]\n",
    "    else:\n",
    "        tensor = F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "    return tensor\n",
    "\n",
    "for feat in features:\n",
    "    folder_path = os.path.join(feature_root, feat)\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    file_list = os.listdir(folder_path)\n",
    "    if not file_list:\n",
    "        print(f\"No files found in: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    file_name = file_list[0]\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    try:\n",
    "        arr = np.load(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nFeature: {feat.upper()} \")\n",
    "    print(f\"File path: {file_path}\")\n",
    "    print(f\"Raw .npy shape: {arr.shape}\")\n",
    "\n",
    "    tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "    if tensor.dim() == 1:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        print(f\"Tensor shape after unsqueeze: {tensor.shape}\")\n",
    "    \n",
    "    processed_tensor = pad_or_resize(tensor, target_shape)\n",
    "    print(f\"Shape after pad_or_resize: {processed_tensor.shape}\")\n",
    "\n",
    "print(\"\\nDone checking all feature folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SiameseMFCCBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMFCCBranch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.flattened_size = 128 * 32 * 64  \n",
    "        self.fc = nn.Linear(self.flattened_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))       \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.fc(x)                   \n",
    "        return x\n",
    "\n",
    "\n",
    "# Final Fusion Model\n",
    "class AudioDeepfakeFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioDeepfakeFusionModel, self).__init__()\n",
    "\n",
    "        self.mfcc_branch = SiameseMFCCBranch()\n",
    "\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.pitch_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.energy_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.zcr_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.onset_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.centroid_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.mel_spec_branch = DenseNeuralNetwork(input_dim=128)\n",
    "\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(10 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, mfcc, chroma, tonnetz, contrast, pitch, energy, zcr, onset, centroid, mel_spec):\n",
    "        mfcc = mfcc.unsqueeze(1)  \n",
    "\n",
    "        def pool(x): return x.mean(dim=-1)  \n",
    "\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(pool(chroma))\n",
    "        tonnetz_out = self.tonnetz_branch(pool(tonnetz))\n",
    "        contrast_out = self.contrast_branch(pool(contrast))\n",
    "        pitch_out = self.pitch_branch(pool(pitch))\n",
    "        energy_out = self.energy_branch(pool(energy))\n",
    "        zcr_out = self.zcr_branch(pool(zcr))\n",
    "        onset_out = self.onset_branch(pool(onset))\n",
    "        centroid_out = self.centroid_branch(pool(centroid))\n",
    "        mel_spec_out = self.mel_spec_branch(pool(mel_spec))\n",
    "\n",
    "        fusion = torch.cat([\n",
    "            mfcc_out, chroma_out, tonnetz_out, contrast_out,\n",
    "            pitch_out, energy_out, zcr_out, onset_out, centroid_out, mel_spec_out\n",
    "        ], dim=1)\n",
    "\n",
    "        x = self.fusion_layer(fusion)\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                all_preds += outputs.cpu().numpy().flatten().tolist()\n",
    "                all_labels += labels.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        all_preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        acc = np.mean(np.array(all_preds_bin) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds_bin)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "       # history['val_loss'].append(val_loss / len(val_loader))\n",
    "        #history['val_acc'].append(acc)\n",
    "        #history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"[{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, #Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return model, history, all_labels, all_preds_bin\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    #plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Accuracy')\n",
    "    #plt.plot(history['val_f1'], label='F1 Score')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1 Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Spoof', 'Bona-fide'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_binary_classification_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=20, batch_size=32, learning_rate=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_prec': [],\n",
    "        'val_rec': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        train_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            train_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            train_trues.extend(labels.cpu().numpy())\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = [b.to(device) for b in batch[:-1]]\n",
    "                labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                val_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "                val_trues.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        acc = accuracy_score(val_trues, val_preds)\n",
    "        prec = precision_score(val_trues, val_preds)\n",
    "        rec = recall_score(val_trues, val_preds)\n",
    "        f1 = f1_score(val_trues, val_preds)\n",
    "\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(acc)\n",
    "        history['val_prec'].append(prec)\n",
    "        history['val_rec'].append(rec)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"\\n[Validation] Loss: {avg_val_loss:.4f} | Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"df_model.pth\")\n",
    "    print(\"\\n Model saved to 'df_model.pth'\")\n",
    "\n",
    "    # Plot metric graphs\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Show confusion matrix for both splits\n",
    "    show_confusion_matrix(train_trues, train_preds, title=\"Train Set\")\n",
    "    show_confusion_matrix(val_trues, val_preds, title=\"Validation Set\")\n",
    "\n",
    "\n",
    "def show_confusion_matrix(true_labels, pred_labels, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(history['val_loss'], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    # Accuracy & F1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_acc'], label=\"Accuracy\")\n",
    "    plt.plot(history['val_f1'], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1\")\n",
    "\n",
    "    # Precision & Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['val_prec'], label=\"Precision\")\n",
    "    plt.plot(history['val_rec'], label=\"Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Precision & Recall\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"datasets/release_in_the_wild\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    csv_path = root / f\"{split}_meta.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARNING] CSV not found: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"label\"] = df[\"label\"].str.strip().str.lower()  \n",
    "\n",
    "    print(f\"\\n{split.upper()} SPLIT:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "    print(\"Total:\", len(df))\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"datasets/release_in_the_wild\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "\n",
    "train_binary_classification_model(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(model, test_dataset, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            preds = (outputs > 0.5).int().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/test_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_test\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "evaluate_on_test_set(model, test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning audio files: 100%|██████████| 43839/43839 [04:36<00:00, 158.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Silent audio files found: 22\n",
      "11475.wav — 99.92% silent\n",
      "12309.wav — 92.17% silent\n",
      "14985.wav — 98.49% silent\n",
      "15113.wav — 98.02% silent\n",
      "15884.wav — 90.82% silent\n",
      "17006.wav — 99.39% silent\n",
      "18769.wav — 98.50% silent\n",
      "18926.wav — 99.34% silent\n",
      "1953.wav — 95.44% silent\n",
      "21183.wav — 99.46% silent\n",
      "21680.wav — 96.36% silent\n",
      "22377.wav — 95.03% silent\n",
      "26614.wav — 90.91% silent\n",
      "2762.wav — 93.87% silent\n",
      "5802.wav — 96.03% silent\n",
      "7169.wav — 95.97% silent\n",
      "9076.wav — 97.67% silent\n",
      "12309.wav — 92.17% silent\n",
      "18926.wav — 99.34% silent\n",
      "21183.wav — 99.46% silent\n",
      "2762.wav — 93.87% silent\n",
      "26614.wav — 90.91% silent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Add this\n",
    "\n",
    "root_dir = \"datasets/release_in_the_wild\"\n",
    "threshold = 0.001\n",
    "min_silence_ratio = 0.9\n",
    "\n",
    "silent_files = []\n",
    "\n",
    "# Collect all .wav files first for progress tracking\n",
    "all_wav_files = []\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            all_wav_files.append(os.path.join(subdir, file))\n",
    "\n",
    "# Use tqdm to show progress\n",
    "for path in tqdm(all_wav_files, desc=\"Scanning audio files\"):\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=22050)\n",
    "        silence_ratio = np.mean(np.abs(y) < threshold)\n",
    "        if silence_ratio > min_silence_ratio:\n",
    "            silent_files.append((os.path.basename(path), silence_ratio))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {os.path.basename(path)}: {e}\")\n",
    "\n",
    "print(f\"\\nSilent audio files found: {len(silent_files)}\")\n",
    "for file, ratio in silent_files:\n",
    "    print(f\"{file} — {ratio*100:.2f}% silent\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
