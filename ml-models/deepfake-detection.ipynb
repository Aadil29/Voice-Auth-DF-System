{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         \n",
    "print(torch.cuda.get_device_name(0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Split and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "BASE_DIR = \"datasets/release_in_the_wild\"\n",
    "META_CSV = os.path.join(BASE_DIR, \"meta.csv\")\n",
    "WAV_DIR = BASE_DIR\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"train\": os.path.join(BASE_DIR, \"train\"),\n",
    "    \"val\": os.path.join(BASE_DIR, \"val\"),\n",
    "    \"test\": os.path.join(BASE_DIR, \"test\")\n",
    "}\n",
    "\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df[\"file\"] = df[\"file\"].str.strip()\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"speaker\"] = df[\"speaker\"].str.strip()\n",
    "\n",
    "df_40_percent, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=0.4,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_40_percent,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_40_percent[\"label\"]\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "spoof_train = train_df[train_df[\"label\"] == \"spoof\"]\n",
    "bonafide_train = train_df[train_df[\"label\"] == \"bona-fide\"]\n",
    "\n",
    "spoof_upsampled = resample(\n",
    "    spoof_train,\n",
    "    replace=True,\n",
    "    n_samples=len(bonafide_train),\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = pd.concat([bonafide_train, spoof_upsampled]).sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "#copy not move \n",
    "def move_files(subset_df, split_name):\n",
    "    for _, row in subset_df.iterrows():\n",
    "        src = os.path.join(WAV_DIR, row[\"file\"])\n",
    "        dst = os.path.join(OUTPUT_DIRS[split_name], row[\"file\"])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# Move files\n",
    "#move_files(train_df, \"train\")\n",
    "#move_files(val_df, \"val\")\n",
    "#move_files(test_df, \"test\")\n",
    "\n",
    "# Save metadata\n",
    "#train_df.to_csv(os.path.join(BASE_DIR, \"train_meta.csv\"), index=False)\n",
    "#val_df.to_csv(os.path.join(BASE_DIR, \"val_meta.csv\"), index=False)\n",
    "#test_df.to_csv(os.path.join(BASE_DIR, \"test_meta.csv\"), index=False)\n",
    "\n",
    "# Summary\n",
    "def count_files(folder_path):\n",
    "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "print(\"✔ 40% of dataset sampled and split into train/val/test (70/15/15).\")\n",
    "print(\"✔ Spoof samples oversampled in train set only.\")\n",
    "print(f\"Train files: {count_files(OUTPUT_DIRS['train'])}\")\n",
    "print(f\"Val files: {count_files(OUTPUT_DIRS['val'])}\")\n",
    "print(f\"Test files: {count_files(OUTPUT_DIRS['test'])}\")\n",
    "\n",
    "# Show label distribution in each set\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "\n",
    "# Preprocess raw audio\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction (With saving feature to save time for future tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main extraction + saving\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\",\"test\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "#Run the feature extraction\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio(path, sr=22050):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "def show_waveform(y, sr, title=\"Waveform\"):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mel_spectrogram(y, sr, title=\"Mel Spectrogram\"):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_audio(original_path, fake_path, apply_preprocessing=False, sr=22050):\n",
    "    print(f\" Original: {original_path}\")\n",
    "    y_real = load_audio(original_path, sr=sr)\n",
    "    y_fake = load_audio(fake_path, sr=sr)\n",
    "\n",
    "    if apply_preprocessing:\n",
    "        y_real = preprocess_audio(y_real, sr, apply_preemphasis=True, normalise='rms')\n",
    "        y_fake = preprocess_audio(y_fake, sr, apply_preemphasis=True, normalise='rms')\n",
    "\n",
    "    # Waveform comparison\n",
    "    show_waveform(y_real, sr, title=\"Real - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "\n",
    "    show_waveform(y_fake, sr, title=\"Fake - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_fake, rate=sr))\n",
    "\n",
    "    # Mel spectrogram comparison\n",
    "    show_mel_spectrogram(y_real, sr, title=\"Real - Mel Spectrogram\")\n",
    "    show_mel_spectrogram(y_fake, sr, title=\"Fake - Mel Spectrogram\")\n",
    "\n",
    "real_audio_path = \"datasets/release_in_the_wild/remaining_files/7.wav\"\n",
    "\n",
    "fake_audio_path = \"datasets/release_in_the_wild/remaining_files/5.wav\"\n",
    "\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=False)\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE THIS VERSION TO TRAIN,TEST,VAL FOR RELEASE_IN_THE_WILD DATASET\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):  \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].str.strip().str.lower()\n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {'bona-fide': 1, 'spoof': 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            tensor = tensor[:target_shape[0], :target_shape[1]] \n",
    "        else:\n",
    "            tensor = F.pad(tensor, (0, pad_w, 0, pad_h))  \n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, feat, file_id) \n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "            feature = np.load(path)\n",
    "            tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self.pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture (DenseNN + Siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SiameseMFCCBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMFCCBranch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.flattened_size = 128 * 32 * 64  \n",
    "        self.fc = nn.Linear(self.flattened_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))       \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.fc(x)                   \n",
    "        return x\n",
    "\n",
    "\n",
    "# Final Fusion Model\n",
    "class AudioDeepfakeFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioDeepfakeFusionModel, self).__init__()\n",
    "\n",
    "        self.mfcc_branch = SiameseMFCCBranch()\n",
    "\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.pitch_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.energy_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.zcr_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.onset_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.centroid_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.mel_spec_branch = DenseNeuralNetwork(input_dim=128)\n",
    "\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(10 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, mfcc, chroma, tonnetz, contrast, pitch, energy, zcr, onset, centroid, mel_spec):\n",
    "        mfcc = mfcc.unsqueeze(1)  \n",
    "\n",
    "        def pool(x): return x.mean(dim=-1)  \n",
    "\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(pool(chroma))\n",
    "        tonnetz_out = self.tonnetz_branch(pool(tonnetz))\n",
    "        contrast_out = self.contrast_branch(pool(contrast))\n",
    "        pitch_out = self.pitch_branch(pool(pitch))\n",
    "        energy_out = self.energy_branch(pool(energy))\n",
    "        zcr_out = self.zcr_branch(pool(zcr))\n",
    "        onset_out = self.onset_branch(pool(onset))\n",
    "        centroid_out = self.centroid_branch(pool(centroid))\n",
    "        mel_spec_out = self.mel_spec_branch(pool(mel_spec))\n",
    "\n",
    "        fusion = torch.cat([\n",
    "            mfcc_out, chroma_out, tonnetz_out, contrast_out,\n",
    "            pitch_out, energy_out, zcr_out, onset_out, centroid_out, mel_spec_out\n",
    "        ], dim=1)\n",
    "\n",
    "        x = self.fusion_layer(fusion)\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                all_preds += outputs.cpu().numpy().flatten().tolist()\n",
    "                all_labels += labels.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        all_preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        acc = np.mean(np.array(all_preds_bin) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds_bin)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "       # history['val_loss'].append(val_loss / len(val_loader))\n",
    "        #history['val_acc'].append(acc)\n",
    "        #history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"[{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, #Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return model, history, all_labels, all_preds_bin\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    #plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Accuracy')\n",
    "    #plt.plot(history['val_f1'], label='F1 Score')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1 Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Spoof', 'Bona-fide'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_binary_classification_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=20, batch_size=32, learning_rate=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_prec': [],\n",
    "        'val_rec': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        train_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            train_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            train_trues.extend(labels.cpu().numpy())\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = [b.to(device) for b in batch[:-1]]\n",
    "                labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                val_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "                val_trues.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        acc = accuracy_score(val_trues, val_preds)\n",
    "        prec = precision_score(val_trues, val_preds)\n",
    "        rec = recall_score(val_trues, val_preds)\n",
    "        f1 = f1_score(val_trues, val_preds)\n",
    "\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(acc)\n",
    "        history['val_prec'].append(prec)\n",
    "        history['val_rec'].append(rec)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"\\n[Validation] Loss: {avg_val_loss:.4f} | Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"df_model.pth\")\n",
    "    print(\"\\n Model saved to 'df_model.pth'\")\n",
    "\n",
    "    # Plot metric graphs\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Show confusion matrix for both splits\n",
    "    show_confusion_matrix(train_trues, train_preds, title=\"Train Set\")\n",
    "    show_confusion_matrix(val_trues, val_preds, title=\"Validation Set\")\n",
    "\n",
    "\n",
    "def show_confusion_matrix(true_labels, pred_labels, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(history['val_loss'], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    # Accuracy & F1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_acc'], label=\"Accuracy\")\n",
    "    plt.plot(history['val_f1'], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1\")\n",
    "\n",
    "    # Precision & Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['val_prec'], label=\"Precision\")\n",
    "    plt.plot(history['val_rec'], label=\"Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Precision & Recall\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaner Training and Classification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BinaryClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1e-4,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "        \n",
    "        # History including AUC & EER for both train and validation\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_prec': [], 'train_rec': [], 'train_f1': [], 'train_auc': [], 'train_eer': [],\n",
    "            'val_loss':   [], 'val_acc':   [], 'val_prec':   [], 'val_rec':   [], 'val_f1':   [], 'val_auc':   [], 'val_eer':   []\n",
    "        }\n",
    "\n",
    "    def _run_epoch(self, loader, train: bool):\n",
    "        epoch_loss = 0.0\n",
    "        all_probs, all_preds, all_labels = [], [], []\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        iterable = tqdm(loader, desc='Training') if train else loader\n",
    "        for batch in iterable:\n",
    "            *inputs, labels = batch\n",
    "            labels = labels.float().to(self.device).unsqueeze(1)\n",
    "            inputs = [x.to(self.device) for x in inputs]\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                outputs = self.model(*inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                if train:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * labels.size(0)\n",
    "            probs = outputs.detach().cpu().numpy().flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(truths.tolist())\n",
    "\n",
    "        # Compute metrics\n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        acc  = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        auc  = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Equal Error Rate (EER)\n",
    "        fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "        fnr = 1 - tpr\n",
    "        idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "        eer = fpr[idx]\n",
    "\n",
    "        return avg_loss, acc, prec, rec, f1, auc, eer, all_labels, all_preds\n",
    "\n",
    "    def fit(self, epochs: int = 20):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Training epoch\n",
    "            train_loss, train_acc, train_prec, train_rec, train_f1, train_auc, train_eer, _, _ = \\\n",
    "                self._run_epoch(self.train_loader, train=True)\n",
    "            # Validation epoch\n",
    "            val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, val_eer, _, _ = \\\n",
    "                self._run_epoch(self.val_loader,   train=False)\n",
    "\n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['train_prec'].append(train_prec)\n",
    "            self.history['train_rec'].append(train_rec)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['train_auc'].append(train_auc)\n",
    "            self.history['train_eer'].append(train_eer)\n",
    "\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_prec'].append(val_prec)\n",
    "            self.history['val_rec'].append(val_rec)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            self.history['val_auc'].append(val_auc)\n",
    "            self.history['val_eer'].append(val_eer)\n",
    "\n",
    "            # Print summary\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | \"\n",
    "                f\"F1: {train_f1:.4f} | AUC: {train_auc:.4f} | EER: {train_eer:.4f} || \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | \"\n",
    "                f\"F1: {val_f1:.4f} | AUC: {val_auc:.4f} | EER: {val_eer:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(self.model.state_dict(), \"df_model.pth\")\n",
    "        print(\"Model saved to 'df_model.pth'\")\n",
    "\n",
    "    def plot_history(self):\n",
    "        # You can extend this method to include plots for AUC and EER if desired\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'],   label='Val Loss')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')\n",
    "\n",
    "        # Accuracy & F1\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.history['train_acc'], label='Train Acc')\n",
    "        plt.plot(self.history['val_acc'],   label='Val Acc')\n",
    "        plt.plot(self.history['train_f1'],  label='Train F1')\n",
    "        plt.plot(self.history['val_f1'],    label='Val F1')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Score'); plt.legend(); plt.title('Acc & F1')\n",
    "\n",
    "        # Precision, Recall, AUC\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.history['train_prec'], label='Train Prec')\n",
    "        plt.plot(self.history['val_prec'],   label='Val Prec')\n",
    "        plt.plot(self.history['train_rec'],  label='Train Rec')\n",
    "        plt.plot(self.history['val_rec'],    label='Val Rec')\n",
    "        plt.plot(self.history['train_auc'],  label='Train AUC')\n",
    "        plt.plot(self.history['val_auc'],    label='Val AUC')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Score'); plt.legend(); plt.title('Prec, Rec & AUC')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion(self, true_labels, pred_labels, title='Confusion Matrix'):\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=['Spoof', 'Bona‑fide'])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength',\n",
    "        'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength',\n",
    "        'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "trainer = BinaryClassifierTrainer(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    device='cuda'  \n",
    ")\n",
    "\n",
    "#comment out if you dont want to run \n",
    "\n",
    "trainer.fit(epochs=50)\n",
    "trainer.plot_history()\n",
    "\n",
    "_, _, _, _, _, _, _, val_labels, val_preds = \\\n",
    "    trainer._run_epoch(trainer.val_loader, train=False)\n",
    "\n",
    "trainer.plot_confusion(val_labels, val_preds, title='Validation set Confusion')\n",
    "\n",
    "_, _, _, _, _, _, _, train_labels, train_preds = trainer._run_epoch(trainer.train_loader, train=False)\n",
    "\n",
    "trainer.plot_confusion(train_labels, train_preds, title='train Set Confusion Matrix')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Train + Val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "\n",
    "train_binary_classification_model(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_labels = np.array(all_labels).astype(int)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Metrics\n",
    "\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc  = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "\n",
    "    # Compute EER\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Load saved weights\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(torch.load(\"df_model.pth\", map_location=torch.device('cpu'), weights_only=True))\n",
    "\n",
    "#RELEASE IN THE WILD TEST\n",
    "release_in_wild_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/test_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_test\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(model, release_in_wild_test_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model\n",
    "For 2-sec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: datasets\\evaluation\\for-2sec\\for-2seconds\\testing\\real\n",
      "Found 544 files in 'testing/real'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|██████████| 544/544 [01:48<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: datasets\\evaluation\\for-2sec\\for-2seconds\\testing\\fake\n",
      "Found 544 files in 'testing/fake'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|██████████| 544/544 [01:47<00:00,  5.04it/s]\n",
      "Testing:   3%|▎         | 1/34 [00:01<01:01,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   6%|▌         | 2/34 [00:03<01:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   9%|▉         | 3/34 [00:05<00:54,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  12%|█▏        | 4/34 [00:07<00:53,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  15%|█▍        | 5/34 [00:08<00:50,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  18%|█▊        | 6/34 [00:10<00:48,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  24%|██▎       | 8/34 [00:14<00:45,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  26%|██▋       | 9/34 [00:16<00:46,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  29%|██▉       | 10/34 [00:17<00:43,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  32%|███▏      | 11/34 [00:19<00:40,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  38%|███▊      | 13/34 [00:22<00:35,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  44%|████▍     | 15/34 [00:26<00:33,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  50%|█████     | 17/34 [00:30<00:29,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  53%|█████▎    | 18/34 [00:31<00:27,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  56%|█████▌    | 19/34 [00:33<00:25,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  62%|██████▏   | 21/34 [00:36<00:21,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  65%|██████▍   | 22/34 [00:38<00:20,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  68%|██████▊   | 23/34 [00:40<00:19,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  71%|███████   | 24/34 [00:41<00:16,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  76%|███████▋  | 26/34 [00:45<00:14,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  79%|███████▉  | 27/34 [00:47<00:12,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  82%|████████▏ | 28/34 [00:49<00:11,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  85%|████████▌ | 29/34 [00:51<00:09,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n",
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  91%|█████████ | 31/34 [00:55<00:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  94%|█████████▍| 32/34 [00:57<00:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  97%|█████████▋| 33/34 [00:59<00:01,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 34/34 [01:01<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> In Linear, after flatten: (32, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 34/34 [01:01<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4384\n",
      "Precision: 0.4631\n",
      "Recall:    0.7721\n",
      "F1 Score:  0.5789\n",
      "AUC:       0.3949\n",
      "EER:       0.5919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGwCAYAAAC+Qv9QAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAStxJREFUeJzt3XlcVPX6B/DPsK8zCAQDCpSKCAm4pU6WK4lALlfvvS2oUIih4JqkliummDdzu6RlJq5ZV9OSSypa4AKaYqgJopD+wAQpFRCU/fz+MM5tEpVhWDzD593rvF7OOd9zzjO+FJ+e5/s9RyYIggAiIiIiCdNr6QCIiIiItMWEhoiIiCSPCQ0RERFJHhMaIiIikjwmNERERCR5TGiIiIhI8pjQEBERkeQZtHQA9Gg1NTW4fv06LC0tIZPJWjocIiLSkCAIuHPnDhwdHaGn13R1hLKyMlRUVGh9HSMjI5iYmDRCRM2LCc0T7vr163BycmrpMIiISEu5ublo165dk1y7rKwMppY2QNVdra+lVCpx5coVySU1TGiecJaWlgCArCu5sJTLWzgaoqbhPGBmS4dA1GSE6gpUpG8Wf543hYqKCqDqLow9ggB9o4ZfqLoC+embUVFRwYSGGldtm8lSLoecCQ3pKJk2P4CJJKJZpg0YmGj190mQSXdqLRMaIiIiXSEDoE3iJOGpmkxoiIiIdIVM7/6mzfkSJd3IiYiIiP7ACg0REZGukMm0bDlJt+fEhIaIiEhXsOVEREREJF2s0BAREekKtpyIiIhI+rRsOUm4cSPdyImIiIj+wAoNERGRrmDLiYiIiCSPq5yIiIiIpIsVGiIiIl3BlhMRERFJXituOTGhISIi0hWtuEIj3VSMiIiI6A+s0BAREekKtpyIiIhI8mQyLRMatpyIiIiIWgwrNERERLpCT3Z/0+Z8iWJCQ0REpCta8Rwa6UZORERE9AdWaIiIiHRFK34ODRMaIiIiXcGWExEREZF0MaEhIiLSFbUtJ222Blq2bBlkMhmmTZsm7hswYABkMpnaFhYWpnZeTk4OAgICYGZmBjs7O0RGRqKqqkrj+7PlREREpCtaqOV06tQpfPLJJ/Dy8nrgWGhoKKKiosTPZmZm4q+rq6sREBAApVKJ5ORk5OXlYdy4cTA0NMTSpUs1ioEVGiIiIl3RAhWakpISBAYGYsOGDWjTps0Dx83MzKBUKsVNLpeLxw4ePIj09HRs27YNXbt2hZ+fHxYvXoyYmBhUVFRoFAcTGiIiIlJTXFystpWXlz90bHh4OAICAuDj41Pn8e3bt8PW1hZdunTBnDlzcPfuXfFYSkoKPD09YW9vL+7z9fVFcXExLly4oFHMbDkRERHpikZqOTk5OantXrBgARYuXPjA8J07d+LMmTM4depUnZd7/fXX4eLiAkdHR5w7dw6zZs1CZmYmvv76awBAfn6+WjIDQPycn5+vUehMaIiIiHRFIz2HJjc3V601ZGxs/MDQ3NxcTJ06FQkJCTAxManzchMmTBB/7enpCQcHBwwePBjZ2dno0KFDw+OsA1tOREREpEYul6ttdSU0qampKCgoQPfu3WFgYAADAwMkJSVhzZo1MDAwQHV19QPn9O7dGwCQlZUFAFAqlbhx44bamNrPSqVSo5iZ0BAREekMvf+1nRqyaZAWDB48GOfPn0daWpq49ezZE4GBgUhLS4O+vv4D56SlpQEAHBwcAAAqlQrnz59HQUGBOCYhIQFyuRweHh4afXO2nIiIiHRFM776wNLSEl26dFHbZ25uDhsbG3Tp0gXZ2dnYsWMH/P39YWNjg3PnzmH69Ono16+fuLx7yJAh8PDwwNixY7F8+XLk5+dj7ty5CA8Pr7Mq9ChMaIiIiKjRGRkZ4dChQ1i1ahVKS0vh5OSE0aNHY+7cueIYfX19xMXFYeLEiVCpVDA3N0dQUJDac2vqiwkNERGRrpDJtFzlpN3LKRMTE8VfOzk5ISkp6bHnuLi4ID4+Xqv7AkxoiIiIdAdfTklEREQkXazQEBER6YpmnBT8pGFCQ0REpCtaccuJCQ0REZGuaMUVGummYkRERER/YIWGiIhIV7DlRERERJLHlhMRERGRdLFCQ0REpCNkMhlkrbRCw4SGiIhIR7TmhIYtJyIiIpI8VmiIiIh0heyPTZvzJYoJDRERkY5gy4mIiIhIwlihISIi0hGtuULDhIaIiEhHMKEhIiIiyWvNCQ3n0BAREZHksUJDRESkK7hsm4iIiKSOLSciIiIiCWOFhoiISEfIZNCyQtN4sTQ3JjREREQ6QgYtW04SzmjYciIiIiLJY4WGiIhIR7TmScFMaIiIiHRFK162zZYTERERSR4rNERERLpCy5aTwJYTERERtTRt59Bot0KqZTGhISIi0hGtOaHhHBoiIiKSPCY0REREukLWCFsDLVu2DDKZDNOmTRP3lZWVITw8HDY2NrCwsMDo0aNx48YNtfNycnIQEBAAMzMz2NnZITIyElVVVRrfnwkNERGRjqhtOWmzNcSpU6fwySefwMvLS23/9OnTsW/fPvznP/9BUlISrl+/jlGjRonHq6urERAQgIqKCiQnJ2Pz5s2IjY3F/PnzNY6BCQ0RERE1WElJCQIDA7Fhwwa0adNG3F9UVISNGzfio48+wqBBg9CjRw9s2rQJycnJOHHiBADg4MGDSE9Px7Zt29C1a1f4+flh8eLFiImJQUVFhUZxMKEhIiLSEY1VoSkuLlbbysvLH3rP8PBwBAQEwMfHR21/amoqKisr1fZ37twZzs7OSElJAQCkpKTA09MT9vb24hhfX18UFxfjwoULGn13JjREREQ6orESGicnJygUCnGLjo6u8347d+7EmTNn6jyen58PIyMjWFlZqe23t7dHfn6+OObPyUzt8dpjmuCybSIiIlKTm5sLuVwufjY2Nq5zzNSpU5GQkAATE5PmDK9OrNAQERHpiMaq0MjlcrWtroQmNTUVBQUF6N69OwwMDGBgYICkpCSsWbMGBgYGsLe3R0VFBQoLC9XOu3HjBpRKJQBAqVQ+sOqp9nPtmPpiQkNERKQrmnHZ9uDBg3H+/HmkpaWJW8+ePREYGCj+2tDQEIcPHxbPyczMRE5ODlQqFQBApVLh/PnzKCgoEMckJCRALpfDw8NDo6/OlhMRERFpzNLSEl26dFHbZ25uDhsbG3F/SEgIZsyYAWtra8jlckyePBkqlQp9+vQBAAwZMgQeHh4YO3Ysli9fjvz8fMydOxfh4eF1VoUehQkNERGRjnjSXn2wcuVK6OnpYfTo0SgvL4evry8+/vhj8bi+vj7i4uIwceJEqFQqmJubIygoCFFRURrfiwkNERGRjmjphCYxMVHts4mJCWJiYhATE/PQc1xcXBAfH6/VfQEmNERERDqjpROalsRJwURERCR5rNAQERHpCi1fMKnVuS2MCQ0REZGOYMuJiIiISMJYoaFWadmn/8UHG75T2+fqYo8fd81DzvWb8B6xoM7zNkW/iZE+3ZsjRKIGmxb0EhZEjMC6L37Aux/tBgDY2VgiasrfMKB3Z1iYGSPr/wqw4vMD2PdDGgCgb3dXxH0ytc7rDQpajp/Sc5orfNJCa67QMKFpRoIg4K233sKuXbtw+/Zt/PTTT+jatWtLh9VqdW7vgL0xk8XPBgb3C5Zt7dvg4ndL1cZu3nMca7cdgs/zzzZrjESa6ubhjOC/9cXPl66p7V+3cBwUlqZ4fcYnuFlUgr/79sSm6DcxcNxynL90DT+e+wVuQ+eonfNu2Mvo/5wbkxkJkUHLhEbCk2jYcmpG+/fvR2xsLOLi4pCXl/fAExapeRno68HeVi5uNlYWAAD9v+y3t5UjLvEsRvp0h4WZZk+uJGpO5qZG+DQqGFOXfoHCO/fUjvXyao8NXybhTPr/4f9+vYkVnx9A0Z176OruBACorKpGwc074narsBT+/bywfd+JlvgqRBpjQtOMsrOz4eDggOeffx5KpRIGBiyQtaRfcn+Du9+76DpiAULnxiI3/1ad49IycnD+0jWMGa5q5giJNPOvd17BweM/I+nHzAeO/XjuF/ztpR6wkptBJpNh1Es9YGxsgGOpl+u8ll8/L1grzLGDCY2kNNbLKaWoVSU0u3btgqenJ0xNTWFjYwMfHx+UlpYiODgYI0eOxKJFi/DUU09BLpcjLCwMFRUV4rnl5eWYMmUK7OzsYGJighdeeAGnTp1Su35SUhJ69eoFY2NjODg4YPbs2aiqqgIABAcHY/LkycjJyYFMJsPTTz/dnF+d/qLHs08jZsEY/GdNOFbMfgX/d/0m/ENX4k5p2QNjt36TArdnlOjt3b4FIiWqn1Ev9YB3ZydExXxb5/E35nwOAwN9XDm8HDeSV2Hlu69ibOQGXLn2e53jx45Q4fsTGbheUNiEUVOja8aXUz5pWk2JIC8vD6+99hqWL1+Ov/3tb7hz5w6OHj0KQRAAAIcPH4aJiQkSExNx9epVvPHGG7CxscGSJUsAAO+88w52796NzZs3w8XFBcuXL4evry+ysrJgbW2NX3/9Ff7+/ggODsaWLVtw8eJFhIaGwsTEBAsXLsTq1avRoUMHfPrppzh16hT09fXrjLO8vBzl5eXi5+Li4qb/zWmFXur7v7kwXVzbomeXp+E5bD72HjqDsSOeF4/dK6vArgOnERkytCXCJKqXtvZWiH57NEZF/BvlFVV1jnkv7GUoLE0xYtKa++2k/l7YFP0m/ENXIT37utpYRzsrDOrjjjfmfN4c4RM1ilaV0FRVVWHUqFFwcXEBAHh6eorHjYyM8Pnnn8PMzAzPPvssoqKiEBkZicWLF+PevXtYt24dYmNj4efnBwDYsGEDEhISsHHjRkRGRuLjjz+Gk5MT/v3vf0Mmk6Fz5864fv06Zs2ahfnz50OhUMDS0hL6+vpQKpUPjTM6OhqLFi1q2t8MeoDC0gwdne3wS+5vavu/+T4N98oq8GpArxaKjOjxvDs7w85GjsSts8R9Bgb6eL5bB4T+ox+e+/tiTHilP1SvvI+Lv+QDAH6+/CtU3Tpg/D/6YcaynWrXe31YH9wqKsV3R8416/cg7XGVUyvg7e2NwYMHw9PTE76+vhgyZAj+/ve/o02bNuJxMzMzcbxKpUJJSQlyc3NRVFSEyspK9O3bVzxuaGiIXr16ISMjAwCQkZEBlUql9oehb9++KCkpwbVr1+Ds7FyvOOfMmYMZM2aIn4uLi+Hk5KTVd6fHK7lbjiu//o5XbNUTl23fJMOvnyds21i2UGREj3fkVCaef3WJ2r5/zx+Dy1dvYPWWBJiZGAEAamoEtTHV1QJkeg/+AxY4rA92xv+IquqapguamkRrTmhazRwafX19JCQk4LvvvoOHhwfWrl0LNzc3XLlypaVDU2NsbAy5XK62UeObt+prHE+9jJzrN3Hy7C8YG/kp9PX0MNq3hzjml9zfkPxTtloLiuhJVHK3HBnZeWrb3XsVuFVUiozsPFy6mo/snAKsnPMaunu44Om2tggPHISBvd0Qn3hW7Vr9nuuEp9vaYuve5Bb6NqQNmUz7TapaTYUGuJ959u3bF3379sX8+fPh4uKCPXv2AADOnj2Le/fuwdTUFABw4sQJWFhYwMnJCba2tjAyMsLx48fFdlVlZSVOnTqFadOmAQDc3d2xe/duCIIgZrjHjx+HpaUl2rVr1/xflh7p14JCjJ+7CbeK7sK2jQV6e7dHwqa31Sox275N+WMuQecWjJRIe1XVNfjntHVYEDECX3z0FszNjHEl9zdMWrgVCcnpamPHDn8eJ89m4/L/3WihaIkaptUkNCdPnsThw4cxZMgQ2NnZ4eTJk/jtt9/g7u6Oc+fOoaKiAiEhIZg7dy6uXr2KBQsWICIiAnp6ejA3N8fEiRMRGRkJa2trODs7Y/ny5bh79y5CQkIAAJMmTcKqVaswefJkREREIDMzEwsWLMCMGTOgp9dqCmGS8fnSNx87Zn74cMwPH94M0RA1vmFhq9U+/5L7G4JmffbY80LnxTZRRNQc7ldZtGk5NWIwzazVJDRyuRxHjhzBqlWrUFxcDBcXF6xYsQJ+fn748ssvMXjwYLi6uqJfv34oLy/Ha6+9hoULF4rnL1u2DDU1NRg7dizu3LmDnj174sCBA+IcnLZt2yI+Ph6RkZHw9vaGtbW1mCARERE1C23bRhJOaGRC7brlViw4OBiFhYXYu3dvS4fygOLiYigUCty4WcT5NKSz2jwX0dIhEDUZoboC5ec3oKio6X6O1/5b0X7KLugbmzf4OtXlpfhlzd+bNNam0moqNERERLquNa9yYkJDRESkI7RdqSThfIYJDQDExsa2dAhERESkBSY0REREOkJPTwa9Oh6WWF+CFue2NCY0REREOqI1t5z4gBQiIiKSPFZoiIiIdARXOREREZHkteaWExMaIiIiHdGaKzScQ0NERESSxwoNERGRjmjNFRomNERERDqiNc+hYcuJiIiIJI8VGiIiIh0hg5YtJ0i3RMMKDRERkY6obTlps2li3bp18PLyglwuh1wuh0qlwnfffSceHzBggDivp3YLCwtTu0ZOTg4CAgJgZmYGOzs7REZGoqqqSuPvzgoNERERNUi7du2wbNkyuLq6QhAEbN68GSNGjMBPP/2EZ599FgAQGhqKqKgo8RwzMzPx19XV1QgICIBSqURycjLy8vIwbtw4GBoaYunSpRrFwoSGiIhIRzT3Kqdhw4apfV6yZAnWrVuHEydOiAmNmZkZlEplnecfPHgQ6enpOHToEOzt7dG1a1csXrwYs2bNwsKFC2FkZFTvWNhyIiIi0hGN1XIqLi5W28rLyx977+rqauzcuROlpaVQqVTi/u3bt8PW1hZdunTBnDlzcPfuXfFYSkoKPD09YW9vL+7z9fVFcXExLly4oNF3Z4WGiIiI1Dg5Oal9XrBgARYuXFjn2PPnz0OlUqGsrAwWFhbYs2cPPDw8AACvv/46XFxc4OjoiHPnzmHWrFnIzMzE119/DQDIz89XS2YAiJ/z8/M1ipkJDRERkY5orJZTbm4u5HK5uN/Y2Pih57i5uSEtLQ1FRUXYtWsXgoKCkJSUBA8PD0yYMEEc5+npCQcHBwwePBjZ2dno0KFDg+OsC1tOREREOqKxWk61q5Zqt0clNEZGRujYsSN69OiB6OhoeHt7Y/Xq1XWO7d27NwAgKysLAKBUKnHjxg21MbWfHzbv5mGY0BAREemIvy6RbsimrZqamofOuUlLSwMAODg4AABUKhXOnz+PgoICcUxCQgLkcrnYtqovtpyIiIioQebMmQM/Pz84Ozvjzp072LFjBxITE3HgwAFkZ2djx44d8Pf3h42NDc6dO4fp06ejX79+8PLyAgAMGTIEHh4eGDt2LJYvX478/HzMnTsX4eHhj6wK1YUJDRERka7Q8l1Omj4ouKCgAOPGjUNeXh4UCgW8vLxw4MABvPTSS8jNzcWhQ4ewatUqlJaWwsnJCaNHj8bcuXPF8/X19REXF4eJEydCpVLB3NwcQUFBas+tqS8mNERERDqiuZ9Ds3Hjxocec3JyQlJS0mOv4eLigvj4eI3uWxfOoSEiIiLJY4WGiIhIRzTkfUx/PV+qmNAQERHpiOZuOT1J2HIiIiIiyWOFhoiISEew5URERESSx5YTERERkYSxQkNERKQjWnOFhgkNERGRjuAcGiIiIpK81lyh4RwaIiIikjxWaIiIiHQEW05EREQkeWw5EREREUkYKzREREQ6QgYtW06NFknzY0JDRESkI/RkMuhpkdFoc25LY8uJiIiIJI8VGiIiIh3BVU5EREQkea15lRMTGiIiIh2hJ7u/aXO+VHEODREREUkeKzRERES6QqZl20jCFRomNERERDqiNU8KZsuJiIiIJI8VGiIiIh0h++M/bc6XKiY0REREOoKrnIiIiIgkjBUaIiIiHcEH6xEREZHkteZVTvVKaL799tt6X3D48OENDoaIiIioIeqV0IwcObJeF5PJZKiurtYmHiIiImogPZkMelqUWbQ5t6XVK6Gpqalp6jiIiIhIS6255aTVKqeysrLGioOIiIi0VDspWJtNE+vWrYOXlxfkcjnkcjlUKhW+++478XhZWRnCw8NhY2MDCwsLjB49Gjdu3FC7Rk5ODgICAmBmZgY7OztERkaiqqpK4++ucUJTXV2NxYsXo23btrCwsMAvv/wCAJg3bx42btyocQBEREQkTe3atcOyZcuQmpqK06dPY9CgQRgxYgQuXLgAAJg+fTr27duH//znP0hKSsL169cxatQo8fzq6moEBASgoqICycnJ2Lx5M2JjYzF//nyNY9E4oVmyZAliY2OxfPlyGBkZifu7dOmCzz77TOMAiIiIqHHUtpy02TQxbNgw+Pv7w9XVFZ06dcKSJUtgYWGBEydOoKioCBs3bsRHH32EQYMGoUePHti0aROSk5Nx4sQJAMDBgweRnp6Obdu2oWvXrvDz88PixYsRExODiooKjWLROKHZsmULPv30UwQGBkJfX1/c7+3tjYsXL2p6OSIiImoktZOCtdkAoLi4WG0rLy9/7L2rq6uxc+dOlJaWQqVSITU1FZWVlfDx8RHHdO7cGc7OzkhJSQEApKSkwNPTE/b29uIYX19fFBcXi1Ween93jUYD+PXXX9GxY8cH9tfU1KCyslLTyxEREdETxsnJCQqFQtyio6MfOvb8+fOwsLCAsbExwsLCsGfPHnh4eCA/Px9GRkawsrJSG29vb4/8/HwAQH5+vloyU3u89pgmNH6wnoeHB44ePQoXFxe1/bt27UK3bt00vRwRERE1EtkfmzbnA0Bubi7kcrm439jY+KHnuLm5IS0tDUVFRdi1axeCgoKQlJSkRRQNo3FCM3/+fAQFBeHXX39FTU0Nvv76a2RmZmLLli2Ii4trihiJiIioHhrr1Qe1q5bqw8jISOzc9OjRA6dOncLq1avxyiuvoKKiAoWFhWpVmhs3bkCpVAIAlEolfvzxR7Xr1a6Cqh1TXxq3nEaMGIF9+/bh0KFDMDc3x/z585GRkYF9+/bhpZde0vRyREREpENqampQXl6OHj16wNDQEIcPHxaPZWZmIicnByqVCgCgUqlw/vx5FBQUiGMSEhIgl8vh4eGh0X0b9C6nF198EQkJCQ05lYiIiJqInuz+ps35mpgzZw78/Pzg7OyMO3fuYMeOHUhMTMSBAwegUCgQEhKCGTNmwNraGnK5HJMnT4ZKpUKfPn0AAEOGDIGHhwfGjh2L5cuXIz8/H3PnzkV4ePgj21x1afDLKU+fPo2MjAwA9+fV9OjRo6GXIiIiokbQ3G/bLigowLhx45CXlweFQgEvLy8cOHBA7NisXLkSenp6GD16NMrLy+Hr64uPP/5YPF9fXx9xcXGYOHEiVCoVzM3NERQUhKioKI1j1zihuXbtGl577TUcP35c7IkVFhbi+eefx86dO9GuXTuNgyAiIiLpedwDdU1MTBATE4OYmJiHjnFxcUF8fLzWsWg8h2b8+PGorKxERkYGbt26hVu3biEjIwM1NTUYP3681gERERFRwzXXQ/WeNBpXaJKSkpCcnAw3Nzdxn5ubG9auXYsXX3yxUYMjIiKi+mvultOTROOExsnJqc4H6FVXV8PR0bFRgiIiIiLNNfek4CeJxi2nf/3rX5g8eTJOnz4t7jt9+jSmTp2KDz/8sFGDIyIiIqqPelVo2rRpo1aGKi0tRe/evWFgcP/0qqoqGBgY4M0338TIkSObJFAiIiJ6NLacHmPVqlVNHAYRERFpq7FefSBF9UpogoKCmjoOIiIiogZr8IP1AKCsrAwVFRVq++r77gciIiJqXHoyGfS0aBtpc25L03hScGlpKSIiImBnZwdzc3O0adNGbSMiIqKWoc0zaKT+LBqNE5p33nkH33//PdatWwdjY2N89tlnWLRoERwdHbFly5amiJGIiIjokTRuOe3btw9btmzBgAED8MYbb+DFF19Ex44d4eLigu3btyMwMLAp4iQiIqLHaM2rnDSu0Ny6dQvt27cHcH++zK1btwAAL7zwAo4cOdK40REREVG9seWkgfbt2+PKlSsAgM6dO+Orr74CcL9yU/uySiIiIqLmpHFC88Ybb+Ds2bMAgNmzZyMmJgYmJiaYPn06IiMjGz1AIiIiqp/aVU7abFKl8Rya6dOni7/28fHBxYsXkZqaio4dO8LLy6tRgyMiIqL607ZtJOF8Rrvn0ACAi4sLXFxcGiMWIiIi0kJrnhRcr4RmzZo19b7glClTGhwMERERUUPUK6FZuXJlvS4mk8mY0DSR3Jt3YVGhdUGN6Ilk2LlXS4dA1GSEynsoP7+hWe6lhwZMjv3L+VJVr38ha1c1ERER0ZOrNbecpJyMEREREQFohEnBRERE9GSQyQA9rnIiIiIiKdPTMqHR5tyWxpYTERERSR4rNERERDqCk4I1dPToUYwZMwYqlQq//vorAGDr1q04duxYowZHRERE9VfbctJmkyqNE5rdu3fD19cXpqam+Omnn1BeXg4AKCoqwtKlSxs9QCIiIqLH0Tihef/997F+/Xps2LABhoaG4v6+ffvizJkzjRocERER1V/tu5y02aRK4zk0mZmZ6Nev3wP7FQoFCgsLGyMmIiIiagBt35gt5bdta1yhUSqVyMrKemD/sWPH0L59+0YJioiIiDSn1wibVGkce2hoKKZOnYqTJ09CJpPh+vXr2L59O2bOnImJEyc2RYxEREREj6Rxy2n27NmoqanB4MGDcffuXfTr1w/GxsaYOXMmJk+e3BQxEhERUT1oOw9Gwh0nzRMamUyG9957D5GRkcjKykJJSQk8PDxgYWHRFPERERFRPelByzk0kG5G0+B2mZGRETw8PNCrVy8mM0RERK1QdHQ0nnvuOVhaWsLOzg4jR45EZmam2pgBAwaID/yr3cLCwtTG5OTkICAgAGZmZrCzs0NkZCSqqqo0ikXjCs3AgQMf+STB77//XtNLEhERUSNo7pZTUlISwsPD8dxzz6GqqgrvvvsuhgwZgvT0dJibm4vjQkNDERUVJX42MzMTf11dXY2AgAAolUokJycjLy8P48aNg6GhoUbPt9M4oenatava58rKSqSlpeHnn39GUFCQppcjIiKiRtLcL6fcv3+/2ufY2FjY2dkhNTVV7REvZmZmUCqVdV7j4MGDSE9Px6FDh2Bvb4+uXbti8eLFmDVrFhYuXAgjI6N6xaJxQrNy5co69y9cuBAlJSWaXo6IiIieMMXFxWqfjY2NYWxs/NjzioqKAADW1tZq+7dv345t27ZBqVRi2LBhmDdvnlilSUlJgaenJ+zt7cXxvr6+mDhxIi5cuIBu3brVK+ZGW3I+ZswYfP755411OSIiItKQTPa/h+s1ZKttOTk5OUGhUIhbdHT0Y+9dU1ODadOmoW/fvujSpYu4//XXX8e2bdvwww8/YM6cOdi6dSvGjBkjHs/Pz1dLZgCIn/Pz8+v93RvtbdspKSkwMTFprMsRERGRhhprDk1ubi7kcrm4vz7VmfDwcPz8888PvKh6woQJ4q89PT3h4OCAwYMHIzs7Gx06dGh4sH+hcUIzatQotc+CICAvLw+nT5/GvHnzGi0wIiIiahlyuVwtoXmciIgIxMXF4ciRI2jXrt0jx/bu3RsAkJWVhQ4dOkCpVOLHH39UG3Pjxg0AeOi8m7ponNAoFAq1z3p6enBzc0NUVBSGDBmi6eWIiIiokTT3pGBBEDB58mTs2bMHiYmJeOaZZx57TlpaGgDAwcEBAKBSqbBkyRIUFBTAzs4OAJCQkAC5XA4PD496x6JRQlNdXY033ngDnp6eaNOmjSanEhERUROT/fGfNudrIjw8HDt27MA333wDS0tLcc6LQqGAqakpsrOzsWPHDvj7+8PGxgbnzp3D9OnT0a9fP3h5eQEAhgwZAg8PD4wdOxbLly9Hfn4+5s6di/Dw8Hq1umppNClYX18fQ4YM4Vu1iYiInkC1FRptNk2sW7cORUVFGDBgABwcHMTtyy+/BHD/IbyHDh3CkCFD0LlzZ7z99tsYPXo09u3bJ15DX18fcXFx0NfXh0qlwpgxYzBu3Di159bUh8Ytpy5duuCXX36pV1mJiIiIdJcgCI887uTkhKSkpMdex8XFBfHx8VrFovGy7ffffx8zZ85EXFwc8vLyUFxcrLYRERFRy2juCs2TpN4VmqioKLz99tvw9/cHAAwfPlztFQiCIEAmk6G6urrxoyQiIqLHqn1XkjbnS1W9E5pFixYhLCwMP/zwQ1PGQ0RERKSxeic0tX2y/v37N1kwRERE1HDNvWz7SaLRpGApl6KIiIh0XXO/bftJolFC06lTp8cmNbdu3dIqICIiIiJNaZTQLFq06IEnBRMREdGTofYlk9qcL1UaJTSvvvqq+FhiIiIierK05jk09X4ODefPEBER0ZNK41VORERE9ITSclKwFq+BanH1TmhqamqaMg4iIiLSkh5k0NMiK9Hm3Jam8buciIiI6MnUmpdta/wuJyIiIqInDSs0REREOqI1r3JiQkNERKQjWvNzaNhyIiIiIsljhYaIiEhHtOZJwUxoiIiIdIQetGw5SXjZNltOREREJHms0BAREekItpyIiIhI8vSgXetFym0bKcdOREREBIAVGiIiIp0hk8kg06JvpM25LY0JDRERkY6QQbsXZks3nWFCQ0REpDP4pGAiIiIiCWOFhoiISIdIt8aiHSY0REREOqI1P4eGLSciIiKSPFZoiIiIdASXbRMREZHk8UnBRERERBLGhIaIiEhH1LactNk0ER0djeeeew6Wlpaws7PDyJEjkZmZqTamrKwM4eHhsLGxgYWFBUaPHo0bN26ojcnJyUFAQADMzMxgZ2eHyMhIVFVVaRQLExoiIiIdIWuETRNJSUkIDw/HiRMnkJCQgMrKSgwZMgSlpaXimOnTp2Pfvn34z3/+g6SkJFy/fh2jRo0Sj1dXVyMgIAAVFRVITk7G5s2bERsbi/nz52sUC+fQEBERUYPs379f7XNsbCzs7OyQmpqKfv36oaioCBs3bsSOHTswaNAgAMCmTZvg7u6OEydOoE+fPjh48CDS09Nx6NAh2Nvbo2vXrli8eDFmzZqFhQsXwsjIqF6xsEJDRESkIxqr5VRcXKy2lZeX1+v+RUVFAABra2sAQGpqKiorK+Hj4yOO6dy5M5ydnZGSkgIASElJgaenJ+zt7cUxvr6+KC4uxoULF+r93ZnQEBER6Qi9RtgAwMnJCQqFQtyio6Mfe++amhpMmzYNffv2RZcuXQAA+fn5MDIygpWVldpYe3t75Ofni2P+nMzUHq89Vl9sOREREemIxnoOTW5uLuRyubjf2Nj4seeGh4fj559/xrFjxxp8f22wQkNERERq5HK52va4hCYiIgJxcXH44Ycf0K5dO3G/UqlERUUFCgsL1cbfuHEDSqVSHPPXVU+1n2vH1AcTGiIiIh3R3KucBEFAREQE9uzZg++//x7PPPOM2vEePXrA0NAQhw8fFvdlZmYiJycHKpUKAKBSqXD+/HkUFBSIYxISEiCXy+Hh4VHvWNhyIiIi0hHN/XLK8PBw7NixA9988w0sLS3FOS8KhQKmpqZQKBQICQnBjBkzYG1tDblcjsmTJ0OlUqFPnz4AgCFDhsDDwwNjx47F8uXLkZ+fj7lz5yI8PLxera5aTGiIiIioQdatWwcAGDBggNr+TZs2ITg4GACwcuVK6OnpYfTo0SgvL4evry8+/vhjcay+vj7i4uIwceJEqFQqmJubIygoCFFRURrFwoSGiIhIR+hBBj2NG0fq52tCEITHjjExMUFMTAxiYmIeOsbFxQXx8fEa3fuvmNAQERHpiOZuOT1JOCmYiIiIJI8VGiIiIh0h++M/bc6XKiY0REREOoItJyIiIiIJY4WGiIhIR8i0XOXElhMRERG1uNbccmJCQ0REpCNac0LDOTREREQkeazQEBER6Qgu2yYiIiLJ05Pd37Q5X6rYciIiIiLJY4WGiIhIR7DlRERERJLHVU5EREREEsYKDRERkY6QQbu2kYQLNExoiIiIdAVXORERERFJGCs01Cqknv8Fm3clISPrGn67dQcfzRuHQc93AQBUVlUjZvMBHDt9EdfybsLS3AS9u7liyht+sLNRPHCtiooqjJm+Fpd+ycPOf09D5w6Ozf11iB5p8stdMO+V7vhkfzrmbT8NK3MjvDOqKwZ4OqCtjTluFpfjuzM5WLYrDXfuVYrntbUxx/Lg3ujrrkRpeSW+OvoL3v/qDKprhBb8NqSJ1rzKqUUrNAMGDMC0adNaMoSHunjxIvr06QMTExN07doVV69ehUwmQ1pa2kPPSUxMhEwmQ2FhYbPFSfVzr6wCndo7YM6kvz1wrKy8AhnZvyL0tcHY+e+pWDF3HK5e+w3TFsXWea2Vn/8XT1nLmzhioobp+owNxg1yxYWcW+I+ZRszKNuYYuEXqeg/51tM2XAcgzzbYtX458UxejIZtr89CEYGeng56jtM/uQ4XnmxA2aN7toC34IaqnaVkzabVLFC8xALFiyAubk5MjMzYWFhASsrK+Tl5cHW1ralQ6MGeOG5znjhuc51HrM0N8UnS0PV9s2eOBJjpq1FXsFtONi1EfcfO3URJ85cxofvjcXx05lNGjORpsyNDbBu4ot4e+MJTB/hKe6/eK0Qb65JEj9fLSjB0l0/4eOwF6CvJ0N1jYABng5wa6vAP5Yl4LfiMiDnNj7YnYZ5r3THv74+i8rqmpb4SqQhGbSb2CvhfIZzaB4mOzsbL7zwAlxcXGBjYwN9fX0olUoYGDAHbA1K7pZBJpPB0txU3Hfz9h1Erd6N92e+AhMTwxaMjqhuy4J6I+HsNRy5kPfYsXJTQ9y5Vym2k57r+BQycgvvJzN/+OH8dcjNjODWzqqpQiZqNC2e0FRVVSEiIgIKhQK2traYN28eBOH+X7Dbt29j3LhxaNOmDczMzODn54fLly+L58bGxsLKygoHDhyAu7s7LCwsMHToUOTl/e8v86lTp/DSSy/B1tYWCoUC/fv3x5kzZx4Zk0wmQ2pqKqKioiCTybBw4cI6W07x8fHo1KkTTE1NMXDgQFy9evWBax07dgwvvvgiTE1N4eTkhClTpqC0tPSh9y4vL0dxcbHaRs2rvKISqz+Px9D+3rAwNwEACIKA+R99hX8E9MGznZxaOEKiB43s8zQ8n7bGkq8e/fMNAKwtjDFjpBe2/nBJ3PeUlSl+KypTG/db0T0AgJ3CpHGDpSajBxn0ZFpsEq7RtHhCs3nzZhgYGODHH3/E6tWr8dFHH+Gzzz4DAAQHB+P06dP49ttvkZKSAkEQ4O/vj8rK/01iu3v3Lj788ENs3boVR44cQU5ODmbOnCkev3PnDoKCgnDs2DGcOHECrq6u8Pf3x507dx4aU15eHp599lm8/fbbyMvLU7terdzcXIwaNQrDhg1DWloaxo8fj9mzZ6uNyc7OxtChQzF69GicO3cOX375JY4dO4aIiIiH3js6OhoKhULcnJz4j2dzqqyqxjtLt0EQBLwXMUrc/8W3x1F6txxv/nNgC0ZHVDdHazMsGfMcJq07ivLKR7eGLEwMsX3mIFz6tQj/2nO2mSKk5iJrhE2qWrx/4uTkhJUrV0Imk8HNzQ3nz5/HypUrMWDAAHz77bc4fvw4nn/+/sS17du3w8nJCXv37sU//vEPAEBlZSXWr1+PDh06AAAiIiIQFRUlXn/QoEFq9/v0009hZWWFpKQkvPzyy3XGVNtasrCwgFKpBAD8/vvvamPWrVuHDh06YMWKFQAgxv7BBx+IY6KjoxEYGChOfHZ1dcWaNWvQv39/rFu3DiYmD/5fz5w5czBjxgzxc3FxMZOaZlKbzOQVFOLTZRPE6gwA/Hg2G+cu/h96DX9X7ZzAKWvgN7Ab3p/5SnOHSyTyfsYGTylMcWjx/36mGejrQeVmj5CXOqPdG9tRIwgwNzHAl+8MRum9KgSv/gFV1f9bvfRb4T10b68+R/Apxf2Wa8FfKjdET6IWT2j69OkD2Z+mVatUKqxYsQLp6ekwMDBA7969xWM2NjZwc3NDRkaGuM/MzExMZgDAwcEBBQUF4ucbN25g7ty5SExMREFBAaqrq3H37l3k5OQAAMLCwrBt2zZxfElJSb3izsjIUIutNvY/O3v2LM6dO4ft27eL+wRBQE1NDa5cuQJ3d/cHrmtsbAxjY+N6xUCNpzaZybn+OzYsewtWcnO147PChiNinK/4ueBmMSbN/QwfzAmEpxsTTmpZRy7kod+cb9X2rQ59HlnXi7D2vxdQIwiwMDHEV+/4oLyqGmNXfv9AJedU1m+YNsITtnIT/P7HPJr+XRxQfLcCl34tbK6vQtpqxbOCWzyh0ZahofrkTJlMJs7BAYCgoCDcvHkTq1evhouLC4yNjaFSqVBRUQEAiIqKqrOl1BhKSkrw1ltvYcqUKQ8cc3Z2bpJ7Ut3u3itHzvWb4udfb9zCxezrUFiawtZajsglW5GR9SvWLHoDNTUCfr91vyWpsDSFoaGB2konADA1NQIAtHOwgf1TVs32PYjqUlpWhYvXCtX23S2vwq2Scly8Vng/mZnlAzMjA0xafxSWpoawNL3/s/P34nLUCAISz+ch89cixLz1AqK+TIWdwhSz/94Vnx/KREUVVzhJRWt+Dk2LJzQnT55U+1w7z8XDwwNVVVU4efKk2HK6efMmMjMz4eHhUe/rHz9+HB9//DH8/f0B3J/78uf2kZ2dHezs7DSO293dHd9+q/5/RCdOnFD73L17d6Snp6Njx44aX58a14XL1xA66xPx84pP4wAAw3x6IGzMS0g8kQ4AeCV8ldp5Gz54C895dQCRlHk9bY2eHZ8CAPy4YpTasR7TdyP391LUCALGrPgey9/ojf/O98Pd8ip8dSwbH+xOa4GIiTTX4glNTk4OZsyYgbfeegtnzpzB2rVrsWLFCri6umLEiBEIDQ3FJ598AktLS8yePRtt27bFiBEj6n19V1dXbN26FT179kRxcTEiIyNhamr6+BMfIywsDCtWrEBkZCTGjx+P1NRUxMbGqo2ZNWsW+vTpg4iICIwfPx7m5uZIT09HQkIC/v3vf2sdA9Xfc14dkPbd8ocef9SxurS1t9b4HKLm9LelB8VfJ1+8AbuxWx57zrWbpXj9w++bMixqato+HE+6BZqWX+U0btw43Lt3D7169UJ4eDimTp2KCRMmAAA2bdqEHj164OWXX4ZKpYIgCIiPj3+gzfQoGzduxO3bt9G9e3eMHTsWU6ZMaVBF5q+cnZ2xe/du7N27F97e3li/fj2WLl2qNsbLywtJSUm4dOkSXnzxRXTr1g3z58+HoyMflU9ERI2vNa9ykgl/nnBCT5zi4mIoFAqcvpQHC0s+bp9003PTd7V0CERNRqi8h5LdYSgqKoJc3jQ/x2v/rfg+LUerfytK7hRjUFfnJo21qbR4y4mIiIgaSSte5dTiLSciIiJqHLJG+E8TR44cwbBhw+Do6AiZTIa9e/eqHQ8ODoZMJlPbhg4dqjbm1q1bCAwMhFwuh5WVFUJCQur9CJU/Y0JDRESkI5r7bdulpaXw9vZGTEzMQ8fUvpKodvviiy/UjgcGBuLChQtISEhAXFwcjhw5Is6l1QRbTkRERNQgfn5+8PPze+QYY2Nj8an7f5WRkYH9+/fj1KlT6NmzJwBg7dq18Pf3x4cffqjRIhpWaIiIiHREY61y+utLksvLyxscU2JiIuzs7ODm5oaJEyfi5s3/PeQ0JSUFVlZWYjIDAD4+PtDT03vgOXWPw4SGiIhIVzRSRuPk5KT2ouTo6OgGhTN06FBs2bIFhw8fxgcffICkpCT4+fmhuroaAJCfn//Ao1QMDAxgbW2N/Px8je7FlhMRERGpyc3NVVu23dB3DL766qvirz09PeHl5YUOHTogMTERgwcP1jrOP2OFhoiISEc01ionuVyutjXWS5Pbt28PW1tbZGVlAQCUSqXaC6UBoKqqCrdu3XrovJuHYUJDRESkI5p7lZOmrl27hps3b8LBwQEAoFKpUFhYiNTUVHHM999/j5qaGvTu3Vuja7PlRERERA1SUlIiVlsA4MqVK0hLS4O1tTWsra2xaNEijB49GkqlEtnZ2XjnnXfQsWNH+Pr6Arj/ouehQ4ciNDQU69evR2VlJSIiIvDqq69q/JogVmiIiIh0RHO/y+n06dPo1q0bunXrBgCYMWOG+N5CfX19nDt3DsOHD0enTp0QEhKCHj164OjRo2otrO3bt6Nz584YPHgw/P398cILL+DTTz/V+LuzQkNERKQrmvnVBwMGDMCjXgl54MCBx17D2toaO3bs0OzGdWCFhoiIiCSPFRoiIiId0ZD3Mf31fKliQkNERKQjtF2p1NSrnJoSExoiIiId0cxTaJ4onENDREREkscKDRERka5oxSUaJjREREQ6ojVPCmbLiYiIiCSPFRoiIiIdwVVOREREJHmteAoNW05EREQkfazQEBER6YpWXKJhQkNERKQjuMqJiIiISMJYoSEiItIRXOVEREREkteKp9AwoSEiItIZrTij4RwaIiIikjxWaIiIiHREa17lxISGiIhIV2g5KVjC+QxbTkRERCR9rNAQERHpiFY8J5gJDRERkc5oxRkNW05EREQkeazQEBER6QiuciIiIiLJa82vPmDLiYiIiCSPFRoiIiId0YrnBDOhISIi0hmtOKNhQkNERKQjWvOkYM6hISIiIsljhYaIiEhHyKDlKqdGi6T5sUJDRESkI2SNsGniyJEjGDZsGBwdHSGTybB3716144IgYP78+XBwcICpqSl8fHxw+fJltTG3bt1CYGAg5HI5rKysEBISgpKSEg0jYUJDREREDVRaWgpvb2/ExMTUeXz58uVYs2YN1q9fj5MnT8Lc3By+vr4oKysTxwQGBuLChQtISEhAXFwcjhw5ggkTJmgcC1tOREREOqK5H6zn5+cHPz+/Oo8JgoBVq1Zh7ty5GDFiBABgy5YtsLe3x969e/Hqq68iIyMD+/fvx6lTp9CzZ08AwNq1a+Hv748PP/wQjo6O9Y6FFRoiIiKd0ThNp+LiYrWtvLxc40iuXLmC/Px8+Pj4iPsUCgV69+6NlJQUAEBKSgqsrKzEZAYAfHx8oKenh5MnT2p0PyY0REREpMbJyQkKhULcoqOjNb5Gfn4+AMDe3l5tv729vXgsPz8fdnZ2ascNDAxgbW0tjqkvtpyIiIh0RGO1nHJzcyGXy8X9xsbGWkbW9FihISIi0hGNtcpJLperbQ1JaJRKJQDgxo0bavtv3LghHlMqlSgoKFA7XlVVhVu3bolj6osJDRERETW6Z555BkqlEocPHxb3FRcX4+TJk1CpVAAAlUqFwsJCpKamimO+//571NTUoHfv3hrdjy0nIiIiHdHcq5xKSkqQlZUlfr5y5QrS0tJgbW0NZ2dnTJs2De+//z5cXV3xzDPPYN68eXB0dMTIkSMBAO7u7hg6dChCQ0Oxfv16VFZWIiIiAq+++qpGK5wAJjREREQ6o7nf5XT69GkMHDhQ/DxjxgwAQFBQEGJjY/HOO++gtLQUEyZMQGFhIV544QXs378fJiYm4jnbt29HREQEBg8eDD09PYwePRpr1qzRPHZBEASNz6JmU1xcDIVCgdOX8mBhKX/8CUQS9Nz0XS0dAlGTESrvoWR3GIqKitQm2jam2n8rLuX+Dkst7nGnuBidnGybNNamwjk0REREJHlsOREREemIhryP6a/nSxUTGiIiIh3R3JOCnyRsOREREZHksUJDRESkI5p7ldOThAkNERGRrmjFk2jYciIiIiLJY4WGiIhIR7TiAg0TGiIiIl3BVU5EREREEsYKDRERkc7QbpWTlJtOTGiIiIh0BFtORERERBLGhIaIiIgkjy0nIiIiHdGaW05MaIiIiHREa371AVtOREREJHms0BAREekItpyIiIhI8lrzqw/YciIiIiLJY4WGiIhIV7TiEg0TGiIiIh3BVU5EREREEsYKDRERkY7gKiciIiKSvFY8hYYJDRERkc5oxRkN59AQERGR5LFCQ0REpCNa8yonJjREREQ6gpOC6YklCAIAoKTkTgtHQtR0hMp7LR0CUZOp/fNd+/O8KRUXF7fo+S2JCc0T7s6d+4nMgO6dWjgSIiLSxp07d6BQKJrk2kZGRlAqlXB9xknraymVShgZGTVCVM1LJjRHykgNVlNTg+vXr8PS0hIyKdcCJaS4uBhOTk7Izc2FXC5v6XCIGhX/fDc/QRBw584dODo6Qk+v6dbilJWVoaKiQuvrGBkZwcTEpBEial6s0Dzh9PT00K5du5YOo1WSy+X8gU86i3++m1dTVWb+zMTERJKJSGPhsm0iIiKSPCY0REREJHlMaIj+wtjYGAsWLICxsXFLh0LU6Pjnm3QVJwUTERGR5LFCQ0RERJLHhIaIiIgkjwkNERERSR4TGqJGJAgCJkyYAGtra8hkMqSlpbV0SPSEGjBgAKZNm9bSYdTp4sWL6NOnD0xMTNC1a1dcvXr1sX+eExMTIZPJUFhY2GxxEv0ZH6xH1Ij279+P2NhYJCYmon379rC1tW3pkIg0tmDBApibmyMzMxMWFhawsrJCXl4e/zzTE40JDVEjys7OhoODA55//vmWDoWowbKzsxEQEAAXFxdxn1KpbMGIiB6PLSfSKbt27YKnpydMTU1hY2MDHx8flJaWIjg4GCNHjsSiRYvw1FNPQS6XIywsTO29J+Xl5ZgyZQrs7OxgYmKCF154AadOnVK7flJSEnr16gVjY2M4ODhg9uzZqKqqAgAEBwdj8uTJyMnJgUwmw9NPP92cX50kqKqqChEREVAoFLC1tcW8efPENzLfvn0b48aNQ5s2bWBmZgY/Pz9cvnxZPDc2NhZWVlY4cOAA3N3dYWFhgaFDhyIvL08cc+rUKbz00kuwtbWFQqFA//79cebMmUfGJJPJkJqaiqioKMhkMixcuLDOllN8fDw6deoEU1NTDBw4EFevXn3gWseOHcOLL74IU1NTODk5YcqUKSgtLdXuN43oYQQiHXH9+nXBwMBA+Oijj4QrV64I586dE2JiYoQ7d+4IQUFBgoWFhfDKK68IP//8sxAXFyc89dRTwrvvviueP2XKFMHR0VGIj48XLly4IAQFBQlt2rQRbt68KQiCIFy7dk0wMzMTJk2aJGRkZAh79uwRbG1thQULFgiCIAiFhYVCVFSU0K5dOyEvL08oKChoid8Gkoj+/fsLFhYWwtSpU4WLFy8K27ZtE8zMzIRPP/1UEARBGD58uODu7i4cOXJESEtLE3x9fYWOHTsKFRUVgiAIwqZNmwRDQ0PBx8dHOHXqlJCamiq4u7sLr7/+uniPw4cPC1u3bhUyMjKE9PR0ISQkRLC3txeKi4sfGldeXp7w7LPPCm+//baQl5cn3LlzR7hy5YoAQPjpp58EQRCEnJwcwdjYWJgxY4YYu729vQBAuH37tiAIgpCVlSWYm5sLK1euFC5duiQcP35c6NatmxAcHNw0v6HU6jGhIZ2RmpoqABCuXr36wLGgoCDB2tpaKC0tFfetW7dOsLCwEKqrq4WSkhLB0NBQ2L59u3i8oqJCcHR0FJYvXy4IgiC8++67gpubm1BTUyOOiYmJEa8hCIKwcuVKwcXFpYm+IemS/v37C+7u7mp/nmbNmiW4u7sLly5dEgAIx48fF4/9/vvvgqmpqfDVV18JgnA/oQEgZGVliWNiYmIEe3v7h96zurpasLS0FPbt2/fI2Ly9vcVEXRCEBxKaOXPmCB4eHmrnzJo1Sy2hCQkJESZMmKA25ujRo4Kenp5w7969R96fqCHYciKd4e3tjcGDB8PT0xP/+Mc/sGHDBty+fVvtuJmZmfhZpVKhpKQEubm5yM7ORmVlJfr27SseNzQ0RK9evZCRkQEAyMjIgEqlgkwmE8f07dsXJSUluHbtWjN8Q9I1ffr0UfvzpFKpcPnyZaSnp8PAwAC9e/cWj9nY2MDNzU388wgAZmZm6NChg/jZwcEBBQUF4ucbN24gNDQUrq6uUCgUkMvlKCkpQU5ODgAgLCwMFhYW4lZfGRkZarHVxv5nZ8+eRWxsrNr1fX19UVNTgytXrtT7XkT1xUnBpDP09fWRkJCA5ORkHDx4EGvXrsV7772HkydPtnRoRE3C0NBQ7bNMJhPn4ABAUFAQbt68idWrV8PFxQXGxsZQqVTi3LGoqCjMnDmzSWIrKSnBW2+9hSlTpjxwzNnZuUnuSa0bExrSKTKZDH379kXfvn0xf/58uLi4YM+ePQDu/x/jvXv3YGpqCgA4ceIELCws4OTkBFtbWxgZGeH48ePiyo7KykqcOnVKfFaIu7s7du/eDUEQxP+rPn78OCwtLdGuXbvm/7IkeX9Ntk+cOAFXV1d4eHigqqoKJ0+eFFfM3bx5E5mZmfDw8Kj39Y8fP46PP/4Y/v7+AIDc3Fz8/vvv4nE7OzvY2dlpHLe7uzu+/fbbB2L/s+7duyM9PR0dO3bU+PpEDcGWE+mMkydPYunSpTh9+jRycnLw9ddf47fffoO7uzsAoKKiAiEhIUhPT0d8fDwWLFiAiIgI6OnpwdzcHBMnTkRkZCT279+P9PR0hIaG4u7duwgJCQEATJo0Cbm5uZg8eTIuXryIb775BgsWLMCMGTOgp8e/SqS5nJwczJgxA5mZmfjiiy+wdu1aTJ06Fa6urhgxYgRCQ0Nx7NgxnD17FmPGjEHbtm0xYsSIel/f1dUVW7duRUZGBk6ePInAwEAxoddGWFgYLl++jMjISGRmZmLHjh2IjY1VGzNr1iwkJycjIiICaWlpuHz5Mr755htERERofX+iuvCnMOkMuVyOI0eOwN/fH506dcLcuXOxYsUK+Pn5AQAGDx4MV1dX9OvXD6+88gqGDx+OhQsXiucvW7YMo0ePxtixY9G9e3dkZWXhwIEDaNOmDQCgbdu2iI+Px48//ghvb2+EhYUhJCQEc+fObYmvSzpg3LhxuHfvHnr16oXw8HBMnToVEyZMAABs2rQJPXr0wMsvvwyVSgVBEBAfH/9Am+lRNm7ciNu3b6N79+4YO3as+FgCbTk7O2P37t3Yu3cvvL29sX79eixdulRtjJeXF5KSknDp0iW8+OKL6NatG+bPnw9HR0et709UF5nw54YrkY4KDg5GYWEh9u7d29KhEBFRE2CFhoiIiCSPCQ0RERFJHltOREREJHms0BAREZHkMaEhIiIiyWNCQ0RERJLHhIaIiIgkjwkNERERSR4TGiJ6rODgYIwcOVL8PGDAAPEdV80pMTERMpkMhYWFDx0jk8k0eoDiwoUL0bVrV63iunr1KmQyGdLS0rS6DhE1HBMaIokKDg6GTCaDTCaDkZEROnbsiKioKFRVVTX5vb/++mssXry4XmPrk4QQEWmLb9smkrChQ4di06ZNKC8vR3x8PMLDw2FoaIg5c+Y8MLaiogJGRkaNcl9ra+tGuQ4RUWNhhYZIwoyNjaFUKuHi4oKJEyfCx8cH3377LYD/tYmWLFkCR0dHuLm5AQByc3Pxz3/+E1ZWVrC2tsaIESNw9epV8ZrV1dWYMWMGrKysYGNjg3feeQd/ff7mX1tO5eXlmDVrFpycnGBsbIyOHTti48aNuHr1KgYOHAgAaNOmDWQyGYKDgwEANTU1iI6OxjPPPANTU1N4e3tj165daveJj49Hp06dYGpqioEDB6rFWV+zZs1Cp06dYGZmhvbt22PevHmorKx8YNwnn3wCJycnmJmZ4Z///CeKiorUjn/22Wdwd3eHiYkJOnfujI8//ljjWIio6TChIdIhpqamqKioED8fPnwYmZmZSEhIQFxcHCorK+Hr6wtLS0scPXoUx48fh4WFBYYOHSqet2LFCsTGxuLzzz/HsWPHcOvWLezZs+eR9x03bhy++OILrFmzBhkZGfjkk09gYWEBJycn7N69GwCQmZmJvLw8rF69GgAQHR2NLVu2YP369bhw4QKmT5+OMWPGICkpCcD9xGvUqFEYNmwY0tLSMH78eMyePVvj3xNLS0vExsYiPT0dq1evxoYNG7By5Uq1MVlZWfjqq6+wb98+7N+/Hz/99BMmTZokHt++fTvmz5+PJUuWICMjA0uXLsW8efOwefNmjeMhoiYiEJEkBQUFCSNGjBAEQRBqamqEhIQEwdjYWJg5c6Z43N7eXigvLxfP2bp1q+Dm5ibU1NSI+8rLywVTU1PhwIEDgiAIgoODg7B8+XLxeGVlpdCuXTvxXoIgCP379xemTp0qCIIgZGZmCgCEhISEOuP84YcfBADC7du3xX1lZWWCmZmZkJycrDY2JCREeO211wRBEIQ5c+YIHh4easdnzZr1wLX+CoCwZ8+ehx7/17/+JfTo0UP8vGDBAkFfX1+4du2auO+7774T9PT0hLy8PEEQBKFDhw7Cjh071K6zePFiQaVSCYIgCFeuXBEACD/99NND70tETYtzaIgkLC4uDhYWFqisrERNTQ1ef/11LFy4UDzu6empNm/m7NmzyMrKgqWlpdp1ysrKkJ2djaKiIuTl5aF3797iMQMDA/Ts2fOBtlOttLQ06Ovro3///vWOOysrC3fv3sVLL72ktr+iogLdunUDAGRkZKjFAQAqlare96j15ZdfYs2aNcjOzkZJSQmqqqogl8vVxjg7O6Nt27Zq96mpqUFmZiYsLS2RnZ2NkJAQhIaGimOqqqqgUCg0joeImgYTGiIJGzhwINatWwcjIyM4OjrCwED9r7S5ubna55KSEvTo0QPbt29/4FpPPfVUg2IwNTXV+JySkhIAwH//+1+1RAK4Py+osaSkpCAwMBCLFi2Cr68vFAoFdu7ciRUrVmgc64YNGx5IsPT19RstViLSDhMaIgkzNzdHx44d6z2+e/fu+PLLL2FnZ/dAlaKWg4MDTp48iX79+gG4X4lITU1F9+7d6xzv6emJmpoaJCUlwcfH54HjtRWi6upqcZ+HhweMjY2Rk5Pz0MqOu7u7OMG51okTJx7/Jf8kOTkZLi4ueO+998R9//d///fAuJycHFy/fh2Ojo7iffT09ODm5gZ7e3s4Ojril19+QWBgoEb3J6Lmw0nBRK1IYGAgbG1tMWLECBw9ehRXrlxBYmIipkyZgmvXrgEApk6dimXLlmHv3r24ePEiJk2a9MhnyDz99NMICgrCm2++ib1794rX/OqrrwAALi4ukMlkiIuLw2+//YaSkhJYWlpi5syZmD59OjZv3ozs7GycOXMGa9euFSfahoWF4fLly4iMjERmZiZ27NiB2NhYjb6vq6srcnJysHPnTmRnZ2PNmjV1TnA2MTFBUFAQzp49i6NHj2LKlCn45z//CaVSCQBYtGgRoqOjsWbNGly6dAnnz5/Hpk2b8NFHH2kUDxE1HSY0RK2ImZkZjhw5AmdnZ4waNQru7u4ICQlBWVmZWLF5++23MXbsWAQFBUGlUsHS0hJ/+9vfHnnddevW4e9//zsmTZqEzp07IzQ0FKWlpQCAtm3bYtGiRZg9ezbs7e0REREBAFi8eDHmzZuH6OhouLu7Y+jQofjvf/+LZ555BsD9eS27d+/G3r174e3tjfXr12Pp0qUafd/hw4dj+vTpiIiIQNeuXZGcnIx58+Y9MK5jx44YNWoU/P39MWTIEHh5eaktyx4/fjw+++wzbNq0CZ6enujfvz9iY2PFWImo5cmEh830IyIiIpIIVmiIiIhI8pjQEBERkeQxoSEiIiLJY0JDREREkseEhoiIiCSPCQ0RERFJHhMaIiIikjwmNERERCR5TGiIiIhI8pjQEBERkeQxoSEiIiLJ+38V1nzf8eXplAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#USE THIS TO PREPROCESS,EXTRACT AND GENERATE CSV \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0,\n",
    "                         apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"testing\"]:\n",
    "        for label_dir in [\"real\", \"fake\"]:\n",
    "            input_folder = input_root / split / label_dir\n",
    "            output_base = output_root / f\"preprocessed_{split}\" / label_dir\n",
    "\n",
    "            print(f\"Looking in: {input_folder}\")\n",
    "            wav_files = sorted(list(input_folder.glob(\"*.wav\")))\n",
    "            print(f\"Found {len(wav_files)} files in '{split}/{label_dir}'\")\n",
    "\n",
    "            for wav_file in tqdm(wav_files, desc=f\"Processing {label_dir}\"):\n",
    "                try:\n",
    "                    y, _ = librosa.load(wav_file, sr=sr)\n",
    "                    y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                    clean_name = wav_file.stem.split(\".\")[0] + \".npy\"\n",
    "\n",
    "                    feature_dict = {\n",
    "                        \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                        \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                        \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                        \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                        \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                        \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                        \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                        \"energy\": librosa.feature.rms(y=y),\n",
    "                        \"zcr\": librosa.feature.zero_crossing_rate(y=y),\n",
    "                        \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                    }\n",
    "\n",
    "                    for feature_name, data in feature_dict.items():\n",
    "                        out_path = output_base / feature_name / clean_name\n",
    "                        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "extract_and_save_all(\n",
    "    input_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    output_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    sr=16000, target_duration=6.0\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def generate_test_meta_csv(real_dir, fake_dir, save_path):\n",
    "    entries = []\n",
    "    for path in sorted(Path(real_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]  \n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 1})\n",
    "    for path in sorted(Path(fake_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]\n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 0})\n",
    "    pd.DataFrame(entries).to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "generate_test_meta_csv(\n",
    "    real_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/real\",\n",
    "    fake_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/fake\",\n",
    "    save_path=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\"\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):\n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int) \n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {1: 1, 0: 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:target_shape[0], :target_shape[1]]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        label_dir = \"real\" if label == 1 else \"fake\"\n",
    "\n",
    "        # Construct the file path\n",
    "        file_id = row[\"file\"]\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, label_dir, feat, file_id)\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "            arr = np.load(path)\n",
    "            tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self._pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "for_2sec_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/for-2sec/for-2seconds/preprocessed_testing\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(model, for_2sec_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of ASVspoof2019 LA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 71237 rows to datasets/evaluation/LA/LA_eval_meta.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_la_eval_metadata(\n",
    "    protocol_path: str,\n",
    "    audio_dir: str,\n",
    "    output_csv: str\n",
    "):\n",
    "    protocol = Path(protocol_path)\n",
    "    if not protocol.exists():\n",
    "        raise FileNotFoundError(f\"Cannot find protocol file: {protocol}\")\n",
    "\n",
    "    entries = []\n",
    "    with protocol.open('r') as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split()\n",
    "            if len(cols) != 5:\n",
    "                continue\n",
    "            speaker, audio_id, system_id, dash, key = cols\n",
    "            filename = audio_id + \".flac\"\n",
    "            audio_path = Path(audio_dir) / filename\n",
    "            if not audio_path.exists():\n",
    "                print(f\"[WARNING] Audio missing: {audio_path}\")\n",
    "                continue\n",
    "            label = 1 if key.lower() == \"bonafide\" else 0\n",
    "            entries.append({\n",
    "                \"file\":    filename,\n",
    "                \"speaker\": speaker,\n",
    "                \"label\":   label\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(entries, columns=[\"file\",\"speaker\",\"label\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "make_la_eval_metadata(\n",
    "    protocol_path=\"datasets/evaluation/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "    audio_dir=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",          \n",
    "    output_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract LA features:  70%|███████   | 49890/71237 [4:33:18<1:51:54,  3.18file/s, ETA 7017s]  "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_la_features_with_eta(\n",
    "    meta_csv: str,\n",
    "    audio_root: str,\n",
    "    output_root: str,\n",
    "    sr: int = 16000,\n",
    "    target_duration: float = 6.0,\n",
    "    apply_preemphasis: bool = False,\n",
    "    coef: float = 0.5,\n",
    "    normalise: str = \"rms\",\n",
    "):\n",
    "    df = pd.read_csv(meta_csv)     \n",
    "    audio_root  = Path(audio_root)\n",
    "    output_base = Path(output_root) / \"preprocessed_eval\"\n",
    "\n",
    "    total_files = len(df)\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm(df.iterrows(),\n",
    "                total=total_files,\n",
    "                desc=\"Extract LA features\",\n",
    "                unit=\"file\")\n",
    "\n",
    "    for idx, (_, row) in enumerate(pbar):\n",
    "        filename = row[\"file\"]\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "        in_path   = audio_root / filename\n",
    "\n",
    "        y, _ = librosa.load(str(in_path), sr=sr)\n",
    "        y = preprocess_audio(y, sr, target_duration,\n",
    "                             apply_preemphasis, coef, normalise)\n",
    "\n",
    "        feats = {\n",
    "            \"mel_spectrogram\":   librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "            \"mfcc\":              librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "            \"chroma\":            librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "            \"tonnetz\":           librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "            \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "            \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "            \"pitch\":             librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "            \"energy\":            librosa.feature.rms(y=y),\n",
    "            \"zcr\":               librosa.feature.zero_crossing_rate(y=y),\n",
    "            \"onset_strength\":    librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        }\n",
    "\n",
    "        npy_name = Path(filename).stem + \".npy\"\n",
    "        for feat_name, arr in feats.items():\n",
    "            out_dir = output_base / label_dir / feat_name\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(out_dir / npy_name, arr.astype(np.float32))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_per_file = elapsed / (idx + 1)\n",
    "        remaining = total_files - (idx + 1)\n",
    "        eta = remaining * avg_per_file\n",
    "        pbar.set_postfix_str(f\"ETA {eta:.0f}s\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "extract_and_save_la_features_with_eta(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    audio_root=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",\n",
    "    output_root=\"datasets/evaluation/LA\",\n",
    "    sr=22050,\n",
    "    target_duration=6.0,\n",
    "    apply_preemphasis=False,\n",
    "    coef=0.5,\n",
    "    normalise=\"rms\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=[\n",
    "                     'mel_spectrogram','mfcc','chroma','tonnetz',\n",
    "                     'spectral_contrast','spectral_centroid',\n",
    "                     'pitch','energy','zcr','onset_strength'\n",
    "                 ],\n",
    "                 target_shape=(128, 259)):\n",
    "                \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.features = features\n",
    "        self.target_shape = target_shape\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        th, tw = target_shape\n",
    "        if th is None: th = h\n",
    "        if tw is None: tw = w\n",
    "        pad_h = th - h\n",
    "        pad_w = tw - w\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:th, :tw]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = Path(row[\"file\"]).stem + \".npy\"\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "\n",
    "        feature_tensors = []\n",
    "        for feat in self.features:\n",
    "            path = self.feature_root / \"preprocessed_eval\" / label_dir / feat / file_id\n",
    "            if not path.exists():\n",
    "                raise FileNotFoundError(f\"Missing {path}\")\n",
    "            arr = np.load(path)\n",
    "            t = torch.tensor(arr, dtype=torch.float32)\n",
    "            if t.dim() == 1:\n",
    "                t = t.unsqueeze(0)\n",
    "            t = self._pad_or_resize(t, self.target_shape)\n",
    "            feature_tensors.append(t)\n",
    "\n",
    "        return (*feature_tensors, torch.tensor(row[\"label\"], dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset,\n",
    "                         batch_size=32,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            *features, labels = batch\n",
    "            features = [f.to(device) for f in features]\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(*features)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds  = np.array(all_preds)\n",
    "    all_probs  = np.array(all_probs)\n",
    "\n",
    "    # compute metrics\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc  = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    eer = brentq(lambda x: 1.-x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"spoof\",\"bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aadil\\AppData\\Local\\Temp\\ipykernel_33632\\2307706330.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"df_model.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing datasets\\evaluation\\LA\\preprocessed_eval\\fake\\mfcc\\LA_E_1611480.npy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 1) Pull out one batch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m debug_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 23\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdebug_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 4) run evaluation\u001b[39;00m\n\u001b[0;32m     26\u001b[0m evaluate_on_test_set(model, dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aadil\\anaconda3\\envs\\final-voice-system-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Aadil\\anaconda3\\envs\\final-voice-system-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Aadil\\anaconda3\\envs\\final-voice-system-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Aadil\\anaconda3\\envs\\final-voice-system-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[74], line 48\u001b[0m, in \u001b[0;36mAudioFeatureDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     46\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m label_dir \u001b[38;5;241m/\u001b[39m feat \u001b[38;5;241m/\u001b[39m file_id\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m     50\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(arr, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Missing datasets\\evaluation\\LA\\preprocessed_eval\\fake\\mfcc\\LA_E_1611480.npy"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"df_model.pth\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/LA\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "debug_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "batch = next(iter(debug_loader))\n",
    "\n",
    "evaluate_on_test_set(model, dataset, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
