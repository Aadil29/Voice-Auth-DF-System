{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         # should return True\n",
    "print(torch.cuda.get_device_name(0))     # should say \"NVIDIA GeForce RTX 3060\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 40% of dataset sampled and split into train/val/test (70/15/15).\n",
      "✔ Spoof samples oversampled in train set only.\n",
      "Train files: 8246\n",
      "Val files: 1907\n",
      "Test files: 1907\n",
      "\n",
      "Label Distribution:\n",
      "Train:\n",
      " label\n",
      "spoof        5589\n",
      "bona-fide    5589\n",
      "Name: count, dtype: int64\n",
      "Val:\n",
      " label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n",
      "Test:\n",
      " label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "BASE_DIR = \"datasets/release_in_the_wild\"\n",
    "META_CSV = os.path.join(BASE_DIR, \"meta.csv\")\n",
    "WAV_DIR = BASE_DIR\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"train\": os.path.join(BASE_DIR, \"train\"),\n",
    "    \"val\": os.path.join(BASE_DIR, \"val\"),\n",
    "    \"test\": os.path.join(BASE_DIR, \"test\")\n",
    "}\n",
    "\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df[\"file\"] = df[\"file\"].str.strip()\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"speaker\"] = df[\"speaker\"].str.strip()\n",
    "\n",
    "df_40_percent, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=0.4,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_40_percent,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_40_percent[\"label\"]\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "spoof_train = train_df[train_df[\"label\"] == \"spoof\"]\n",
    "bonafide_train = train_df[train_df[\"label\"] == \"bona-fide\"]\n",
    "\n",
    "spoof_upsampled = resample(\n",
    "    spoof_train,\n",
    "    replace=True,\n",
    "    n_samples=len(bonafide_train),\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = pd.concat([bonafide_train, spoof_upsampled]).sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "#copy not move \n",
    "def move_files(subset_df, split_name):\n",
    "    for _, row in subset_df.iterrows():\n",
    "        src = os.path.join(WAV_DIR, row[\"file\"])\n",
    "        dst = os.path.join(OUTPUT_DIRS[split_name], row[\"file\"])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# Move files\n",
    "#move_files(train_df, \"train\")\n",
    "#move_files(val_df, \"val\")\n",
    "#move_files(test_df, \"test\")\n",
    "\n",
    "# Save metadata\n",
    "train_df.to_csv(os.path.join(BASE_DIR, \"train_meta.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(BASE_DIR, \"val_meta.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(BASE_DIR, \"test_meta.csv\"), index=False)\n",
    "\n",
    "# Summary\n",
    "def count_files(folder_path):\n",
    "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "print(\"✔ 40% of dataset sampled and split into train/val/test (70/15/15).\")\n",
    "print(\"✔ Spoof samples oversampled in train set only.\")\n",
    "print(f\"Train files: {count_files(OUTPUT_DIRS['train'])}\")\n",
    "print(f\"Val files: {count_files(OUTPUT_DIRS['val'])}\")\n",
    "print(f\"Test files: {count_files(OUTPUT_DIRS['test'])}\")\n",
    "\n",
    "# Show label distribution in each set\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# Preprocess raw audio\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# Main extraction + saving\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "# Run the feature extraction\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0,\n",
    "                         apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    splits = [\"test\"] \n",
    "\n",
    "    for split in splits:\n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"\\nLooking in: {input_folder}\")\n",
    "        wav_files = sorted(input_folder.glob(\"*.wav\"))\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "                \n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio(path, sr=22050):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "def show_waveform(y, sr, title=\"Waveform\"):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mel_spectrogram(y, sr, title=\"Mel Spectrogram\"):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_audio(original_path, fake_path, apply_preprocessing=False, sr=22050):\n",
    "    print(f\" Original: {original_path}\")\n",
    "    y_real = load_audio(original_path, sr=sr)\n",
    "    y_fake = load_audio(fake_path, sr=sr)\n",
    "\n",
    "    if apply_preprocessing:\n",
    "        y_real = preprocess_audio(y_real, sr, apply_preemphasis=True, normalise='rms')\n",
    "        y_fake = preprocess_audio(y_fake, sr, apply_preemphasis=True, normalise='rms')\n",
    "\n",
    "    # Waveform comparison\n",
    "    show_waveform(y_real, sr, title=\"Real - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "\n",
    "    show_waveform(y_fake, sr, title=\"Fake - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_fake, rate=sr))\n",
    "\n",
    "    # Mel spectrogram comparison\n",
    "    show_mel_spectrogram(y_real, sr, title=\"Real - Mel Spectrogram\")\n",
    "    show_mel_spectrogram(y_fake, sr, title=\"Fake - Mel Spectrogram\")\n",
    "\n",
    "real_audio_path = \"datasets/release_in_the_wild/remaining_files/7.wav\"\n",
    "\n",
    "fake_audio_path = \"datasets/release_in_the_wild/remaining_files/5.wav\"\n",
    "\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=False)\n",
    "\n",
    "# compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):  \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].str.strip().str.lower()\n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {'bona-fide': 1, 'spoof': 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            tensor = tensor[:target_shape[0], :target_shape[1]] \n",
    "        else:\n",
    "            tensor = F.pad(tensor, (0, pad_w, 0, pad_h))  \n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, feat, file_id) \n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "            feature = np.load(path)\n",
    "            tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self.pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: CHROMA \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\chroma\\1000.npy\n",
      "Raw .npy shape: (12, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: ENERGY \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\energy\\1000.npy\n",
      "Raw .npy shape: (1, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: MEL_SPECTROGRAM \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\mel_spectrogram\\1000.npy\n",
      "Raw .npy shape: (128, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: MFCC \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\mfcc\\1000.npy\n",
      "Raw .npy shape: (20, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: ONSET_STRENGTH \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\onset_strength\\1000.npy\n",
      "Raw .npy shape: (259,)\n",
      "Tensor shape after unsqueeze: torch.Size([1, 259])\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: PITCH \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\pitch\\1000.npy\n",
      "Raw .npy shape: (259,)\n",
      "Tensor shape after unsqueeze: torch.Size([1, 259])\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: SPECTRAL_CENTROID \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\spectral_centroid\\1000.npy\n",
      "Raw .npy shape: (1, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: SPECTRAL_CONTRAST \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\spectral_contrast\\1000.npy\n",
      "Raw .npy shape: (7, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: TONNETZ \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\tonnetz\\1000.npy\n",
      "Raw .npy shape: (6, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Feature: ZCR \n",
      "File path: datasets/release_in_the_wild/preprocessed_train\\zcr\\1000.npy\n",
      "Raw .npy shape: (1, 259)\n",
      "Shape after pad_or_resize: torch.Size([128, 259])\n",
      "\n",
      "Done checking all feature folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "feature_root = \"datasets/release_in_the_wild/preprocessed_train\"\n",
    "features = ['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "            'onset_strength', 'pitch', 'spectral_centroid',\n",
    "            'spectral_contrast', 'tonnetz', 'zcr']\n",
    "target_shape = (128, 259)  \n",
    "\n",
    "def pad_or_resize(tensor, target_shape):\n",
    "    h, w = tensor.shape\n",
    "    pad_h = target_shape[0] - h\n",
    "    pad_w = target_shape[1] - w\n",
    "\n",
    "    if pad_h < 0 or pad_w < 0:\n",
    "        tensor = tensor[:target_shape[0], :target_shape[1]]\n",
    "    else:\n",
    "        tensor = F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "    return tensor\n",
    "\n",
    "for feat in features:\n",
    "    folder_path = os.path.join(feature_root, feat)\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    file_list = os.listdir(folder_path)\n",
    "    if not file_list:\n",
    "        print(f\"No files found in: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    file_name = file_list[0]\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    try:\n",
    "        arr = np.load(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nFeature: {feat.upper()} \")\n",
    "    print(f\"File path: {file_path}\")\n",
    "    print(f\"Raw .npy shape: {arr.shape}\")\n",
    "\n",
    "    tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "    if tensor.dim() == 1:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        print(f\"Tensor shape after unsqueeze: {tensor.shape}\")\n",
    "    \n",
    "    processed_tensor = pad_or_resize(tensor, target_shape)\n",
    "    print(f\"Shape after pad_or_resize: {processed_tensor.shape}\")\n",
    "\n",
    "print(\"\\nDone checking all feature folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SiameseMFCCBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMFCCBranch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.flattened_size = 128 * 32 * 64  \n",
    "        self.fc = nn.Linear(self.flattened_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))       \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.fc(x)                   \n",
    "        return x\n",
    "\n",
    "\n",
    "# Final Fusion Model\n",
    "class AudioDeepfakeFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioDeepfakeFusionModel, self).__init__()\n",
    "\n",
    "        self.mfcc_branch = SiameseMFCCBranch()\n",
    "\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.pitch_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.energy_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.zcr_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.onset_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.centroid_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.mel_spec_branch = DenseNeuralNetwork(input_dim=128)\n",
    "\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(10 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, mfcc, chroma, tonnetz, contrast, pitch, energy, zcr, onset, centroid, mel_spec):\n",
    "        mfcc = mfcc.unsqueeze(1)  \n",
    "\n",
    "        def pool(x): return x.mean(dim=-1)  \n",
    "\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(pool(chroma))\n",
    "        tonnetz_out = self.tonnetz_branch(pool(tonnetz))\n",
    "        contrast_out = self.contrast_branch(pool(contrast))\n",
    "        pitch_out = self.pitch_branch(pool(pitch))\n",
    "        energy_out = self.energy_branch(pool(energy))\n",
    "        zcr_out = self.zcr_branch(pool(zcr))\n",
    "        onset_out = self.onset_branch(pool(onset))\n",
    "        centroid_out = self.centroid_branch(pool(centroid))\n",
    "        mel_spec_out = self.mel_spec_branch(pool(mel_spec))\n",
    "\n",
    "        fusion = torch.cat([\n",
    "            mfcc_out, chroma_out, tonnetz_out, contrast_out,\n",
    "            pitch_out, energy_out, zcr_out, onset_out, centroid_out, mel_spec_out\n",
    "        ], dim=1)\n",
    "\n",
    "        x = self.fusion_layer(fusion)\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                all_preds += outputs.cpu().numpy().flatten().tolist()\n",
    "                all_labels += labels.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        all_preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        acc = np.mean(np.array(all_preds_bin) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds_bin)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "       # history['val_loss'].append(val_loss / len(val_loader))\n",
    "        #history['val_acc'].append(acc)\n",
    "        #history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"[{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, #Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return model, history, all_labels, all_preds_bin\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    #plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Accuracy')\n",
    "    #plt.plot(history['val_f1'], label='F1 Score')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1 Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Spoof', 'Bona-fide'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_binary_classification_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=20, batch_size=32, learning_rate=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_prec': [],\n",
    "        'val_rec': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        train_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            train_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            train_trues.extend(labels.cpu().numpy())\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = [b.to(device) for b in batch[:-1]]\n",
    "                labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                val_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "                val_trues.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        acc = accuracy_score(val_trues, val_preds)\n",
    "        prec = precision_score(val_trues, val_preds)\n",
    "        rec = recall_score(val_trues, val_preds)\n",
    "        f1 = f1_score(val_trues, val_preds)\n",
    "\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(acc)\n",
    "        history['val_prec'].append(prec)\n",
    "        history['val_rec'].append(rec)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"\\n[Validation] Loss: {avg_val_loss:.4f} | Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"df_model.pth\")\n",
    "    print(\"\\n Model saved to 'df_model.pth'\")\n",
    "\n",
    "    # Plot metric graphs\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Show confusion matrix for both splits\n",
    "    show_confusion_matrix(train_trues, train_preds, title=\"Train Set\")\n",
    "    show_confusion_matrix(val_trues, val_preds, title=\"Validation Set\")\n",
    "\n",
    "\n",
    "def show_confusion_matrix(true_labels, pred_labels, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(history['val_loss'], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    # Accuracy & F1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_acc'], label=\"Accuracy\")\n",
    "    plt.plot(history['val_f1'], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1\")\n",
    "\n",
    "    # Precision & Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['val_prec'], label=\"Precision\")\n",
    "    plt.plot(history['val_rec'], label=\"Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Precision & Recall\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN SPLIT:\n",
      "label\n",
      "spoof        5589\n",
      "bona-fide    5589\n",
      "Name: count, dtype: int64\n",
      "Total: 11178\n",
      "\n",
      "VAL SPLIT:\n",
      "label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n",
      "Total: 1907\n",
      "\n",
      "TEST SPLIT:\n",
      "label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n",
      "Total: 1907\n",
      "\n",
      "TRAIN SPLIT:\n",
      "label\n",
      "spoof        5589\n",
      "bona-fide    5589\n",
      "Name: count, dtype: int64\n",
      "Total: 11178\n",
      "\n",
      "VAL SPLIT:\n",
      "label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n",
      "Total: 1907\n",
      "\n",
      "TEST SPLIT:\n",
      "label\n",
      "bona-fide    1198\n",
      "spoof         709\n",
      "Name: count, dtype: int64\n",
      "Total: 1907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"datasets/release_in_the_wild\")\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    csv_path = root / f\"{split}_meta.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARNING] CSV not found: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"label\"] = df[\"label\"].str.strip().str.lower()  \n",
    "\n",
    "    print(f\"\\n{split.upper()} SPLIT:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "    print(\"Total:\", len(df))\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"datasets/release_in_the_wild\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 350/350 [01:03<00:00,  5.47it/s, Loss=0.516] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.4075 | Acc: 0.8002 | Precision: 0.7587 | Recall: 1.0000 | F1: 0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 350/350 [01:03<00:00,  5.50it/s, Loss=0.0254] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0717 | Acc: 0.9754 | Precision: 0.9800 | Recall: 0.9808 | F1: 0.9804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 350/350 [01:03<00:00,  5.53it/s, Loss=0.841] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0891 | Acc: 0.9759 | Precision: 0.9737 | Recall: 0.9883 | F1: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 350/350 [01:03<00:00,  5.53it/s, Loss=0.0952] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.1165 | Acc: 0.9497 | Precision: 0.9298 | Recall: 0.9950 | F1: 0.9613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 350/350 [01:03<00:00,  5.53it/s, Loss=0.0406] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0856 | Acc: 0.9738 | Precision: 0.9931 | Recall: 0.9649 | F1: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 350/350 [01:03<00:00,  5.51it/s, Loss=0.0835] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0937 | Acc: 0.9738 | Precision: 0.9940 | Recall: 0.9641 | F1: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 350/350 [01:03<00:00,  5.54it/s, Loss=0.794]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0714 | Acc: 0.9706 | Precision: 0.9948 | Recall: 0.9583 | F1: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 350/350 [01:03<00:00,  5.54it/s, Loss=0.0521] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0801 | Acc: 0.9717 | Precision: 0.9948 | Recall: 0.9599 | F1: 0.9771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 350/350 [01:03<00:00,  5.55it/s, Loss=0.0163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0828 | Acc: 0.9727 | Precision: 0.9614 | Recall: 0.9967 | F1: 0.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 350/350 [01:03<00:00,  5.54it/s, Loss=0.0739] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0703 | Acc: 0.9848 | Precision: 0.9851 | Recall: 0.9908 | F1: 0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 350/350 [01:03<00:00,  5.49it/s, Loss=0.14]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0769 | Acc: 0.9670 | Precision: 0.9974 | Recall: 0.9499 | F1: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 350/350 [01:03<00:00,  5.51it/s, Loss=0.673]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0785 | Acc: 0.9680 | Precision: 0.9965 | Recall: 0.9524 | F1: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 350/350 [01:03<00:00,  5.51it/s, Loss=0.897]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0780 | Acc: 0.9727 | Precision: 0.9584 | Recall: 1.0000 | F1: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 350/350 [01:03<00:00,  5.50it/s, Loss=0.338]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0998 | Acc: 0.9570 | Precision: 0.9973 | Recall: 0.9341 | F1: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 350/350 [01:04<00:00,  5.41it/s, Loss=0.459]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0743 | Acc: 0.9806 | Precision: 0.9949 | Recall: 0.9741 | F1: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 350/350 [01:03<00:00,  5.51it/s, Loss=0.00808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0433 | Acc: 0.9869 | Precision: 0.9803 | Recall: 0.9992 | F1: 0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 350/350 [01:04<00:00,  5.47it/s, Loss=0.646]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0599 | Acc: 0.9822 | Precision: 0.9974 | Recall: 0.9741 | F1: 0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 350/350 [01:05<00:00,  5.38it/s, Loss=0.176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0725 | Acc: 0.9706 | Precision: 0.9561 | Recall: 0.9992 | F1: 0.9771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 350/350 [01:04<00:00,  5.40it/s, Loss=0.00536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0886 | Acc: 0.9654 | Precision: 0.9485 | Recall: 0.9992 | F1: 0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 350/350 [02:31<00:00,  2.31it/s, Loss=0.106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.1144 | Acc: 0.9455 | Precision: 0.9991 | Recall: 0.9140 | F1: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 350/350 [08:46<00:00,  1.50s/it, Loss=0.0058]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0849 | Acc: 0.9664 | Precision: 0.9974 | Recall: 0.9491 | F1: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 350/350 [02:43<00:00,  2.14it/s, Loss=0.699]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0629 | Acc: 0.9717 | Precision: 0.9605 | Recall: 0.9958 | F1: 0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 350/350 [01:07<00:00,  5.15it/s, Loss=0.021]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0527 | Acc: 0.9827 | Precision: 0.9786 | Recall: 0.9942 | F1: 0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 350/350 [01:03<00:00,  5.47it/s, Loss=0.00595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0504 | Acc: 0.9832 | Precision: 0.9875 | Recall: 0.9858 | F1: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 350/350 [01:04<00:00,  5.47it/s, Loss=0.0785]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0478 | Acc: 0.9853 | Precision: 0.9819 | Recall: 0.9950 | F1: 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 350/350 [01:04<00:00,  5.46it/s, Loss=0.0507]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0713 | Acc: 0.9675 | Precision: 0.9956 | Recall: 0.9524 | F1: 0.9735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 350/350 [01:04<00:00,  5.39it/s, Loss=0.385]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0917 | Acc: 0.9670 | Precision: 0.9974 | Recall: 0.9499 | F1: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 350/350 [01:03<00:00,  5.49it/s, Loss=0.000783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0518 | Acc: 0.9827 | Precision: 0.9883 | Recall: 0.9841 | F1: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 350/350 [01:03<00:00,  5.48it/s, Loss=0.35]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0444 | Acc: 0.9874 | Precision: 0.9859 | Recall: 0.9942 | F1: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 350/350 [01:07<00:00,  5.17it/s, Loss=0.000467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0543 | Acc: 0.9811 | Precision: 0.9907 | Recall: 0.9791 | F1: 0.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 350/350 [01:08<00:00,  5.15it/s, Loss=0.425]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0656 | Acc: 0.9769 | Precision: 0.9808 | Recall: 0.9825 | F1: 0.9817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 350/350 [01:07<00:00,  5.22it/s, Loss=0.753]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0495 | Acc: 0.9853 | Precision: 0.9974 | Recall: 0.9791 | F1: 0.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 350/350 [01:06<00:00,  5.27it/s, Loss=0.0259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0413 | Acc: 0.9890 | Precision: 0.9860 | Recall: 0.9967 | F1: 0.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 350/350 [01:06<00:00,  5.24it/s, Loss=0.0856]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0513 | Acc: 0.9811 | Precision: 0.9770 | Recall: 0.9933 | F1: 0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 350/350 [01:07<00:00,  5.22it/s, Loss=0.00292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0413 | Acc: 0.9837 | Precision: 0.9787 | Recall: 0.9958 | F1: 0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 350/350 [01:07<00:00,  5.22it/s, Loss=0.415]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0390 | Acc: 0.9853 | Precision: 0.9891 | Recall: 0.9875 | F1: 0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 350/350 [01:06<00:00,  5.29it/s, Loss=0.00439] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0832 | Acc: 0.9664 | Precision: 0.9982 | Recall: 0.9482 | F1: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 350/350 [01:08<00:00,  5.10it/s, Loss=0.0671]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0905 | Acc: 0.9654 | Precision: 0.9499 | Recall: 0.9975 | F1: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 350/350 [01:07<00:00,  5.20it/s, Loss=0.133]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0496 | Acc: 0.9853 | Precision: 0.9859 | Recall: 0.9908 | F1: 0.9883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 350/350 [01:07<00:00,  5.17it/s, Loss=0.123]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0780 | Acc: 0.9664 | Precision: 0.9551 | Recall: 0.9933 | F1: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 350/350 [01:06<00:00,  5.28it/s, Loss=8.8e-5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0428 | Acc: 0.9858 | Precision: 0.9900 | Recall: 0.9875 | F1: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 350/350 [01:05<00:00,  5.33it/s, Loss=0.0222]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0418 | Acc: 0.9869 | Precision: 0.9892 | Recall: 0.9900 | F1: 0.9896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 350/350 [01:11<00:00,  4.86it/s, Loss=0.00597] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0862 | Acc: 0.9717 | Precision: 0.9974 | Recall: 0.9574 | F1: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 350/350 [03:52<00:00,  1.50it/s, Loss=0.712]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0438 | Acc: 0.9837 | Precision: 0.9875 | Recall: 0.9866 | F1: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 350/350 [07:42<00:00,  1.32s/it, Loss=0.0245]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0635 | Acc: 0.9769 | Precision: 0.9661 | Recall: 0.9983 | F1: 0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 350/350 [02:41<00:00,  2.17it/s, Loss=0.000639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0483 | Acc: 0.9795 | Precision: 0.9708 | Recall: 0.9975 | F1: 0.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 350/350 [01:04<00:00,  5.44it/s, Loss=0.000193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0451 | Acc: 0.9864 | Precision: 0.9819 | Recall: 0.9967 | F1: 0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 350/350 [08:38<00:00,  1.48s/it, Loss=0.154]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] Loss: 0.0495 | Acc: 0.9816 | Precision: 0.9755 | Recall: 0.9958 | F1: 0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50:  82%|████████▏ | 287/350 [06:42<01:42,  1.63s/it, Loss=0.123]   "
     ]
    }
   ],
   "source": [
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "\n",
    "train_binary_classification_model(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0f096d1054c5f81241f20578de164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Accuracy:  0.9853\n",
      "Precision: 0.9795\n",
      "Recall:    0.9975\n",
      "F1 Score:  0.9884\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAHFCAYAAADyj/PrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVU1JREFUeJzt3XdYFNf6B/Dv0pa6K0VYUEDsWLEiEmMXifWnsUSTQOyxkmg0xkRQI5YYNWqsMUKsMbFcY9QY673GhkasBBMrRhCiCNLb+f3hZa4r6FIWV2e/H595HnfmzNl31lVe33POjEIIIUBERERkBEwMHQARERHRi8LEh4iIiIwGEx8iIiIyGkx8iIiIyGgw8SEiIiKjwcSHiIiIjAYTHyIiIjIaTHyIiIjIaDDxISIiIqPBxIdk6cKFC3jvvffg5eUFS0tL2NraomnTppg/fz4ePHhQoe997tw5tG3bFmq1GgqFAosXL9b7eygUCoSFhem9X10iIiKgUCigUChw5MiRIseFEKhZsyYUCgXatWtXpvdYvnw5IiIiSnXOkSNHnhlTRQoODpY+j+dtwcHBenm/TZs2ler7lJubi1WrVqFFixZwcHCAtbU1PD090atXL+zYsaNMMYSHh2Pnzp1lOpfoZaDgIytIbtasWYPRo0ejTp06GD16NOrVq4fc3FycOXMGa9asQePGjcv8j35JNGnSBOnp6fjqq69gb2+PatWqQaPR6PU9Tp48iapVq6Jq1ap67VeXiIgIvPfee7Czs0OvXr2wfv16reNHjhxB+/btYWdnh6ZNm5YpEWnQoAGcnJxKdW5qaiquXLmCevXqQaVSlfo9y+ratWtISkqSXv/+++8YM2YMwsPD0b59e2l/5cqVUaNGjXK/X/fu3XHp0iXcvHmzRO0HDhyI7du3IyQkBO3atYNSqcT169exb98+VK5cGStXrix1DLa2tnjzzTdLnZwSvSzMDB0AkT6dOHEC77//Pjp37oydO3dCqVRKxzp37oyJEydi3759FRrDpUuXMHz4cAQGBlbYe7Rq1arC+i6JAQMGYOPGjfj666+1Eo21a9fCz88PqampLySO3NxcKBQKqFQqg3wmNWrU0EposrKyAAC1atUy+J/RjRs38P3332P69OmYMWOGtL9jx44YPnw4CgoKDBgdkeFwqItkJTw8HAqFAqtXr9ZKegpZWFigZ8+e0uuCggLMnz8fdevWhVKphLOzM959913cuXNH67x27dqhQYMGiIqKQps2bWBtbY3q1atj7ty50g+QwmGgvLw8rFixQhrmAICwsDDp908qPOfJ/8EfOnQI7dq1g6OjI6ysrODh4YG+ffsiIyNDalPcUNelS5fQq1cv2Nvbw9LSEj4+PoiMjNRqUzgktHnzZkybNg1ubm5QqVTo1KkTYmNjS/YhA3jrrbcAAJs3b5b2paSkYNu2bRgyZEix58yYMQO+vr5wcHCASqVC06ZNsXbtWjxZdK5WrRouX76Mo0ePSp9ftWrVtGJfv349Jk6ciCpVqkCpVOKvv/4qMtT1zz//wN3dHa1bt0Zubq7U/5UrV2BjY4N33nmnxNeqDwcOHEDHjh2hUqlgbW0Nf39/HDx4UKtNUlISRowYAXd3dyiVSlSuXBn+/v44cOAAgMffwZ9//hm3bt3SGkZ7lvv37wMAXF1diz1uYqL9z39qaiomTZoELy8vWFhYoEqVKggJCUF6errURqFQID09HZGRkdL7l3VIk8hQmPiQbOTn5+PQoUNo1qwZ3N3dS3TO+++/jylTpqBz587YtWsXZs2ahX379qF169b4559/tNomJCRg8ODBePvtt7Fr1y4EBgZi6tSp2LBhAwCgW7duOHHiBADgzTffxIkTJ6TXJXXz5k1069YNFhYW+Pbbb7Fv3z7MnTsXNjY2yMnJeeZ5sbGxaN26NS5fvowlS5Zg+/btqFevHoKDgzF//vwi7T/55BPcunUL33zzDVavXo0///wTPXr0QH5+foniVKlUePPNN/Htt99K+zZv3gwTExMMGDDgmdc2cuRIbN26Fdu3b0efPn0wbtw4zJo1S2qzY8cOVK9eHU2aNJE+v6eHJadOnYrbt29j5cqV+Omnn+Ds7FzkvZycnLBlyxZERUVhypQpAICMjAz069cPHh4eZRriKasNGzagS5cuUKlUiIyMxNatW+Hg4ICAgACt5Oedd97Bzp07MX36dOzfvx/ffPMNOnXqJCUwy5cvh7+/PzQajfTZPO/75e3tjUqVKmHGjBlYvXr1c4fHMjIy0LZtW0RGRmL8+PHYu3cvpkyZgoiICPTs2VNKTk+cOAErKyu88cYb0vsvX75cPx8U0YsiiGQiISFBABADBw4sUfuYmBgBQIwePVpr/6lTpwQA8cknn0j72rZtKwCIU6dOabWtV6+eCAgI0NoHQIwZM0ZrX2hoqCjur9u6desEAHHjxg0hhBA//vijACCio6OfGzsAERoaKr0eOHCgUCqV4vbt21rtAgMDhbW1tXj48KEQQojDhw8LAOKNN97Qard161YBQJw4ceK571sYb1RUlNTXpUuXhBBCtGjRQgQHBwshhKhfv75o27btM/vJz88Xubm5YubMmcLR0VEUFBRIx551buH7vf766888dvjwYa398+bNEwDEjh07RFBQkLCyshIXLlx47jWWR2EcP/zwgxBCiPT0dOHg4CB69Oih1S4/P180btxYtGzZUtpna2srQkJCntt/t27dhKenZ4nj+fnnn4WTk5MAIAAIR0dH0a9fP7Fr1y6tdnPmzBEmJiYiKipKa3/h93HPnj3SPhsbGxEUFFTiGIheNqz4kNE6fPgwABRZcdOyZUt4e3sXGYrQaDRo2bKl1r5GjRrh1q1beovJx8cHFhYWGDFiBCIjI3H9+vUSnXfo0CF07NixSKUrODgYGRkZRSoDTw73AY+vA0CprqVt27aoUaMGvv32W1y8eBFRUVHPHOYqjLFTp05Qq9UwNTWFubk5pk+fjvv37yMxMbHE79u3b98St/3oo4/QrVs3vPXWW4iMjMTSpUvRsGFDnefl5eVpbaKMa0COHz+OBw8eICgoSKu/goICdO3aFVFRUdJQUsuWLREREYHPP/8cJ0+e1BqiK6s33ngDt2/fxo4dOzBp0iTUr18fO3fuRM+ePTF27Fip3e7du9GgQQP4+PhoxRkQEGCQ1XJEFYmJD8mGk5MTrK2tcePGjRK1f94cCDc3N+l4IUdHxyLtlEolMjMzyxBt8WrUqIEDBw7A2dkZY8aMkSbPfvXVV8897/79+8+8jsLjT3r6WgrnQ5XmWhQKBd577z1s2LABK1euRO3atdGmTZti254+fRpdunQB8HjV3W+//YaoqChMmzat1O/7rDkrz4oxODgYWVlZ0Gg0JZrbc/PmTZibm2ttR48eLfF7PunevXsAHg99Pt3nvHnzIISQbq/w/fffIygoCN988w38/Pzg4OCAd999FwkJCWV670JWVlbo3bs3vvjiCxw9ehR//fUX6tWrh6+//hqXL1+W4rxw4UKRGO3s7CCEKDLsS/Qq46oukg1TU1N07NgRe/fuxZ07d3Qu9S784R8fH1+k7d27d+Hk5KS32CwtLQEA2dnZWpOui/uB0qZNG7Rp0wb5+fk4c+YMli5dipCQELi4uGDgwIHF9u/o6Ij4+Pgi++/evQsAer2WJwUHB2P69OlYuXIlZs+e/cx2W7Zsgbm5OXbv3i19FgDKdD+Y503ofVp8fDzGjBkDHx8fXL58GZMmTcKSJUuee46bmxuioqK09tWpU6fUcQL/+9yXLl36zFVeLi4uUtvFixdj8eLFuH37Nnbt2oWPP/4YiYmJel2J6OHhgREjRiAkJASXL19G/fr14eTkBCsrK605W8VdB5EcsOJDsjJ16lQIITB8+PBiJwPn5ubip59+AgB06NABAKTJyYWioqIQExODjh076i2uwpVJFy5c0NpfGEtxTE1N4evri6+//hrA43vEPEvHjh1x6NAhKdEp9N1338Ha2rrCllZXqVIFH330EXr06IGgoKBntlMoFDAzM4Opqam0LzMzs8h9gAD9VdHy8/Px1ltvQaFQYO/evZgzZw6WLl2K7du3P/c8CwsLNG/eXGuzs7MrUwz+/v6oVKkSrly5UqTPws3CwqLIeR4eHhg7diw6d+6s9edems/m0aNHSEtLK/ZYTEwMgP9VBLt3745r167B0dGx2BgLv7+ljYHoZcSKD8mKn58fVqxYgdGjR6NZs2Z4//33Ub9+feTm5uLcuXNYvXo1GjRogB49eqBOnToYMWIEli5dChMTEwQGBuLmzZv47LPP4O7ujg8++EBvcb3xxhtwcHDA0KFDMXPmTJiZmSEiIgJxcXFa7VauXIlDhw6hW7du8PDwQFZWlvS/8E6dOj2z/9DQUOzevRvt27fH9OnT4eDggI0bN+Lnn3/G/PnzoVar9XYtT5s7d67ONt26dcPChQsxaNAgjBgxAvfv38eCBQuKveVAw4YNsWXLFnz//feoXr06LC0tSzQv52mhoaH4z3/+g/3790Oj0WDixIk4evQohg4diiZNmsDLy6vUfZaWra0tli5diqCgIDx48ABvvvkmnJ2dkZSUhPPnzyMpKQkrVqxASkoK2rdvj0GDBqFu3bqws7NDVFQU9u3bhz59+kj9NWzYENu3b8eKFSvQrFkzmJiYoHnz5sW+d2xsLAICAjBw4EC0bdsWrq6uSE5Oxs8//4zVq1ejXbt2aN26NQAgJCQE27Ztw+uvv44PPvgAjRo1QkFBAW7fvo39+/dj4sSJ8PX1lWI4cuQIfvrpJ7i6usLOzq7MFTEigzDs3GqiihEdHS2CgoKEh4eHsLCwEDY2NqJJkyZi+vTpIjExUWqXn58v5s2bJ2rXri3Mzc2Fk5OTePvtt0VcXJxWf23bthX169cv8j5BQUFFVtmgmFVdQghx+vRp0bp1a2FjYyOqVKkiQkNDxTfffKO1quvEiRPi//7v/4Snp6dQKpXC0dFRtG3btsgqHDy1qksIIS5evCh69Ogh1Gq1sLCwEI0bNxbr1q3TavP0qqNCN27cEACKtH/ak6u6nqe4lVnffvutqFOnjlAqlaJ69epizpw5Yu3atVrXL4QQN2/eFF26dBF2dnYCgPT5Piv2J48Vrurav3+/MDExKfIZ3b9/X3h4eIgWLVqI7Ozs515DWTwrxqNHj4pu3boJBwcHYW5uLqpUqSK6desmtcvKyhKjRo0SjRo1EiqVSlhZWYk6deqI0NBQkZ6eLvXz4MED8eabb4pKlSoJhUJR7ErBQsnJyeLzzz8XHTp0EFWqVJH+Hvj4+IjPP/9cZGRkaLVPS0sTn376qahTp46wsLAQarVaNGzYUHzwwQciISFBahcdHS38/f2FtbW1APDc1XtELyM+soKIiIiMBuf4EBERkdFg4kNERERGg4kPERERGQ0mPkRERGQ0mPgQERGR0WDiQ0REREaDNzB8BRQUFODu3buws7Mr1e36iYjo5SCEwKNHj+Dm5gYTk4qpOWRlZRV7x/qysLCw0Hq8jJww8XkF3L17t8hTt4mI6NUTFxen8zmCZZGVlQUrO0cgL0Mv/Wk0Gty4cUOWyQ8Tn1dA4XOCAr/4GeZWNgaOhqhirOjf2NAhEFWYR49SUbeGZ5mf+6ZLTk4OkJcBZb0gwLTo899KJT8HCVcikZOTw8SHDKNweMvcygbmVrYGjoaoYqhUKkOHQFThKny6gpklFOVMfIRC3tN/mfgQERHJhQJAeZMrmU8lZeJDREQkFwqTx1t5+5AxeV8dERER0RNY8SEiIpILhUIPQ13yHuti4kNERCQXHOrSSd5XR0RERPQEVnyIiIjkgkNdOjHxISIikg09DHXJfDBI3ldHRERE9ARWfIiIiOSCQ106MfEhIiKSC67q0kneV0dERET0BFZ8iIiI5IJDXTox8SEiIpILDnXpxMSHiIhILljx0UneaR0RERHRE1jxISIikgsOdenExIeIiEguFAo9JD4c6iIiIiKSBVZ8iIiI5MJE8Xgrbx8yxsSHiIhILjjHRyd5Xx0RERHRE1jxISIikgvex0cnJj5ERERywaEuneR9dURERERPYMWHiIhILjjUpRMTHyIiIrngUJdOTHyIiIjkghUfneSd1hERERE9gRUfIiIiueBQl05MfIiIiOSCQ106yTutIyIiInoCKz5ERESyoYehLpnXRJj4EBERyQWHunSSd1pHRERE9ARWfIiIiORCodDDqi55V3yY+BAREckFl7PrJO+rIyIiInoCKz5ERERywcnNOjHxISIikgsOdenExIeIiEguWPHRSd5pHREREdETWPEhIiKSCw516cTEh4iISC441KWTvNM6IiIioiew4kNERCQTCoUCClZ8nouJDxERkUww8dGNQ11ERERkNFjxISIikgvFf7fy9iFjTHyIiIhkgkNdunGoi4iIiIwGKz5EREQywYqPbkx8iIiIZIKJj24c6iIiIpKJwsSnvFtp/Pvf/0aPHj3g5uYGhUKBnTt3ah0XQiAsLAxubm6wsrJCu3btcPnyZa022dnZGDduHJycnGBjY4OePXvizp07Wm2Sk5PxzjvvQK1WQ61W45133sHDhw9L/Rkx8SEiIqIyS09PR+PGjbFs2bJij8+fPx8LFy7EsmXLEBUVBY1Gg86dO+PRo0dSm5CQEOzYsQNbtmzBsWPHkJaWhu7duyM/P19qM2jQIERHR2Pfvn3Yt28foqOj8c4775Q6Xg51ERERyYUBlrMHBgYiMDCw2GNCCCxevBjTpk1Dnz59AACRkZFwcXHBpk2bMHLkSKSkpGDt2rVYv349OnXqBADYsGED3N3dceDAAQQEBCAmJgb79u3DyZMn4evrCwBYs2YN/Pz8EBsbizp16pQ4XlZ8iIiIZMIQQ13Pc+PGDSQkJKBLly7SPqVSibZt2+L48eMAgLNnzyI3N1erjZubGxo0aCC1OXHiBNRqtZT0AECrVq2gVqulNiXFig8REREVkZqaqvVaqVRCqVSWqo+EhAQAgIuLi9Z+FxcX3Lp1S2pjYWEBe3v7Im0Kz09ISICzs3OR/p2dnaU2JcWKDxERkUwoFPqo+jzuy93dXZpIrFarMWfOnHLEpV1FEkLorCw93aa49iXp52ms+BAREcmEAvoYqnp8flxcHFQqlbS3tNUeANBoNAAeV2xcXV2l/YmJiVIVSKPRICcnB8nJyVpVn8TERLRu3Vpqc+/evSL9JyUlFakm6cKKDxERERWhUqm0trIkPl5eXtBoNPj111+lfTk5OTh69KiU1DRr1gzm5uZabeLj43Hp0iWpjZ+fH1JSUnD69GmpzalTp5CSkiK1KSlWfIiIiGTCEDcwTEtLw19//SW9vnHjBqKjo+Hg4AAPDw+EhIQgPDwctWrVQq1atRAeHg5ra2sMGjQIAKBWqzF06FBMnDgRjo6OcHBwwKRJk9CwYUNplZe3tze6du2K4cOHY9WqVQCAESNGoHv37qVa0QUw8SEiIpIPAyxnP3PmDNq3by+9/vDDDwEAQUFBiIiIwOTJk5GZmYnRo0cjOTkZvr6+2L9/P+zs7KRzFi1aBDMzM/Tv3x+ZmZno2LEjIiIiYGpqKrXZuHEjxo8fL63+6tmz5zPvHfTcyxNCiFKfRS9Uamoq1Go1ei47AnMrW0OHQ1QhIgY3MXQIRBUmNTUVVZztkZKSojVvRp/9q9Vq2A/8BgoL63L1JXIykLxlWIXFamis+BAREcmFHoa6hMyf1cXEh4iISCb0McdHnzcwfBkx8SEiIpIJJj66cTk7ERERGQ1WfIiIiOTCAKu6XjVMfIiIiGSCQ126caiLiIiIjAYrPkRERDLBio9uTHyIiIhkgomPbhzqIiIiIqPBig8REZFMsOKjGxMfIiIiueBydp041EVERERGgxUfIiIimeBQl25MfIiIiGSCiY9uTHyIiIhkgomPbpzjQ0REREaDFR8iIiK54KounZj4EBERyQSHunTjUBcREREZDVZ8XrA//vgDwcHBiI6ORt26dREdHW3okIyGvZU5BjargkZV1LAwNUFCahbWHL+Fmw8yAABKMxMMaFoFzd0rwVZphqS0bOz/IxEHr/5TbH8fdayJxlXUWHT4L5yNS3mRl0Kk0+LI/fj5yAX8eeserJTmaNHQC9PH9ERNTxepzdiZG/D9ntNa5zWr74l9aye+6HBJT1jx0c1oEp/ExER89tln2Lt3L+7duwd7e3s0btwYYWFh8PPze2FxhIaGwsbGBrGxsbC1tX1h72vsrC1MMT2wDmISHuGLA38iNSsPLnZKZOTkSW3eblEV9VzssOLYDSSl5aChmwrBvh5IzszF708lNl29nSHEi74KopI7fu4vDOnbBk3qeSAvvwDhK3ej34TlOLb5E9hYKaV2HVp5Y8lng6XXFmamhgiX9EQBPSQ+Mp/kYzSJT9++fZGbm4vIyEhUr14d9+7dw8GDB/HgwYMXGse1a9fQrVs3eHp6vtD3NXY9GmjwID0Hq4/fkvb9k56j1aamky3+c+0+Yu6lAQAO//kPOtR2QnVHG63Ex8PeCoH1XDD95xh83b/xi7kAolLauni01uslnw6Cd+A0nP8jDq2b1JT2Ky3M4OKoetHhERmMUczxefjwIY4dO4Z58+ahffv28PT0RMuWLTF16lR069YNwOPS3ooVKxAYGAgrKyt4eXnhhx9+0Orn4sWL6NChA6ysrODo6IgRI0YgLS1NOl5QUICZM2eiatWqUCqV8PHxwb59+6TjCoUCZ8+excyZM6FQKBAWFvZCrp+AplXVuH4/A+Ner46v+zXC59290a6Wk1abq4lpaOpeCfZW5gAAbxdbaFSWuHD3f0mPhakCY9p4IfL0baRk5YHoVZGalgUAsFdZa+3/7fe/4B34CXz7zcIH4ZuR9OCRIcIjPSkc6irvJmdGkfjY2trC1tYWO3fuRHZ29jPbffbZZ+jbty/Onz+Pt99+G2+99RZiYmIAABkZGejatSvs7e0RFRWFH374AQcOHMDYsWOl87/66it8+eWXWLBgAS5cuICAgAD07NkTf/75JwAgPj4e9evXx8SJExEfH49JkyZV7IWTpLKdEh3rVMa9R1mYf/BPHIpNwrst3PFadQepzXdRcfg7JQtL+zVCxNtNMblTLUScuo2rielSm7dbuOPPpPQiQ19ELzMhBKZ/tQO+javDu4abtL+jXz2smPEuti8bi5nje+NczG30GbsM2Tm5BoyWykWhp03GjCLxMTMzQ0REBCIjI1GpUiX4+/vjk08+wYULF7Ta9evXD8OGDUPt2rUxa9YsNG/eHEuXLgUAbNy4EZmZmfjuu+/QoEEDdOjQAcuWLcP69etx7949AMCCBQswZcoUDBw4EHXq1MG8efPg4+ODxYsXAwA0Gg3MzMxga2sLjUbzzDk+2dnZSE1N1dqofEwA3Lyfga3n7uLWg0wc+vMfHP7zH3SsU1lqE1DXGTWdbPDlob/w2e4YbDpzB8G+HqjvagfgcdWonsYO66PiDHQVRGUzZcEPuPLXXayeFaS1//86N0UX//rwruGGgDYNsWXRKFy7nYhff7tioEiJKp5RJD7A4zk+d+/exa5duxAQEIAjR46gadOmiIiIkNo8PcnZz89PqvjExMSgcePGsLGxkY77+/ujoKAAsbGxSE1Nxd27d+Hv76/Vh7+/v9RHSc2ZMwdqtVra3N3dS3m19LSHmbm4m5Klte9uSiYcbSwAAOamCvRv4oaNZ+Jw7k4K4h5m4tfYJJy6mYxu9R6vgqmnsYOznRKrB/og8u2miHy7KQBgQtsamNal9ou9IKIS+njBj/jlP5ewY/k4uDnbP7etxkmNqhoHXI9LfEHRkb5xqEs3o5ncDACWlpbo3LkzOnfujOnTp2PYsGEIDQ1FcHDwM88p/AIIIZ75ZXhy/9Ntnnfes0ydOhUffvih9Do1NZXJTzldTUqHq0qptU+jssQ/aY8nOJuZKGBmaoKCp1ZqFTzx5/fTpQQc+Ut7afvcnvWx4b/JEtHLRAiBj7/8EXuOXsDOr8fB081R5zkPUtJxNzEZLk7qFxAhVQQuZ9fNaCo+xalXrx7S0/83f+PkyZNax0+ePIm6detKbaOjo7Xa//bbbzAxMUHt2rWhUqng5uaGY8eOafVx/PhxeHt7lyoupVIJlUqltVH57LtyDzUq26JnAw1c7JTw87JH+1pOOBCbBADIzC1ATMIjvNWsKrxdbFHZ1gJtajjiteqOOHM7GQCQkpWHOw+ztDYAuJ+eg6S0nGe+N5EhTPniB/y47wxWzngXtjaWuHc/FffupyIz6/F3NS0jG6FLdiLq4g3cvnsfv539E29PWgUHtQ26tW1k4OiprBQK/WxyZhQVn/v376Nfv34YMmQIGjVqBDs7O5w5cwbz589Hr169pHY//PADmjdvjtdeew0bN27E6dOnsXbtWgDA4MGDERoaiqCgIISFhSEpKQnjxo3DO++8AxeXx0MhH330EUJDQ1GjRg34+Phg3bp1iI6OxsaNGw1y3fQ/1+9nYPHhaxjQtAp6N3ZF0qNsbDhzB8dv/O92Bsv+fR0DmlbB+228YGthhn/Sc/DDub+feQNDopfZuu2P/xPWe/RSrf1LPh2Mt7r7wtREgSvX7mLr3tNIeZQJFycV/JvWwprP34OtjaUhQiZ6IYwi8bG1tYWvry8WLVqEa9euITc3F+7u7hg+fDg++eQTqd2MGTOwZcsWjB49GhqNBhs3bkS9evUAANbW1vjll18wYcIEtGjRAtbW1ujbty8WLlwonT9+/HikpqZi4sSJSExMRL169bBr1y7UqlXrhV8zFRX9dwqi/372kFRKVp7WfX5K4u3vzpY3LKIKkXRyyXOPW1la4IevRj+3Db16HldsyjvUpadgXlIKIXj/WeDxF2XHjh3o3bu3oUMpIjU1FWq1Gj2XHYG5Fe/2TPIUMbiJoUMgqjCpqamo4myPlJSUCpm+UPhzovr4H2GqtNF9wnPkZ6fj+pI3KyxWQzPqOT5ERERkXIxiqIuIiMgYcFWXbkx8/osjfkRE9KrTx6osmec9HOoiIiIi48GKDxERkUyYmChgYlK+ko0o5/kvOyY+REREMsGhLt041EVERERGgxUfIiIimeCqLt2Y+BAREckEh7p0Y+JDREQkE6z46MY5PkRERGQ0WPEhIiKSCVZ8dGPiQ0REJBOc46Mbh7qIiIjIaLDiQ0REJBMK6GGoC/Iu+TDxISIikgkOdenGoS4iIiIyGqz4EBERyQRXdenGxIeIiEgmONSlG4e6iIiIqMzy8vLw6aefwsvLC1ZWVqhevTpmzpyJgoICqY0QAmFhYXBzc4OVlRXatWuHy5cva/WTnZ2NcePGwcnJCTY2NujZsyfu3Lmj93iZ+BAREclE4VBXebfSmDdvHlauXIlly5YhJiYG8+fPxxdffIGlS5dKbebPn4+FCxdi2bJliIqKgkajQefOnfHo0SOpTUhICHbs2IEtW7bg2LFjSEtLQ/fu3ZGfn6+3zwfgUBcREZFsGGKo68SJE+jVqxe6desGAKhWrRo2b96MM2fOAHhc7Vm8eDGmTZuGPn36AAAiIyPh4uKCTZs2YeTIkUhJScHatWuxfv16dOrUCQCwYcMGuLu748CBAwgICCjfRT2BFR8iIiKZMETF57XXXsPBgwdx9epVAMD58+dx7NgxvPHGGwCAGzduICEhAV26dJHOUSqVaNu2LY4fPw4AOHv2LHJzc7XauLm5oUGDBlIbfWHFh4iIiIpITU3Veq1UKqFUKou0mzJlClJSUlC3bl2YmpoiPz8fs2fPxltvvQUASEhIAAC4uLhonefi4oJbt25JbSwsLGBvb1+kTeH5+sKKDxERkVwo/jfcVdat8MbN7u7uUKvV0jZnzpxi3/L777/Hhg0bsGnTJvz++++IjIzEggULEBkZqR3aU5UkIYTO6lJJ2pQWKz5EREQyoc/7+MTFxUGlUkn7i6v2AMBHH32Ejz/+GAMHDgQANGzYELdu3cKcOXMQFBQEjUYD4HFVx9XVVTovMTFRqgJpNBrk5OQgOTlZq+qTmJiI1q1bl+t6nsaKDxERERWhUqm0tmclPhkZGTAx0U4nTE1NpeXsXl5e0Gg0+PXXX6XjOTk5OHr0qJTUNGvWDObm5lpt4uPjcenSJb0nPqz4EBERyYQhVnX16NEDs2fPhoeHB+rXr49z585h4cKFGDJkyH/7UyAkJATh4eGoVasWatWqhfDwcFhbW2PQoEEAALVajaFDh2LixIlwdHSEg4MDJk2ahIYNG0qrvPSFiQ8REZFMGOKRFUuXLsVnn32G0aNHIzExEW5ubhg5ciSmT58utZk8eTIyMzMxevRoJCcnw9fXF/v374ednZ3UZtGiRTAzM0P//v2RmZmJjh07IiIiAqampuW6nqcphBBCrz2S3qWmpkKtVqPnsiMwt7I1dDhEFSJicBNDh0BUYVJTU1HF2R4pKSla82b02b9arUbLmXthZmlTrr7ystJxenpghcVqaKz4EBERyQSf1aUbEx8iIiKZ4NPZdeOqLiIiIjIarPgQERHJBCs+ujHxISIikgnO8dGNiQ8REZFMsOKjG+f4EBERkdFgxYeIiEgmONSlGxMfIiIimeBQl24c6iIiIiKjwYoPERGRTCigh6EuvUTy8mLiQ0REJBMmCgVMypn5lPf8lx2HuoiIiMhosOJDREQkE1zVpRsTHyIiIpngqi7dmPgQERHJhIni8VbePuSMc3yIiIjIaLDiQ0REJBcKPQxVybziw8SHiIhIJji5WTcOdREREZHRYMWHiIhIJhT//VXePuSMiQ8REZFMcFWXbhzqIiIiIqPBig8REZFM8AaGupUo8VmyZEmJOxw/fnyZgyEiIqKy46ou3UqU+CxatKhEnSkUCiY+RERE9NIqUeJz48aNio6DiIiIyslEoYBJOUs25T3/ZVfmyc05OTmIjY1FXl6ePuMhIiKiMioc6irvJmelTnwyMjIwdOhQWFtbo379+rh9+zaAx3N75s6dq/cAiYiIqGQKJzeXd5OzUic+U6dOxfnz53HkyBFYWlpK+zt16oTvv/9er8ERERER6VOpl7Pv3LkT33//PVq1aqWVFdarVw/Xrl3Ta3BERERUclzVpVupE5+kpCQ4OzsX2Z+eni778hgREdHLjJObdSv1UFeLFi3w888/S68Lk501a9bAz89Pf5ERERER6VmpKz5z5sxB165dceXKFeTl5eGrr77C5cuXceLECRw9erQiYiQiIqISUPx3K28fclbqik/r1q3x22+/ISMjAzVq1MD+/fvh4uKCEydOoFmzZhURIxEREZUAV3XpVqZndTVs2BCRkZH6joWIiIioQpUp8cnPz8eOHTsQExMDhUIBb29v9OrVC2ZmfOYpERGRoZgoHm/l7UPOSp2pXLp0Cb169UJCQgLq1KkDALh69SoqV66MXbt2oWHDhnoPkoiIiHTj09l1K/Ucn2HDhqF+/fq4c+cOfv/9d/z++++Ii4tDo0aNMGLEiIqIkYiIiEgvSl3xOX/+PM6cOQN7e3tpn729PWbPno0WLVroNTgiIiIqHZkXbMqt1BWfOnXq4N69e0X2JyYmombNmnoJioiIiEqPq7p0K1HFJzU1Vfp9eHg4xo8fj7CwMLRq1QoAcPLkScycORPz5s2rmCiJiIhIJ05u1q1EiU+lSpW0MkAhBPr37y/tE0IAAHr06IH8/PwKCJOIiIio/EqU+Bw+fLii4yAiIqJy4qou3UqU+LRt27ai4yAiIqJy4iMrdCvzHQczMjJw+/Zt5OTkaO1v1KhRuYMiIiIiqgilTnySkpLw3nvvYe/evcUe5xwfIiIiwzBRKGBSzqGq8p7/siv1cvaQkBAkJyfj5MmTsLKywr59+xAZGYlatWph165dFREjERERlYBCoZ9Nzkpd8Tl06BD+9a9/oUWLFjAxMYGnpyc6d+4MlUqFOXPmoFu3bhURJxEREVG5lbrik56eDmdnZwCAg4MDkpKSADx+Yvvvv/+u3+iIiIioxHgDQ93KdOfm2NhYAICPjw9WrVqFv//+GytXroSrq6veAyQiIqKS4VCXbmWa4xMfHw8ACA0Nxb59++Dh4YElS5YgPDxc7wESERHRy+3vv//G22+/DUdHR1hbW8PHxwdnz56VjgshEBYWBjc3N1hZWaFdu3a4fPmyVh/Z2dkYN24cnJycYGNjg549e+LOnTt6j7XUc3wGDx4s/b5Jkya4efMm/vjjD3h4eMDJyUmvwREREVHJGWJVV3JyMvz9/dG+fXvs3bsXzs7OuHbtGipVqiS1mT9/PhYuXIiIiAjUrl0bn3/+OTp37ozY2FjY2dkBeFxY+emnn7BlyxY4Ojpi4sSJ6N69O86ePQtTU9NyXdOTynwfn0LW1tZo2rSpPmIhIiKictDHUFVpz583bx7c3d2xbt06aV+1atWk3wshsHjxYkybNg19+vQBAERGRsLFxQWbNm3CyJEjkZKSgrVr12L9+vXo1KkTAGDDhg1wd3fHgQMHEBAQUL6LekKJEp8PP/ywxB0uXLiwzMEQERFR2RnikRW7du1CQEAA+vXrh6NHj6JKlSoYPXo0hg8fDgC4ceMGEhIS0KVLF+kcpVKJtm3b4vjx4xg5ciTOnj2L3NxcrTZubm5o0KABjh8//uITn3PnzpWoM7nPBCciIjIWqampWq+VSiWUSmWRdtevX8eKFSvw4Ycf4pNPPsHp06cxfvx4KJVKvPvuu0hISAAAuLi4aJ3n4uKCW7duAQASEhJgYWEBe3v7Im0Kz9cXPqT0FbLmrSZQqVSGDoOoQti3GGvoEIgqjMjP0d1ID0xQhlVLxfQBAO7u7lr7Q0NDERYWVqR9QUEBmjdvLi1watKkCS5fvowVK1bg3Xffldo9XRwRQugsmJSkTWmVe44PERERvRz0OdQVFxen9Z/t4qo9AODq6op69epp7fP29sa2bdsAABqNBsDjqs6Tt71JTEyUqkAajQY5OTlITk7WqvokJiaidevW5bqep5U3MSQiIiIZUqlUWtuzEh9/f3/p/n6Frl69Ck9PTwCAl5cXNBoNfv31V+l4Tk4Ojh49KiU1zZo1g7m5uVab+Ph4XLp0Se+JDys+REREMqFQACYveFXXBx98gNatWyM8PBz9+/fH6dOnsXr1aqxevfq//SkQEhKC8PBw1KpVC7Vq1UJ4eDisra0xaNAgAIBarcbQoUMxceJEODo6wsHBAZMmTULDhg2lVV76wsSHiIhIJkz0kPiU9vwWLVpgx44dmDp1KmbOnAkvLy8sXrxY675/kydPRmZmJkaPHo3k5GT4+vpi//790j18AGDRokUwMzND//79kZmZiY4dOyIiIkKv9/ABAIUQQui1R9K71NRUqNVq3LufwsnNJFuc3ExyJvJzkH1xDVJSKubf8cKfE6M3R0FpbVuuvrIz0rD8rRYVFquhlWmOz/r16+Hv7w83NzdpKdrixYvxr3/9S6/BERERUcnxIaW6lTrxKVyr/8Ybb+Dhw4fIz88HAFSqVAmLFy/Wd3xERERUQoVDXeXd5KzUic/SpUuxZs0aTJs2TWvcrXnz5rh48aJegyMiIiLSp1JPbr5x4waaNGlSZL9SqUR6erpegiIiIqLSM8Szul41pa74eHl5ITo6usj+vXv3FrmBEREREb04hU9nL+8mZ6Wu+Hz00UcYM2YMsrKyIITA6dOnsXnzZsyZMwfffPNNRcRIREREJaDPR1bIVakTn/feew95eXmYPHkyMjIyMGjQIFSpUgVfffUVBg4cWBExEhEREelFmW5gOHz4cAwfPhz//PMPCgoK4OzsrO+4iIiIqJQ4x0e3ct252cnJSV9xEBERUTmZoPxzdEwg78yn1ImPl5fXc29udP369XIFRERERFRRSp34hISEaL3Ozc3FuXPnsG/fPnz00Uf6iouIiIhKiUNdupU68ZkwYUKx+7/++mucOXOm3AERERFR2RjiIaWvGr2tWgsMDMS2bdv01R0RERGR3pVrcvOTfvzxRzg4OOirOyIiIiolhQLlntzMoa6nNGnSRGtysxACCQkJSEpKwvLly/UaHBEREZUc5/joVurEp3fv3lqvTUxMULlyZbRr1w5169bVV1xEREREeleqxCcvLw/VqlVDQEAANBpNRcVEREREZcDJzbqVanKzmZkZ3n//fWRnZ1dUPERERFRGCj39krNSr+ry9fXFuXPnKiIWIiIiKofCik95Nzkr9Ryf0aNHY+LEibhz5w6aNWsGGxsbreONGjXSW3BERERE+lTixGfIkCFYvHgxBgwYAAAYP368dEyhUEAIAYVCgfz8fP1HSURERDpxjo9uJU58IiMjMXfuXNy4caMi4yEiIqIyUigUz32eZkn7kLMSJz5CCACAp6dnhQVDREREVJFKNcdH7lkgERHRq4xDXbqVKvGpXbu2zuTnwYMH5QqIiIiIyoZ3btatVInPjBkzoFarKyoWIiIiogpVqsRn4MCBcHZ2rqhYiIiIqBxMFIpyP6S0vOe/7Eqc+HB+DxER0cuNc3x0K/GdmwtXdRERERG9qkpc8SkoKKjIOIiIiKi89DC5WeaP6ir9IyuIiIjo5WQCBUzKmbmU9/yXHRMfIiIimeBydt1K/XR2IiIiolcVKz5EREQywVVdujHxISIikgnex0c3DnURERGR0WDFh4iISCY4uVk3Jj5EREQyYQI9DHXJfDk7h7qIiIjIaLDiQ0REJBMc6tKNiQ8REZFMmKD8QzlyHwqS+/URERERSVjxISIikgmFQgFFOceqynv+y46JDxERkUwoUP6Hq8s77WHiQ0REJBu8c7NunONDRERERoMVHyIiIhmRd72m/Jj4EBERyQTv46Mbh7qIiIjIaLDiQ0REJBNczq4bEx8iIiKZ4J2bdZP79REREdELMmfOHCgUCoSEhEj7hBAICwuDm5sbrKys0K5dO1y+fFnrvOzsbIwbNw5OTk6wsbFBz549cefOnQqJkYkPERGRTBQOdZV3K4uoqCisXr0ajRo10to/f/58LFy4EMuWLUNUVBQ0Gg06d+6MR48eSW1CQkKwY8cObNmyBceOHUNaWhq6d++O/Pz8cn0exWHiQ0REJBMKPW2llZaWhsGDB2PNmjWwt7eX9gshsHjxYkybNg19+vRBgwYNEBkZiYyMDGzatAkAkJKSgrVr1+LLL79Ep06d0KRJE2zYsAEXL17EgQMHyvZBPAcTHyIiIiqXMWPGoFu3bujUqZPW/hs3biAhIQFdunSR9imVSrRt2xbHjx8HAJw9exa5ublabdzc3NCgQQOpjT5xcjMREZFM6HNVV2pqqtZ+pVIJpVJZpP2WLVvw+++/IyoqqsixhIQEAICLi4vWfhcXF9y6dUtqY2FhoVUpKmxTeL4+seJDREQkEyZ62gDA3d0darVa2ubMmVPk/eLi4jBhwgRs2LABlpaWz4zr6WRMCKEzQStJm7JgxYeIiEgm9FnxiYuLg0qlkvYXV+05e/YsEhMT0axZM2lffn4+/v3vf2PZsmWIjY0F8Liq4+rqKrVJTEyUqkAajQY5OTlITk7WqvokJiaidevW5bqW4rDiQ0REREWoVCqtrbjEp2PHjrh48SKio6OlrXnz5hg8eDCio6NRvXp1aDQa/Prrr9I5OTk5OHr0qJTUNGvWDObm5lpt4uPjcenSpQpJfFjxISIikomyrsp6uo+SsrOzQ4MGDbT22djYwNHRUdofEhKC8PBw1KpVC7Vq1UJ4eDisra0xaNAgAIBarcbQoUMxceJEODo6wsHBAZMmTULDhg2LTJbWByY+REREMvEyPqR08uTJyMzMxOjRo5GcnAxfX1/s378fdnZ2UptFixbBzMwM/fv3R2ZmJjp27IiIiAiYmprqNxgACiGE0HuvpFepqalQq9W4dz9Fa7yVSE7sW4w1dAhEFUbk5yD74hqkpFTMv+OFPyc2Hb8Ka1s73Sc8R0baIwxqXbvCYjU0VnyIiIhkwgQKmJRzsKu857/smPgQERHJxMs41PWy4aouIiIiMhqs+BAREcmE4r+/ytuHnDHxISIikgkOdenGoS4iIiIyGqz4EBERyYRCD6u6ONRFRERErwQOdenGxIeIiEgmmPjoxjk+REREZDRY8SEiIpIJLmfXjYkPERGRTJgoHm/l7UPOONRFRERERoMVHyIiIpngUJduTHyIiIhkgqu6dONQFxERERkNVnyIiIhkQoHyD1XJvODDxIeIiEguuKpLNw51ERERkdFgxec5du7ciUmTJuHGjRsYN24cfHx8EBISgocPHz7znLCwMOzcuRPR0dEvLE7Sj7U//gffbvsP4uIfAADqVtfgo6GB6Oxf38CRERXVukkNjHunExrX9YBrZTUGT1qNPUcvSMe7t2+M4P97DT7e7nCsZIs2g+fg0tW/tfqoVsUJsyb8H1r5VIeFuRkOnojBlAU/IOnBI6nN+X/NgIebo9Z5iyP3Y8ayXRV7gVQmXNWlm0ErPsHBwVAoFNLm6OiIrl274sKFC7pPfgFGjhyJN998E3FxcZg1axYGDBiAq1evGjosqiBuzpUQOrYXDkV+hEORH6FN89oYPGk1Yq7FGzo0oiKsrZS4dPVvTP5ia7HHbSwtcOrCNcxY9q/iz7e0wPZlYyAg0Ov9pQgctggW5qbYvHAkFE8t65m9cjfqdJ0qbQvW7tP79ZB+FK7qKu8mZwav+HTt2hXr1q0DACQkJODTTz9F9+7dcfv2bYPGlZaWhsTERAQEBMDNzU3ab2VlZcCoqCIFvt5Q6/Vno3vi223HcObSDXjXcDVQVETFO3D8Cg4cv/LM49/vjQIAuLs6FHvct3F1eLg6ou3b8/AoPQsAMGbmBtw89AVeb1EbR0/HSm3TMrKQeP9Rsf3Qy0WB8k9OlnneY/g5PkqlEhqNBhqNBj4+PpgyZQri4uKQlJQEALh48SI6dOgAKysrODo6YsSIEUhLS5PODw4ORu/evbFgwQK4urrC0dERY8aMQW5urtRmw4YNaN68Oezs7KDRaDBo0CAkJiY+M6YjR47Azs4OANChQwcoFAocOXIEERERqFSpklbbuXPnwsXFBXZ2dhg6dCiysrKK9Ldu3Tp4e3vD0tISdevWxfLly8vzkdELkJ9fgG37zyAjMwctGnoZOhwivVNamEEIgeycPGlfdk4e8vML0KpxDa22E97tjGu/zsO/N36Mie8FwNzM9EWHS6Q3Bq/4PCktLQ0bN25EzZo14ejoiIyMDHTt2hWtWrVCVFQUEhMTMWzYMIwdOxYRERHSeYcPH4arqysOHz6Mv/76CwMGDICPjw+GDx8OAMjJycGsWbNQp04dJCYm4oMPPkBwcDD27NlTbBytW7dGbGws6tSpg23btqF169ZwcHDAzZs3tdpt3boVoaGh+Prrr9GmTRusX78eS5YsQfXq1aU2a9asQWhoKJYtW4YmTZrg3LlzGD58OGxsbBAUFFTs+2dnZyM7O1t6nZqaWsZPlErr8l9/I2DIl8jKyYONlRLrvxiOutVZ7SH5ibp4ExlZOQgb1wuzvt4FhUKBsHG9YGpqAo2TSmq3cssRnI+NQ0pqBprW98T0MT3h4eaICbM3GTB6ehYTKGBSzrEqE5nXfAye+OzevRu2trYAgPT0dLi6umL37t0wMTHBxo0bkZmZie+++w42NjYAgGXLlqFHjx6YN28eXFxcAAD29vZYtmwZTE1NUbduXXTr1g0HDx6UEp8hQ4ZI71e9enUsWbIELVu2RFpamvTeT7KwsICzszMAwMHBARqNptjYFy9ejCFDhmDYsGEAgM8//xwHDhzQqvrMmjULX375Jfr06QMA8PLywpUrV7Bq1apnJj5z5szBjBkzSv4hkt7U8nTBvzdORcqjDOw6FI3RYeuxe9UEJj8kO/cfpiH447X48uMBGDmgLQoKBLbtP4vomNvILyiQ2q3YfFj6/eW/7uJhaia+mz8MYcv+heSUdEOETs/BoS7dDD7U1b59e0RHRyM6OhqnTp1Cly5dEBgYiFu3biEmJgaNGzeWkh4A8Pf3R0FBAWJj/zf+XL9+fZia/q/06urqqjWUde7cOfTq1Quenp6ws7NDu3btAECaR1S/fn3Y2trC1tYWgYGBJY49JiYGfn5+WvuefJ2UlIS4uDgMHTpU6t/W1haff/45rl279sx+p06dipSUFGmLi4srcUxUPhbmZqjuXhlN6nkidGwvNKhVBSu3HDF0WEQV4vCpP9D0/2agVpepqNH5Y4wK/Q6uzpVw6+/7zzznzKUbAIDqVZ1eVJhEemXwio+NjQ1q1qwpvW7WrBnUajXWrFkDIUSR1QWFntxvbm5e5FjBf//Hkp6eji5duqBLly7YsGEDKleujNu3byMgIAA5OTkAgD179khzgvQ5ebkwhjVr1sDX11fr2JOJ2tOUSiWUSqXe4qCyE0Ig54k5EERy9OC/lZs2zWujsr0t9v7n4jPbNqrjDgC49w+H4F9KLPnoZPDE52kKhQImJibIzMxEvXr1EBkZifT0dKnq89tvv8HExAS1a9cuUX9//PEH/vnnH8ydOxfu7o//wp45c0arjaenZ5li9fb2xsmTJ/Huu+9K+06ePCn93sXFBVWqVMH169cxePDgMr0HvTgzv96FTq3roaqLPR5lZGH7/rM49vuf+HHJaEOHRlSEjZUFvNwrS6893RzRoHYVPEzJwJ17yaikskZVjT1cndQAHg/jAkDi/VRphdagHq1w9UYC/klOQ8tGXpjz4ZtYvvkw/rr1uGLeoqEXmjeohv+cvYrUtCw0reeB2R/0xZ6jF3DnXvILvmIqCd7HRzeDJz7Z2dlISEgAACQnJ2PZsmVIS0tDjx490LJlS4SGhiIoKAhhYWFISkrCuHHj8M4770jze3Tx8PCAhYUFli5dilGjRuHSpUuYNWuWXmKfMGECgoKC0Lx5c7z22mvYuHEjLl++rDW5OSwsDOPHj4dKpUJgYCCys7Nx5swZJCcn48MPP9RLHKQfSQ8eYVTod7j3TypUtpaoX7MKflwyGu19vQ0dGlERPt6e2L1qgvQ6/MO+AIBNu09izIwNCHy9IZaHviMd/zb88VzHuav3YN6axws7ank6Y/qYnrBXWeP23Qf4ct0vWL7pkHROdk4u/q9zU0wZHggLczPEJTzAdzuPY8l3v76ISySqEAZPfPbt2wdX18cTR+3s7FC3bl388MMP0jycX375BRMmTECLFi1gbW2Nvn37YuHChSXuv3LlyoiIiMAnn3yCJUuWoGnTpliwYAF69uxZ7tgHDBiAa9euYcqUKcjKykLfvn3x/vvv45dffpHaDBs2DNbW1vjiiy8wefJk2NjYoGHDhggJCSn3+5N+Lf2MVTl6dfz2+5+wbzH2mcc37z6FzbtPPbePGct2PfcOzBdi76DLkC/LHCMZgD5uQCjvgg8UQghh6CDo+VJTU6FWq3HvfgpUKpXuE4heQc/7IU70qhP5Oci+uAYpKRXz73jhz4lD0bdha1e+/tMepaKDj0eFxWpoBl/VRURERPSiGHyoi4iIiPSEq7p0YuJDREQkE1zVpRsTHyIiIpnQx9PV5f50ds7xISIiIqPBig8REZFMcIqPbkx8iIiI5IKZj04c6iIiIiKjwYoPERGRTHBVl25MfIiIiGSCq7p041AXERERGQ1WfIiIiGSCc5t1Y+JDREQkF8x8dOJQFxERERkNVnyIiIhkgqu6dGPiQ0REJBNc1aUbEx8iIiKZ4BQf3TjHh4iIiIwGKz5ERERywZKPTkx8iIiIZIKTm3XjUBcRERGV2Zw5c9CiRQvY2dnB2dkZvXv3RmxsrFYbIQTCwsLg5uYGKysrtGvXDpcvX9Zqk52djXHjxsHJyQk2Njbo2bMn7ty5o/d4mfgQERHJROGqrvJupXH06FGMGTMGJ0+exK+//oq8vDx06dIF6enpUpv58+dj4cKFWLZsGaKioqDRaNC5c2c8evRIahMSEoIdO3Zgy5YtOHbsGNLS0tC9e3fk5+fr6+MBACiEEEKvPZLepaamQq1W4979FKhUKkOHQ1Qh7FuMNXQIRBVG5Ocg++IapKRUzL/jhT8nTv9xF7Z25es/7VEqWtZ1K3OsSUlJcHZ2xtGjR/H6669DCAE3NzeEhIRgypQpAB5Xd1xcXDBv3jyMHDkSKSkpqFy5MtavX48BAwYAAO7evQt3d3fs2bMHAQEB5bqmJ7HiQ0RERHqTkpICAHBwcAAA3LhxAwkJCejSpYvURqlUom3btjh+/DgA4OzZs8jNzdVq4+bmhgYNGkht9IWTm4mIiORCj6u6UlNTtXYrlUoolcrnniqEwIcffojXXnsNDRo0AAAkJCQAAFxcXLTauri44NatW1IbCwsL2NvbF2lTeL6+sOJDREQkEwo9/QIAd3d3qNVqaZszZ47O9x87diwuXLiAzZs3F43tqclDQogi+55WkjalxYoPERERFREXF6c1x0dXtWfcuHHYtWsX/v3vf6Nq1arSfo1GA+BxVcfV1VXan5iYKFWBNBoNcnJykJycrFX1SUxMROvWrfVyPYVY8SEiIpIJfa7qUqlUWtuzEh8hBMaOHYvt27fj0KFD8PLy0jru5eUFjUaDX3/9VdqXk5ODo0ePSklNs2bNYG5urtUmPj4ely5d0nviw4oPERGRTBjixs1jxozBpk2b8K9//Qt2dnbSnBy1Wg0rKysoFAqEhIQgPDwctWrVQq1atRAeHg5ra2sMGjRIajt06FBMnDgRjo6OcHBwwKRJk9CwYUN06tSpnFekjYkPERGRXBgg81mxYgUAoF27dlr7161bh+DgYADA5MmTkZmZidGjRyM5ORm+vr7Yv38/7OzspPaLFi2CmZkZ+vfvj8zMTHTs2BEREREwNTUtz9UUwfv4vAJ4Hx8yBryPD8nZi7qPz9k/4/VyH59mtVwrLFZDY8WHiIhIJvisLt2Y+BAREclFGR45UVwfcsZVXURERGQ0WPEhIiKSCUOs6nrVMPEhIiKSC2Y+OnGoi4iIiIwGKz5EREQywVVdujHxISIikgmFHlZ16fmZoC8dDnURERGR0WDFh4iISCY4t1k3Jj5ERERywcxHJyY+REREMsHJzbpxjg8REREZDVZ8iIiIZEIBPazq0kskLy8mPkRERDLBKT66caiLiIiIjAYrPkRERDLBGxjqxsSHiIhINjjYpQuHuoiIiMhosOJDREQkExzq0o2JDxERkUxwoEs3DnURERGR0WDFh4iISCY41KUbEx8iIiKZ4LO6dGPiQ0REJBec5KMT5/gQERGR0WDFh4iISCZY8NGNiQ8REZFMcHKzbhzqIiIiIqPBig8REZFMcFWXbkx8iIiI5IKTfHTiUBcREREZDVZ8iIiIZIIFH92Y+BAREckEV3XpxqEuIiIiMhqs+BAREclG+Vd1yX2wi4kPERGRTHCoSzcOdREREZHRYOJDRERERoNDXURERDLBoS7dmPgQERHJBB9ZoRuHuoiIiMhosOJDREQkExzq0o2JDxERkUzwkRW6caiLiIiIjAYrPkRERHLBko9OTHyIiIhkgqu6dONQFxERERkNVnyIiIhkgqu6dGPiQ0REJBOc4qMbEx8iIiK5YOajE+f4EBERkdFgxYeIiEgmuKpLNyY+REREMsHJzbox8XkFCCEAAI9SUw0cCVHFEfk5hg6BqMIUfr8L/z2vKKl6+Dmhjz5eZkx8XgGPHj0CANT0cjdwJEREVB6PHj2CWq3We78WFhbQaDSopaefExqNBhYWFnrp62WjEBWdflK5FRQU4O7du7Czs4NC7jXIl0Bqairc3d0RFxcHlUpl6HCI9I7f8RdPCIFHjx7Bzc0NJiYVs64oKysLOTn6qZxaWFjA0tJSL329bFjxeQWYmJigatWqhg7D6KhUKv5QIFnjd/zFqohKz5MsLS1lm6zoE5ezExERkdFg4kNERERGg4kP0VOUSiVCQ0OhVCoNHQpRheB3nIwZJzcTERGR0WDFh4iIiIwGEx8iIiIyGkx8iIiIyGgw8SGqAH/88QdatWoFS0tL+Pj4GDocolLbuXMnatasCVNTU4SEhCAiIgKVKlV67jlhYWH8vtNLj4kPyUpiYiJGjhwJDw8PKJVKaDQaBAQE4MSJEy80jtDQUNjY2CA2NhYHDx58oe9Nr4bg4GAoFAppc3R0RNeuXXHhwgVDhwYAGDlyJN58803ExcVh1qxZGDBgAK5evWrosIjKjYkPyUrfvn1x/vx5REZG4urVq9i1axfatWuHBw8evNA4rl27htdeew2enp5wdHR8oe9Nr46uXbsiPj4e8fHxOHjwIMzMzNC9e3dDh4W0tDQkJiYiICAAbm5usLOzg5WVFZydnQ0dGlG5MfEh2Xj48CGOHTuGefPmoX379vD09ETLli0xdepUdOvWDQCgUCiwYsUKBAYGwsrKCl5eXvjhhx+0+rl48SI6dOgAKysrODo6YsSIEUhLS5OOFxQUYObMmahatSqUSiV8fHywb98+6bhCocDZs2cxc+ZMKBQKhIWFvZDrp1dPYVVSo9HAx8cHU6ZMQVxcHJKSkgDo/i4GBwejd+/eWLBgAVxdXeHo6IgxY8YgNzdXarNhwwY0b94cdnZ20Gg0GDRoEBITE58Z05EjR2BnZwcA6NChAxQKBY4cOVLsUNfcuXPh4uICOzs7DB06FFlZWUX6W7duHby9vWFpaYm6deti+fLl5fnIiMpPEMlEbm6usLW1FSEhISIrK6vYNgCEo6OjWLNmjYiNjRWffvqpMDU1FVeuXBFCCJGeni7c3NxEnz59xMWLF8XBgweFl5eXCAoKkvpYuHChUKlUYvPmzeKPP/4QkydPFubm5uLq1atCCCHi4+NF/fr1xcSJE0V8fLx49OhRhV87vXqCgoJEr169pNePHj0SI0eOFDVr1hT5+fkl+i4GBQUJlUolRo0aJWJiYsRPP/0krK2txerVq6U2a9euFXv27BHXrl0TJ06cEK1atRKBgYHPjCs7O1vExsYKAGLbtm0iPj5eZGdni3Xr1gm1Wi21+/7774WFhYVYs2aN+OOPP8S0adOEnZ2daNy4sdRm9erVwtXVVWzbtk1cv35dbNu2TTg4OIiIiAh9fIREZcLEh2Tlxx9/FPb29sLS0lK0bt1aTJ06VZw/f146DkCMGjVK6xxfX1/x/vvvCyEe/0Ntb28v0tLSpOM///yzMDExEQkJCUIIIdzc3MTs2bO1+mjRooUYPXq09Lpx48YiNDRU35dHMhIUFCRMTU2FjY2NsLGxEQCEq6urOHv2rBCiZN/FoKAg4enpKfLy8qQ2/fr1EwMGDHjm+54+fVoAeG5CnpycLACIw4cPS/ueTnz8/PyK/bv0ZOLj7u4uNm3apNVm1qxZws/P75nvTVTRONRFstK3b1/cvXsXu3btQkBAAI4cOYKmTZsiIiJCauPn56d1jp+fH2JiYgAAMTExaNy4MWxsbKTj/v7+KCgoQGxsLFJTU3H37l34+/tr9eHv7y/1QVRS7du3R3R0NKKjo3Hq1Cl06dIFgYGBuHXrls7vYqH69evD1NRUeu3q6qo1lHXu3Dn06tULnp6esLOzQ7t27QAAt2/fls63tbWFra0tAgMDSxx7TExMsX+XCiUlJSEuLg5Dhw6V+re1tcXnn3+Oa9eulfh9iPTNzNABEOmbpaUlOnfujM6dO2P69OkYNmwYQkNDERwc/MxzFAoFAEAIIf3+WW2e/r2u84iexcbGBjVr1pReN2vWDGq1GmvWrCnxd9Hc3LzIsYKCAgBAeno6unTpgi5dumDDhg2oXLkybt++jYCAAOTk5AAA9uzZI80JsrKy0tu1FcawZs0a+Pr6ah17MlEjetFY8SHZq1evHtLT06XXJ0+e1Dp+8uRJ1K1bV2obHR2t1f63336DiYkJateuDZVKBTc3Nxw7dkyrj+PHj8Pb27sCr4KMgUKhgImJCTIzM3V+F0vijz/+wD///IO5c+eiTZs2qFu3bpGJzZ6enqhZsyZq1qyJKlWqlDhWb2/vYv8uFXJxcUGVKlVw/fp1qf/CzcvLq8TvQ6RvrPiQbNy/fx/9+vXDkCFD0KhRI9jZ2eHMmTOYP38+evXqJbX74Ycf0Lx5c7z22mvYuHEjTp8+jbVr1wIABg8ejNDQUAQFBSEsLAxJSUkYN24c3nnnHbi4uAAAPvroI4SGhqJGjRrw8fHBunXrEB0djY0bNxrkuunVlZ2djYSEBABAcnIyli1bhrS0NPTo0QMtW7bU+V3UxcPDAxYWFli6dClGjRqFS5cuYdasWXqJfcKECQgKCtL6u3T58mVUr15dahMWFobx48dDpVIhMDAQ2dnZOHPmDJKTk/Hhhx/qJQ6iUjPwHCMivcnKyhIff/yxaNq0qVCr1cLa2lrUqVNHfPrppyIjI0MI8Xhy89dffy06d+4slEql8PT0FJs3b9bq58KFC6J9+/bC0tJSODg4iOHDh2tNBM3PzxczZswQVapUEebm5qJx48Zi7969Wn1wcjPpEhQUJABIm52dnWjRooX48ccfpTa6votPrwwTQogJEyaItm3bSq83bdokqlWrJpRKpfDz8xO7du0SAMS5c+eeGVtJJjcLIcTs2bOFk5OTsLW1FUFBQWLy5Mlak5uFEGLjxo3Cx8dHWFhYCHt7e/H666+L7du3l/RjItI7hRBCGDLxInqRFAoFduzYgd69exs6FCIiMgDO8SEiIiKjwcSHiIiIjAYnN5NR4cguEZFxY8WHiIiIjAYTHyIiIjIaTHyIiIjIaDDxISIiIqPBxIeISiQsLAw+Pj7S6+DgYIPcD+nmzZtQKBSIjo5+Zptq1aph8eLFJe4zIiIClSpVKndsCoUCO3fuLHc/RFRxmPgQvcKCg4OhUCigUChgbm6O6tWrY9KkSVrPd6ooX331ldZT75+nJMkKEdGLwOXsRK+4rl27Yt26dcjNzcV//vMfDBs2DOnp6VixYkWRtrm5uUWe5l1WarVaL/0QEb1IrPgQveKUSiU0Gg3c3d0xaNAgDB48WBpuKRye+vbbb1G9enUolUoIIZCSkoIRI0bA2dkZKpUKHTp0wPnz57X6nTt3LlxcXGBnZ4ehQ4ciKytL6/jTQ10FBQWYN28eatasCaVSCQ8PD8yePRsApKdxN2nSBAqFAu3atZPOW7duHby9vWFpaYm6deti+fLlWu9z+vRpNGnSBJaWlmjevDnOnTtX6s9o4cKFaNiwIWxsbODu7o7Ro0cjLS2tSLudO3eidu3asLS0ROfOnREXF6d1/KeffkKzZs1gaWmJ6tWrY8aMGcjLyyt1PERkOEx8iGTGysoKubm50uu//voLW7duxbZt26Shpm7duiEhIQF79uzB2bNn0bRpU3Ts2BEPHjwAAGzduhWhoaGYPXs2zpw5A1dX1yIJydOmTp2KefPm4bPPPsOVK1ewadMm6Snip0+fBgAcOHAA8fHx2L59OwBgzZo1mDZtGmbPno2YmBiEh4fjs88+Q2RkJAAgPT0d3bt3R506dXD27FmEhYVh0qRJpf5MTExMsGTJEly6dAmRkZE4dOgQJk+erNUmIyMDs2fPRmRkJH777TekpqZi4MCB0vFffvkFb7/9NsaPH48rV65g1apViIiIkJI7InpFGPYZqURUHk8/nfvUqVPC0dFR9O/fXwghRGhoqDA3NxeJiYlSm4MHDwqVSiWysrK0+qpRo4ZYtWqVEEIIPz8/MWrUKK3jvr6+Wk/efvK9U1NThVKpFGvWrCk2zhs3bhT7RHB3d3exadMmrX2zZs0Sfn5+QgghVq1aJRwcHER6erp0fMWKFTqfLu7p6SkWLVr0zONbt24Vjo6O0ut169YJAOLkyZPSvpiYGAFAnDp1SgghRJs2bUR4eLhWP+vXrxeurq7SawBix44dz3xfIjI8zvEhesXt3r0btra2yMvLQ25uLnr16oWlS5dKxz09PVG5cmXp9dmzZ5GWlgZHR0etfjIzM3Ht2jUAQExMDEaNGqV13M/PD4cPHy42hpiYGGRnZ6Njx44ljjspKQlxcXEYOnQohg8fLu3Py8uT5g/FxMSgcePGsLa21oqjtA4fPozw8HBcuXIFqampyMvLQ1ZWFtLT02FjYwMAMDMzQ/PmzaVz6tati0qVKiEmJgYtW7bE2bNnERUVpVXhyc/PR1ZWFjIyMrRiJKKXFxMfoldc+/btsWLFCpibm8PNza3I5OXCH+yFCgoK4OrqiiNHjhTpq6xLuq2srEp9TkFBAYDHw12+vr5ax0xNTQHo59lqt27dwhtvvIFRo0Zh1qxZcHBwwLFjxzB06FCtIUHg8XL0pxXuKygowIwZM9CnT58ibSwtLcsdJxG9GEx8iF5xNjY2qFmzZonbN23aFAkJCTAzM0O1atWKbePt7Y2TJ0/i3XfflfadPHnymX3WqlULVlZWOHjwIIYNG1bkuIWFBYDHFZJCLi4uqFKlCq5fv47BgwcX22+9evWwfv16ZGZmSsnV8+IozpkzZ5CXl4cvv/wSJiaPpzVu3bq1SLu8vDycOXMGLVu2BADExsbi4cOHqFu3LoDHn1tsbGypPmsievkw8SEyMp06dYKfnx969+6NefPmoU6dOrh79y727NmD3r17o3nz5pgwYQKCgoLQvHlzvPbaa9i4cSMuX76M6tWrF9unpaUlpkyZgsmTJ8PCwgL+/v5ISkrC5cuXMXToUDg7O8PKygr79u1D1apVYWlpCbVajbCwMIwfPx4qlQqBgYHIzs7GmTNnkJycjA8//BCDBg3CtGnTMHToUHz66ae4efMmFixYUKrrrVGjBvLy8rB06VL06NEDv/32G1auXFmknbm5OcaNG4clS5bA3NwcY8eORatWraREaPr06ejevTvc3d3Rr18/mJiY4MKFC7h48SI+//zz0v9BEJFBcFUXkZFRKBTYs2cPXn/9dQwZMgS1a9fGwIEDcfPmTWkV1oABAzB9+nRMmTIFzZo1w61bt/D+++8/t9/PPvsMEydOxPTp0+Ht7Y0BAwYgMTERwOP5M0uWLMGqVavg5uaGXr16AQCGDRuGb775BhEREWjYsCHatm2LiIgIafm7ra0tfvrpJ1y5cgVNmjTBtGnTMG/evFJdr4+PDxYuXIh58+ahQYMG2LhxI+bMmVOknbW1NaZMmYJBgwbBz88PVlZW2LJli3Q8ICAAu3fvxq+//ooWLVqgVatWWLhwITw9PUsVDxEZlkLoYxCdiIiI6BXAig8REREZDSY+REREZDSY+BAREZHRYOJDRERERoOJDxERERkNJj5ERERkNJj4EBERkdFg4kNERERGg4kPERERGQ0mPkRERGQ0mPgQERGR0WDiQ0REREbj/wHsmOTuIHZO4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_on_test_set(model, test_dataset, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            preds = (outputs > 0.5).int().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/test_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_test\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "evaluate_on_test_set(model, test_dataset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
