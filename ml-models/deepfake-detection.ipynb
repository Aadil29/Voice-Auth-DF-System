{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Standard Libraries =====\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# ===== Data Handling =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== Audio Processing =====\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import IPython.display as ipd\n",
    "\n",
    "# ===== PyTorch =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ===== Evaluation & Metrics =====\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# ===== Visualization & Utilities =====\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if cuda is working\n",
    "print(torch.cuda.is_available())    #should say true if available     \n",
    "print(torch.cuda.get_device_name(0))   #should say the GPU you have, i.e for me it says NVIDIA GeForce RTX 3060 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of Dataset, i.e the number of total files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to see the number of files in the root of release-in-the-wild dataset\n",
    "\n",
    "def count_root_files(root_dir):\n",
    "    return len([\n",
    "        f for f in os.listdir(root_dir) # List all items in the folder\n",
    "        if os.path.isfile(os.path.join(root_dir, f))  # Only count actual files so no folders \n",
    "    ])\n",
    "\n",
    "#path to the dataset \n",
    "folder_path = \"datasets/release_in_the_wild\"\n",
    "file_count = count_root_files(folder_path)\n",
    "\n",
    "#printing the result\n",
    "print(f\"Total files directly in '{folder_path}': {file_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Sample, Split and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "\n",
    "# Samples 40% of a deepfake audio dataset,\n",
    "# splits it into stratified train/val/test sets (70/15/15),\n",
    "# and saves both the split metadata and the .wav\n",
    "# files themselves into separate folders.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Setting random seed to ensure reproducability\n",
    "RANDOM_SEED = 42\n",
    "folder_path = \"datasets/release_in_the_wild\"\n",
    "meta_csv_path = os.path.join(folder_path, \"meta.csv\")\n",
    "WAV_DIR = folder_path\n",
    "\n",
    "# Defining the output directory of the spilt, so once the data takes that 40 split that data is split into train/val/test\n",
    "OUTPUT_DIRS = {\n",
    "    \"train\": os.path.join(folder_path, \"train\"),\n",
    "    \"val\": os.path.join(folder_path, \"val\"),\n",
    "    \"test\": os.path.join(folder_path, \"test\")\n",
    "}\n",
    "\n",
    "# Create output folders\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Load and clean csv files \n",
    "df = pd.read_csv(meta_csv_path)\n",
    "df[\"file\"] = df[\"file\"].str.strip()\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"speaker\"] = df[\"speaker\"].str.strip()\n",
    "\n",
    "# Sample 40% of full dataset, stratified by label \n",
    "df_40, _ = train_test_split(df, train_size=0.4, stratify=df[\"label\"], random_state=RANDOM_SEED)\n",
    "\n",
    "# Split sample into Train (70%), Val (15%), Test (15%) \n",
    "train_df, temp_df = train_test_split(df_40, test_size=0.30, stratify=df_40[\"label\"], random_state=RANDOM_SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "train_df = train_df.sample(frac=1.0, random_state=RANDOM_SEED)  # just shuffle it\n",
    "\n",
    "# File copying function\n",
    "def copy_files(subset_df, split_name):\n",
    "    for _, row in subset_df.iterrows():\n",
    "        src = os.path.join(WAV_DIR, row[\"file\"])\n",
    "        dst = os.path.join(OUTPUT_DIRS[split_name], row[\"file\"])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "\n",
    "# Copy files (uncomment to run if dataset has not yet been split) \n",
    "#copy_files(train_df, \"train\")\n",
    "#copy_files(val_df, \"val\")\n",
    "#copy_files(test_df, \"test\")\n",
    "\n",
    "\n",
    "# Save csv (uncomment to run if csv files not yet created)\n",
    "#train_df.to_csv(os.path.join(folder_path, \"train_meta.csv\"), index=False)\n",
    "#val_df.to_csv(os.path.join(folder_path, \"val_meta.csv\"), index=False)\n",
    "#test_df.to_csv(os.path.join(folder_path, \"test_meta.csv\"), index=False)\n",
    "\n",
    "#  checking to ensure splits were done correct and rechecking the split of labels (spoof or bona-fide) for each sub set\n",
    "def count_files(folder):\n",
    "    return len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "\n",
    "print(\"40% of dataset sampled and split into train/val/test (70/15/15).\")\n",
    "print(\"Spoof samples oversampled in train set only.\")\n",
    "print(f\"Train files: {count_files(OUTPUT_DIRS['train'])}\")\n",
    "print(f\"Val files: {count_files(OUTPUT_DIRS['val'])}\")\n",
    "print(f\"Test files: {count_files(OUTPUT_DIRS['test'])}\")\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# This function prepares raw audio to be ready for model input.\n",
    "# It:\n",
    "# - Removes silence at the start/end of the audio.\n",
    "# - Optionally applies noise reduction to clean the signal.\n",
    "# - Optionally applies pre-emphasis to boost high frequencies.\n",
    "# - Normalises volume using RMS or peak method.\n",
    "# - Ensures the audio is a fixed length by padding or trimming.\n",
    "# ================================================================\n",
    "\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.5, normalise='rms'):\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Apply noise reduction / dereverberation\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    # Apply pre-emphasis\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Normalisation\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Pad or trim to target length\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio data visualisation - (see effect of preprocessing function on audio samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# This function compares real vs fake audio in two ways:\n",
    "# 1. Before preprocessing (original)\n",
    "# 2. After preprocessing (denoised, normalised, fixed-length)\n",
    "#\n",
    "# It visualises:\n",
    "# - Waveforms side by side\n",
    "# - Mel spectrograms side by side\n",
    "# - Also plays back the audio for listening comparison\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def load_audio(path, sr=22050):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "# Visualise waveform comparison between real and fake audio\n",
    "def show_waveform_comparison(y_real, y_fake, sr, title_suffix=\"\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 3))\n",
    "    librosa.display.waveshow(y_real, sr=sr, ax=axes[0])\n",
    "    axes[0].set_title(f\"Real - Waveform {title_suffix}\")\n",
    "    librosa.display.waveshow(y_fake, sr=sr, ax=axes[1])\n",
    "    axes[1].set_title(f\"Fake - Waveform {title_suffix}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise mel spectrogram comparison\n",
    "def show_mel_spectrogram_comparison(y_real, y_fake, sr, title_suffix=\"\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    mel_real = librosa.feature.melspectrogram(y=y_real, sr=sr, n_mels=128)\n",
    "    mel_fake = librosa.feature.melspectrogram(y=y_fake, sr=sr, n_mels=128)\n",
    "\n",
    "    db_real = librosa.power_to_db(mel_real, ref=np.max)\n",
    "    db_fake = librosa.power_to_db(mel_fake, ref=np.max)\n",
    "\n",
    "    img1 = librosa.display.specshow(db_real, sr=sr, x_axis='time', y_axis='mel', ax=axes[0])\n",
    "    axes[0].set_title(f\"Real - Mel Spectrogram {title_suffix}\")\n",
    "    fig.colorbar(img1, ax=axes[0], format='%+2.0f dB')\n",
    "\n",
    "    img2 = librosa.display.specshow(db_fake, sr=sr, x_axis='time', y_axis='mel', ax=axes[1])\n",
    "    axes[1].set_title(f\"Fake - Mel Spectrogram {title_suffix}\")\n",
    "    fig.colorbar(img2, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# compares both audio files, but putting them side by side to be able to celay visaly see difference based on post and pre processing\n",
    "def compare_audio(real_path, fake_path, apply_preprocessing=False, sr=22050):\n",
    "    y_real = load_audio(real_path, sr=sr)\n",
    "    y_fake = load_audio(fake_path, sr=sr)\n",
    "\n",
    "    if apply_preprocessing:\n",
    "        y_real = preprocess_audio(y_real, sr, apply_preemphasis=True, apply_reduction=True, normalise='rms')\n",
    "        y_fake = preprocess_audio(y_fake, sr, apply_preemphasis=True, apply_reduction=True, normalise='rms')\n",
    "\n",
    "    label = \"(Preprocessed)\" if apply_preprocessing else \"(Original)\"\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "\n",
    "    # Waveform\n",
    "    show_waveform_comparison(y_real, y_fake, sr, title_suffix=label)\n",
    "\n",
    "    # able to listen to the audio files\n",
    "    print(\"Real Audio:\")\n",
    "    ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "    print(\"Fake Audio:\")\n",
    "    ipd.display(ipd.Audio(y_fake, rate=sr))\n",
    "\n",
    "    # Mel spectrogram\n",
    "    show_mel_spectrogram_comparison(y_real, y_fake, sr, title_suffix=label)\n",
    "\n",
    "# Paths to files\n",
    "real_audio_path = \"datasets/release_in_the_wild/72.wav\"\n",
    "fake_audio_path = \"datasets/release_in_the_wild/2.wav\"\n",
    "\n",
    "# Run comparisons\n",
    "compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=False)\n",
    "compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising each feature I intend to extract to help see differences in bona-fide and spoof audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# This function visually compares feature representations of\n",
    "# a real and a fake audio clip after preprocessing and feature extraction.\n",
    "# \n",
    "# It extracts the 10 features using librosa:\n",
    "# - Mel Spectrogram, MFCC, Chroma, Tonnetz, Spectral Contrast,\n",
    "#   Spectral Centroid, Pitch (YIN), Energy (RMS), ZCR, Onset Strength\n",
    "#\n",
    "# For each feature, it shows side-by-side plots:\n",
    "#   Left: Real audio\n",
    "#   Right: Fake audio.\n",
    "# ================================================================\n",
    "\n",
    "class AudioFeatureComparator:\n",
    "    def __init__(self, y_real, y_fake, sr):\n",
    "        self.sr = sr\n",
    "        \n",
    "        # Extract features for both real and fake audio\n",
    "        self.features_real = self._extract_features(y_real)\n",
    "        self.features_fake = self._extract_features(y_fake)\n",
    "\n",
    "    def _extract_features(self, y):\n",
    "        return {\n",
    "            \"mel_spectrogram\": librosa.power_to_db(\n",
    "                librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=128), ref=np.max\n",
    "            ),\n",
    "            \"mfcc\": librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=20),\n",
    "            \"chroma\": librosa.feature.chroma_stft(y=y, sr=self.sr),\n",
    "            \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=self.sr),\n",
    "            \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=self.sr),\n",
    "            \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=self.sr),\n",
    "            \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=self.sr),\n",
    "            \"energy\": librosa.feature.rms(y=y),\n",
    "            \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "            \"onset_strength\": librosa.onset.onset_strength(y=y, sr=self.sr),\n",
    "        }\n",
    "\n",
    "    def plot_feature(self, key, y_axis='linear'):\n",
    "       # Plot one feature side-by-side (spoof vs bona-fide)\n",
    "        feat_real = self.features_real[key]\n",
    "        feat_fake = self.features_fake[key]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 3))\n",
    "        for ax, feat, title in zip(\n",
    "            axes, [feat_real, feat_fake], [\"Real\", \"Fake\"]\n",
    "        ):\n",
    "             # For flat features like pitch/energy/ZCR\n",
    "            if feat.ndim == 1 or feat.shape[0] == 1:\n",
    "                ax.plot(feat.T)\n",
    "                ax.set_title(f\"{title} - {key}\")\n",
    "                ax.set_xlabel(\"Frames\")\n",
    "            # For 2D features like mel or MFCC\n",
    "            else:\n",
    "                img = librosa.display.specshow(\n",
    "                    feat, sr=self.sr, x_axis='time', y_axis=y_axis, ax=ax\n",
    "                )\n",
    "                fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "                ax.set_title(f\"{title} - {key}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all(self):\n",
    "        axis_map = {\n",
    "            \"mel_spectrogram\": \"mel\",\n",
    "            \"mfcc\": \"linear\",\n",
    "            \"chroma\": \"chroma\",\n",
    "            \"tonnetz\": \"tonnetz\",\n",
    "            \"spectral_contrast\": \"linear\",\n",
    "            \"spectral_centroid\": \"linear\",\n",
    "            \"pitch\": \"linear\",\n",
    "            \"energy\": \"linear\",\n",
    "            \"zcr\": \"linear\",\n",
    "            \"onset_strength\": \"linear\"\n",
    "        }\n",
    "        for key in self.features_real:\n",
    "            self.plot_feature(key, y_axis=axis_map.get(key, 'linear'))\n",
    "\n",
    "# Preprocess two example files\n",
    "y_real = preprocess_audio(load_audio(\"datasets/release_in_the_wild/72.wav\"), sr=22050, apply_preemphasis=True, apply_reduction=True)\n",
    "y_fake = preprocess_audio(load_audio(\"datasets/release_in_the_wild/2.wav\"), sr=22050, apply_preemphasis=True, apply_reduction=True)\n",
    "\n",
    "# Create the visual comparator\n",
    "comparator = AudioFeatureComparator(y_real, y_fake, sr=22050)\n",
    "comparator.plot_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction Function (With saving feature to save time for future training/validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# This function loads all `.wav` audio files from the specified \n",
    "# input folder structure (train/val/test) and:\n",
    "# - Applies preprocessing (normalisation, padding, etc.)\n",
    "# - Extracts the 10 audio features using librosa\n",
    "# - Saves the features as `.npy` files under:\n",
    "#     output_root/preprocessed_<split>/<feature_name>/<filename>.npy\n",
    "#\n",
    "# This allows the model to train on consistent, aligned features\n",
    "# across all sets, and ready to be passed into the the audiofeatuedataset \n",
    "# ================================================================\n",
    "\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\",\"test\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                # Load audio\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                \n",
    "                # Apply standard preprocessing\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "                # using the same feature extraction settings as seen in audio data visualisation\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "                # Save each feature to its own folder under train/val/test\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "#Uncomment the line below to preproecess and extact the 5 features types (for train/val/test) if not already done\n",
    "\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Custom Dataset class for deepfake audio classification.\n",
    "# \n",
    "# - Loads file paths and labels from a meta CSV (bona-fide / spoof).\n",
    "# - Loads 10 pre-extracted features (e.g. MFCC, pitch, ZCR, etc.)\n",
    "#   stored as `.npy` files.\n",
    "# - Each feature is padded or cropped to a uniform shape for batching.\n",
    "# - Returns a tuple: (features..., label) for training or testing.\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):  \n",
    "        \n",
    "        # Load metadata from CSV\n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].str.strip().str.lower()\n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {'bona-fide': 1, 'spoof': 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    # Ensure all audio files are the same length\n",
    "    def pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            # crop if larger than target\n",
    "            tensor = tensor[:target_shape[0], :target_shape[1]] \n",
    "        else:\n",
    "            # pad if smaller than target\n",
    "            tensor = F.pad(tensor, (0, pad_w, 0, pad_h))  \n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, feat, file_id) \n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "            feature = np.load(path)\n",
    "            tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self.pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture (DenseNN + Siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "#\n",
    "# Architecture for the binary classification hybrid model\n",
    "# - Processes 10 audio features (MFCC, pitch, energy, etc.)\n",
    "# - MFCC handled by a CNN (Siamese-style)\n",
    "# - All other features pass through feedforward dense branches\n",
    "# - Final output: sigmoid score (0 = fake, 1 = real)\n",
    "# ===============================================================\n",
    "\n",
    "# Used for the all features  but mfcc\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "       \n",
    "        # First dense layer with batch norm\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # Second dense layer with batch norm\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # Final dense layer to reduce to output_dim\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        # Dropout for regularisation and ReLU activation\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SiameseMFCCBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMFCCBranch, self).__init__()\n",
    "        # 1st convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # 2nd convolutional layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # 3rd convolutional layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Flattened size expected after conv + pooling\n",
    "        self.flattened_size = 128 * 32 * 64\n",
    "        # Fully connected layer to get 128-dim output\n",
    "        self.fc = nn.Linear(self.flattened_size, 128)\n",
    "        # dropout after FC\n",
    "        self.fc_dropout = nn.Dropout(0.3)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))       \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten to ensure shape compatability \n",
    "        x = x.view(x.size(0), -1)     \n",
    "        x = self.fc(x)                 \n",
    "        return x\n",
    "\n",
    "\n",
    "# Final Fusion Model\n",
    "class AudioDeepfakeFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioDeepfakeFusionModel, self).__init__()\n",
    "\n",
    "        # CNN-based branch for MFCC\n",
    "        self.mfcc_branch = SiameseMFCCBranch()\n",
    "\n",
    "        # MLP branches for the remaining 9 features\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.pitch_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.energy_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.zcr_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.onset_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.centroid_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.mel_spec_branch = DenseNeuralNetwork(input_dim=128)\n",
    "\n",
    "        # Fusion layer that combines all 10 feature vectors into a larger vector\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "           \n",
    "            # Combine all branches\n",
    "            nn.Linear(10 * 128, 512),  \n",
    "            #Adding ReLU activation function    \n",
    "            nn.ReLU(),\n",
    "            # Adding 0.3 dropout to help prevent overfitting \n",
    "            nn.Dropout(0.3),\n",
    "           \n",
    "            # Reduce to 256-dim\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Final classifier layer (binary output)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, mfcc, chroma, tonnetz, contrast, pitch, energy, zcr, onset, centroid, mel_spec):\n",
    "\n",
    "        # MFCC input is 2D, add channel dim for CNN: (B, 1, H, W)\n",
    "        mfcc = mfcc.unsqueeze(1)  \n",
    "\n",
    "        # Pooling for time-dimension on all 1D features\n",
    "        def pool(x): return x.mean(dim=-1)\n",
    "\n",
    "        # Forward through each branch\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(pool(chroma))\n",
    "        tonnetz_out = self.tonnetz_branch(pool(tonnetz))\n",
    "        contrast_out = self.contrast_branch(pool(contrast))\n",
    "        pitch_out = self.pitch_branch(pool(pitch))\n",
    "        energy_out = self.energy_branch(pool(energy))\n",
    "        zcr_out = self.zcr_branch(pool(zcr))\n",
    "        onset_out = self.onset_branch(pool(onset))\n",
    "        centroid_out = self.centroid_branch(pool(centroid))\n",
    "        mel_spec_out = self.mel_spec_branch(pool(mel_spec))\n",
    "\n",
    "        # Concatenate all feature vectors into one\n",
    "        fusion = torch.cat([\n",
    "            mfcc_out, chroma_out, tonnetz_out, contrast_out,\n",
    "            pitch_out, energy_out, zcr_out, onset_out, centroid_out, mel_spec_out\n",
    "        ], dim=1)\n",
    "\n",
    "        # Forward pass through fusion and output layer\n",
    "        x = self.fusion_layer(fusion)\n",
    "\n",
    "        # Binary output in range [0, 1]\n",
    "        return torch.sigmoid(self.output_layer(x))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "# ===============================================================\n",
    "# this function is used to create metrics for deepfake detection.\n",
    "# - Calculates accuracy, precision, recall, F1, AUC, and EER.\n",
    "# - Optionally plots ROC curve and confusion matrix side by side.\n",
    "# - Can be used during training or evaluation phase.\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def evaluate_verification_metrics(preds, probs, labels, threshold=0.5, title=\"ROC Curve\", plot=True):\n",
    "    \n",
    "    # Convert predicted probabilities into binary predictions using threshold\n",
    "    preds = (np.array(probs) >= threshold).astype(int)\n",
    "\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    labels = np.array(labels)\n",
    "    probs = np.array(probs)\n",
    "\n",
    "    #  Compute classification metrics \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds, zero_division=0)\n",
    "    rec = recall_score(labels, preds, zero_division=0)\n",
    "    f1 = f1_score(labels, preds, zero_division=0)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "\n",
    "    # Compute EER (Equal Error Rate) \n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    eer = fpr[np.nanargmin(np.abs((1 - tpr) - fpr))]\n",
    "\n",
    "    # Print result\n",
    "    print(\n",
    "        f\"{title} | \"\n",
    "        f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | \"\n",
    "        f\"F1: {f1:.4f} | AUC: {auc:.4f} | EER: {eer:.4f}\"\n",
    "    )\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # ROC Curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=plt.gca(), colorbar=False)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    #  Return metrics as a dictionary for logging\n",
    "    return {\n",
    "        'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "        'f1_score': f1, 'auc': auc, 'eer': eer\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Train loop to train the binary classifer\n",
    "#  - Uses BCELoss for binary output (sigmoid).\n",
    "# - Tracks loss and accuracy per epoch.\n",
    "# - Evaluates performance with precision, recall, F1, AUC, and EER.\n",
    "# - Optionally saves model weights and plots training history.\n",
    "# - not the best version of this code,it good weights were recored\n",
    "#  in pth_models, hence difference in result here/ this one is \n",
    "# overfitting, and generalising well!\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def train_binary(\n",
    "    model,\n",
    "    dataset,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_model_path=\"model-weights/binary_model.pth\"\n",
    "):\n",
    "    # Move model to the appropriate device (GPU or CPU) \n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    # Define loss function (Binary Cross Entropy)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    save_model_path = Path(save_model_path)\n",
    "    save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Lists to track metrics\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "\n",
    "     # === Training loop ===\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "        # tqdm progress bar per epoch\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in loop:\n",
    "\n",
    "            # Unpack and move inputs/labels to device\n",
    "            *inputs, labels = batch\n",
    "            inputs = [x.to(device) for x in inputs]\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert outputs to predictions\n",
    "            probs = outputs.detach().cpu().numpy().flatten()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "            # Store for metric tracking\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(truths)\n",
    "\n",
    "            # Show loss live during training\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # End of epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        epoch_accuracies.append(acc)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n[Epoch {epoch}] Duration: {duration:.2f}s | Loss: {avg_loss:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "        # Evaluate metrics (without plotting roc curve and feature matrix during training, only at the end)\n",
    "        evaluate_verification_metrics(all_preds, all_probs, all_labels, threshold=0.5, plot=False)\n",
    "\n",
    "        # Step LR scheduler\n",
    "        scheduler.step()\n",
    "        for pg in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {pg['lr']:.6f}\")\n",
    "\n",
    "    # Save final model weights\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    print(f\"\\nModel saved to '{save_model_path}'\")\n",
    "\n",
    "    # Final evaluation with visualisation\n",
    "    evaluate_verification_metrics(all_preds, all_probs, all_labels, threshold=0.5, title=\"Final Evaluation\", plot=True)\n",
    "\n",
    "    # Plotting loss and accuracy curves \n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Loss curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), epoch_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs + 1), epoch_accuracies, marker='o', color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Over Epochs\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run the train binary classification model\n",
    "\n",
    "\"\"\"\n",
    "train_binary(\n",
    "    model=AudioDeepfakeFusionModel(),\n",
    "    dataset=AudioFeatureDataset(\n",
    "        \"datasets/release_in_the_wild/train_meta.csv\",  \n",
    "        \"datasets/release_in_the_wild/preprocessed_train\"  \n",
    "    ),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    lr=0.0005,\n",
    "    device='cuda',\n",
    "    save_model_path=\"model-weights/df_model.pth\"\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Train + Val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Evaluates trained binary deepfake detection model on a test dataset.\n",
    "# - Computes: Accuracy, Precision, Recall, F1 Score, AUC, and EER\n",
    "# - Displays a Confusion Matrix\n",
    "# - Takes: a model, test dataset, and runs inference using no gradients\n",
    "# ===============================================================\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Prepare test data loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Set model to evaluation mode and move to device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to collect predictions and labels\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "#\n",
    "    # No gradients needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Split inputs and labels, move to device\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            # Model inference\n",
    "            outputs = model(*inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            # Collect batch results\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert all results to numpy arrays\n",
    "    all_labels = np.array(all_labels).astype(int)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Compute metrics \n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc  = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Compute Equal Error Rate (EER)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Print results \n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    #Show confusion matrix \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model instance\n",
    "model = AudioDeepfakeFusionModel()\n",
    "\n",
    "# Load model weights \n",
    "model.load_state_dict(torch.load(\"model-weights/df_model.pth\"))\n",
    "\n",
    "# Load test dataset \n",
    "release_in_wild_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_on_test_set(model, release_in_wild_test_dataset)\n",
    "\n",
    "\n",
    "# Load test dataset \n",
    "release_in_wild_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/test_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_test\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_on_test_set(model, release_in_wild_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATING ON NEW DATASETS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model\n",
    "For 2-sec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# This function loads all `.wav` audio files from the specified \n",
    "# input folder structure (ttest) and:\n",
    "# - Applies preprocessing \n",
    "# - Extracts the 10 audio features using librosa\n",
    "# - Saves the features as `.npy` files under:\n",
    "#     output_root/preprocessed_<split>/<feature_name>/<filename>.npy\n",
    "#\n",
    "# This allows the model to train on consistent, aligned features\n",
    "# across all sets, and ready to be passed into the the audiofeatuedataset \n",
    "# ================================================================\n",
    "\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0,\n",
    "                         apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"testing\"]:\n",
    "        for label_dir in [\"real\", \"fake\"]:\n",
    "            input_folder = input_root / split / label_dir\n",
    "            output_base = output_root / f\"preprocessed_{split}\" / label_dir\n",
    "\n",
    "            print(f\"Looking in: {input_folder}\")\n",
    "            wav_files = sorted(list(input_folder.glob(\"*.wav\")))\n",
    "            print(f\"Found {len(wav_files)} files in '{split}/{label_dir}'\")\n",
    "\n",
    "            for wav_file in tqdm(wav_files, desc=f\"Processing {label_dir}\"):\n",
    "                try:\n",
    "                    y, _ = librosa.load(wav_file, sr=sr)\n",
    "                    y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                    clean_name = wav_file.stem.split(\".\")[0] + \".npy\"\n",
    "\n",
    "                    feature_dict = {\n",
    "                        \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                        \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                        \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                        \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                        \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                        \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                        \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                        \"energy\": librosa.feature.rms(y=y),\n",
    "                        \"zcr\": librosa.feature.zero_crossing_rate(y=y),\n",
    "                        \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                    }\n",
    "\n",
    "                    for feature_name, data in feature_dict.items():\n",
    "                        out_path = output_base / feature_name / clean_name\n",
    "                        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "\"\"\"\n",
    "extract_and_save_all(\n",
    "    input_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    output_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    sr=16000, target_duration=6.0\n",
    ")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_meta_csv(real_dir, fake_dir, save_path):\n",
    "    entries = []\n",
    "    for path in sorted(Path(real_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]  \n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 1})\n",
    "    for path in sorted(Path(fake_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]\n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 0})\n",
    "    pd.DataFrame(entries).to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "generate_test_meta_csv(\n",
    "    real_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/real\",\n",
    "    fake_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/fake\",\n",
    "    save_path=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Custom Dataset class for deepfake audio classification.\n",
    "# \n",
    "# - Loads file paths and labels from a meta CSV (bona-fide / spoof).\n",
    "# - Loads 10 pre-extracted features (e.g. MFCC, pitch, ZCR, etc.)\n",
    "#   stored as `.npy` files.\n",
    "# - Each feature is padded or cropped to a uniform shape for batching.\n",
    "# - Returns a tuple: (features..., label) for training or testing.\n",
    "# ===============================================================\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):\n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int) \n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {1: 1, 0: 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:target_shape[0], :target_shape[1]]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        label_dir = \"real\" if label == 1 else \"fake\"\n",
    "\n",
    "        # Construct the file path\n",
    "        file_id = row[\"file\"]\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, label_dir, feat, file_id)\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "            arr = np.load(path)\n",
    "            tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self._pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Evaluates trained binary deepfake detection model on a test dataset.\n",
    "# - Computes: Accuracy, Precision, Recall, F1 Score, AUC, and EER\n",
    "# - Displays a Confusion Matrix\n",
    "# - Takes: a model, test dataset, and runs inference using no gradients\n",
    "# ===============================================================\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset,\n",
    "                         batch_size=32,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Create DataLoader for batching test data\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Set model to evaluation mode and move it to the correct device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Containers for predicted probabilities, labels, and binary predictions\n",
    "    all_probs, all_preds_05, all_labels = [], [], []\n",
    "\n",
    "    # Inference without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            \n",
    "            # Unpack features and labels from batch\n",
    "            *features, labels = batch\n",
    "\n",
    "            # Move features and labels to device\n",
    "            features = [f.to(device) for f in features]\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            # Forward pass through model\n",
    "            outputs = model(*features)\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "            # Binary prediction at threshold 0.5\n",
    "            preds_05 = (probs > 0.5).astype(int)\n",
    "\n",
    "            # Collect results\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds_05.extend(preds_05.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs  = np.array(all_probs)\n",
    "    all_preds_05 = np.array(all_preds_05)\n",
    "\n",
    "    # ROC Curve & AUC \n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    #  Equal Error Rate (EER) \n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Find optimal threshold (maximizing TPR - FPR) \n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_optimal = (all_probs > optimal_threshold).astype(int)\n",
    "\n",
    "    #  Evaluation at default threshold (0.5) \n",
    "    print(\"\\n--- Evaluation at Threshold = 0.5 ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(all_labels, all_preds_05):.4f}\")\n",
    "    print(f\"Precision:{precision_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:{recall_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:{f1_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "\n",
    "    #  Evaluation at optimal threshold \n",
    "    print(\"\\n--- Evaluation at Optimal Threshold ---\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Accuracy:{accuracy_score(all_labels, preds_optimal):.4f}\")\n",
    "    print(f\"Precision:{precision_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:{recall_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:{f1_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "\n",
    "    # Summary stats \n",
    "    print(f\"\\nAUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    #  Confusion Matrix (Optimal Threshold) \n",
    "    cm = confusion_matrix(all_labels, preds_optimal)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"spoof\", \"bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix @ Optimal Threshold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve with EER point marked \n",
    "    eer_x = eer\n",
    "    eer_y = 1 - eer\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label=\"Random Guess\")\n",
    "    plt.plot(eer_x, eer_y, 'ro', label=f\"EER = {eer:.4f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "# Run the evaluation on the dataset\n",
    "# ===============================================================\n",
    "\n",
    "# Ensure the model is defined and pre-trained weights are loaded\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(torch.load(\"model-weights/df_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "for_2sec_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/for-2sec/for-2seconds/preprocessed_testing\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "evaluate_on_test_set(model, for_2sec_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of ASVspoof2019 LA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Generates metadata CSV for the ASVspoof 2019 LA evaluation set.\n",
    "# - Parses a protocol file to extract speaker, file name, and label\n",
    "# - Checks for missing audio files in the provided directory\n",
    "# - Outputs a CSV with 'file', 'speaker', and binary 'label' (1 = bonafide, 0 = spoof)\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def make_la_eval_metadata(\n",
    "    protocol_path: str,\n",
    "    audio_dir: str,\n",
    "    output_csv: str\n",
    "):\n",
    "    # Convert to Path object for easier handling\n",
    "    protocol = Path(protocol_path)\n",
    "\n",
    "    # Ensure the protocol file exists\n",
    "    if not protocol.exists():\n",
    "        raise FileNotFoundError(f\"Cannot find protocol file: {protocol}\")\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    # Read and parse each line from the protocol file\n",
    "    with protocol.open('r') as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split()\n",
    "\n",
    "            # Skip malformed lines\n",
    "            if len(cols) != 5:\n",
    "                continue\n",
    "\n",
    "\n",
    "            speaker, audio_id, _, _, key = cols\n",
    "            filename = audio_id + \".flac\"\n",
    "            audio_path = Path(audio_dir) / filename\n",
    "\n",
    "            # Warn if the corresponding audio file is missing\n",
    "            if not audio_path.exists():\n",
    "                print(f\"[WARNING] Audio missing: {audio_path}\")\n",
    "                continue\n",
    "\n",
    "            # Assign label: 1 for bonafide, 0 for spoof\n",
    "            label = 1 if key.lower() == \"bonafide\" else 0\n",
    "\n",
    "            # Append entry for CSV\n",
    "            entries.append({\n",
    "                \"file\":    filename,\n",
    "                \"speaker\": speaker,\n",
    "                \"label\":   label\n",
    "            })\n",
    "\n",
    "    # Create DataFrame and save as CSV\n",
    "    df = pd.DataFrame(entries, columns=[\"file\", \"speaker\", \"label\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "# Run the function to generate metadata CSV\n",
    "make_la_eval_metadata(\n",
    "    protocol_path=\"datasets/evaluation/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "    audio_dir=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",          \n",
    "    output_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# This function loads all `.wav` audio files from the specified \n",
    "# input folder structure (train/val/test) and:\n",
    "# - Applies preprocessing (normalisation, padding, etc.)\n",
    "# - Extracts the 10 audio features using librosa\n",
    "# - Saves the features as `.npy` files under:\n",
    "#     output_root/preprocessed_<split>/<feature_name>/<filename>.npy\n",
    "#\n",
    "# This allows the model to train on consistent, aligned features\n",
    "# across all sets, and ready to be passed into the the audiofeatuedataset \n",
    "#  has a few changes to the version used in training with release-in-the-wild\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def extract_and_save_la_features_with_eta(\n",
    "    meta_csv: str,\n",
    "    audio_root: str,\n",
    "    output_root: str,\n",
    "    sr: int = 16000,\n",
    "    target_duration: float = 6.0,\n",
    "    apply_preemphasis: bool = False,\n",
    "    coef: float = 0.5,\n",
    "    normalise: str = \"rms\",\n",
    "):\n",
    "    df = pd.read_csv(meta_csv)     \n",
    "    audio_root  = Path(audio_root)\n",
    "    output_base = Path(output_root) / \"preprocessed_eval\"\n",
    "\n",
    "    total_files = len(df)\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm(df.iterrows(),\n",
    "                total=total_files,\n",
    "                desc=\"Extract LA features\",\n",
    "                unit=\"file\")\n",
    "\n",
    "    for idx, (_, row) in enumerate(pbar):\n",
    "        filename = row[\"file\"]\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "        in_path   = audio_root / filename\n",
    "\n",
    "        y, _ = librosa.load(str(in_path), sr=sr)\n",
    "        y = preprocess_audio(y, sr, target_duration,\n",
    "                             apply_preemphasis, coef, normalise)\n",
    "\n",
    "        feats = {\n",
    "            \"mel_spectrogram\":librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "            \"mfcc\":librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "            \"chroma\":librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "            \"tonnetz\":           librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "            \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "            \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "            \"pitch\":             librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "            \"energy\":            librosa.feature.rms(y=y),\n",
    "            \"zcr\":               librosa.feature.zero_crossing_rate(y=y),\n",
    "            \"onset_strength\":    librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        }\n",
    "\n",
    "        npy_name = Path(filename).stem + \".npy\"\n",
    "        for feat_name, arr in feats.items():\n",
    "            out_dir = output_base / label_dir / feat_name\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(out_dir / npy_name, arr.astype(np.float32))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_per_file = elapsed / (idx + 1)\n",
    "        remaining = total_files - (idx + 1)\n",
    "        eta = remaining * avg_per_file\n",
    "        pbar.set_postfix_str(f\"ETA {eta:.0f}s\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "\"\"\"\n",
    "extract_and_save_la_features_with_eta(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    audio_root=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",\n",
    "    output_root=\"datasets/evaluation/LA\",\n",
    "    sr=22050,\n",
    "    target_duration=6.0,\n",
    "    apply_preemphasis=False,\n",
    "    coef=0.5,\n",
    "    normalise=\"rms\",\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Dataset function for loading preprocessed audio features.\n",
    "# \n",
    "# - Loads feature data from .npy files (one per feature per file)\n",
    "# - Supports 10 audio features (e.g. MFCC, chroma, ZCR, etc.)\n",
    "# - Automatically pads or crops features to uniform shape\n",
    "# - Returns all features as tensors + binary label (real/fake)\n",
    "# ===============================================================\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=[\n",
    "                     'mel_spectrogram','mfcc','chroma','tonnetz',\n",
    "                     'spectral_contrast','spectral_centroid',\n",
    "                     'pitch','energy','zcr','onset_strength'\n",
    "                 ],\n",
    "                 target_shape=(128, 259)):\n",
    "        \n",
    "        # Load the metadata CSV into a dataframe\n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        \n",
    "        # Path where preprocessed features are stored\n",
    "        self.feature_root = Path(feature_root)\n",
    "        \n",
    "        # List of audio features to load for each sample\n",
    "        self.features = features\n",
    "        \n",
    "        # Desired shape for each feature tensor\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "        # Ensure label is int (1 = real, 0 = fake)\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "        #test\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of samples\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        # Pad or crop tensor to match target shape\n",
    "        h, w = tensor.shape\n",
    "        th, tw = target_shape\n",
    "        if th is None: th = h\n",
    "        if tw is None: tw = w\n",
    "        pad_h = th - h\n",
    "        pad_w = tw - w\n",
    "\n",
    "        # Crop if too big, pad if too small\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:th, :tw]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))  # Pad (left, right, top, bottom)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the row from metadata\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # File name without extension + .npy\n",
    "        file_id = Path(row[\"file\"]).stem + \".npy\"\n",
    "\n",
    "        # Determine folder based on label\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "\n",
    "        feature_tensors = []\n",
    "\n",
    "        # Loop through each feature and load corresponding .npy\n",
    "        for feat in self.features:\n",
    "            path = self.feature_root / \"preprocessed_eval\" / label_dir / feat / file_id\n",
    "            if not path.exists():\n",
    "                raise FileNotFoundError(f\"Missing {path}\")\n",
    "            \n",
    "            # Load .npy feature file as tensor\n",
    "            arr = np.load(path)\n",
    "            t = torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "            # If 1D, unsqueeze to make it 2D (e.g. (n,) -> (1, n))\n",
    "            if t.dim() == 1:\n",
    "                t = t.unsqueeze(0)\n",
    "\n",
    "            # Ensure shape consistency\n",
    "            t = self._pad_or_resize(t, self.target_shape)\n",
    "\n",
    "            # Collect feature tensor\n",
    "            feature_tensors.append(t)\n",
    "\n",
    "        # Return tuple of (features..., label)\n",
    "        return (*feature_tensors, torch.tensor(row[\"label\"], dtype=torch.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Evaluates trained binary deepfake detection model on a test dataset.\n",
    "# - Computes: Accuracy, Precision, Recall, F1 Score, AUC, and EER\n",
    "# - Displays a Confusion Matrix\n",
    "# - Takes: a model, test dataset, and runs inference using no gradients\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset,\n",
    "                         batch_size=32,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs, all_preds_05, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            *features, labels = batch\n",
    "            features = [f.to(device) for f in features]\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(*features)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "            preds_05 = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds_05.extend(preds_05.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs  = np.array(all_probs)\n",
    "    all_preds_05 = np.array(all_preds_05)\n",
    "\n",
    "    # ROC and EER\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Optimal threshold based on max(TPR - FPR)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_optimal = (all_probs > optimal_threshold).astype(int)\n",
    "\n",
    "    print(\"\\n--- Evaluation at Threshold = 0.5 ---\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, all_preds_05):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Evaluation at Optimal Threshold ---\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, preds_optimal):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "\n",
    "    print(f\"\\nAUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # Confusion matrix for optimal threshold\n",
    "    cm = confusion_matrix(all_labels, preds_optimal)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"spoof\", \"bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix @ Optimal Threshold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    eer_x = eer\n",
    "    eer_y = 1 - eer\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label=\"Random Guess\")\n",
    "    plt.plot(eer_x, eer_y, 'ro', label=f\"EER = {eer:.4f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running evalution loop on eval data fro LA dataset\n",
    "\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model-weights/df_model.pth\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/LA\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "debug_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "batch = next(iter(debug_loader))\n",
    "\n",
    "evaluate_on_test_set(model, dataset, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
