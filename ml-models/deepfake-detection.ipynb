{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         \n",
    "print(torch.cuda.get_device_name(0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Split and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Config\n",
    "RANDOM_SEED = 42\n",
    "BASE_DIR = \"datasets/release_in_the_wild\"\n",
    "META_CSV = os.path.join(BASE_DIR, \"meta.csv\")\n",
    "WAV_DIR = BASE_DIR\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"train\": os.path.join(BASE_DIR, \"train\"),\n",
    "    \"val\": os.path.join(BASE_DIR, \"val\"),\n",
    "    \"test\": os.path.join(BASE_DIR, \"test\")\n",
    "}\n",
    "\n",
    "for path in OUTPUT_DIRS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df[\"file\"] = df[\"file\"].str.strip()\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"speaker\"] = df[\"speaker\"].str.strip()\n",
    "\n",
    "df_40_percent, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=0.4,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_40_percent,\n",
    "    test_size=0.30,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_40_percent[\"label\"]\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "spoof_train = train_df[train_df[\"label\"] == \"spoof\"]\n",
    "bonafide_train = train_df[train_df[\"label\"] == \"bona-fide\"]\n",
    "\n",
    "spoof_upsampled = resample(\n",
    "    spoof_train,\n",
    "    replace=True,\n",
    "    n_samples=len(bonafide_train),\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df = pd.concat([bonafide_train, spoof_upsampled]).sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "#copy not move \n",
    "def move_files(subset_df, split_name):\n",
    "    for _, row in subset_df.iterrows():\n",
    "        src = os.path.join(WAV_DIR, row[\"file\"])\n",
    "        dst = os.path.join(OUTPUT_DIRS[split_name], row[\"file\"])\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# Move files\n",
    "#move_files(train_df, \"train\")\n",
    "#move_files(val_df, \"val\")\n",
    "#move_files(test_df, \"test\")\n",
    "\n",
    "# Save metadata\n",
    "#train_df.to_csv(os.path.join(BASE_DIR, \"train_meta.csv\"), index=False)\n",
    "#val_df.to_csv(os.path.join(BASE_DIR, \"val_meta.csv\"), index=False)\n",
    "#test_df.to_csv(os.path.join(BASE_DIR, \"test_meta.csv\"), index=False)\n",
    "\n",
    "# Summary\n",
    "def count_files(folder_path):\n",
    "    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "print(\"✔ 40% of dataset sampled and split into train/val/test (70/15/15).\")\n",
    "print(\"✔ Spoof samples oversampled in train set only.\")\n",
    "print(f\"Train files: {count_files(OUTPUT_DIRS['train'])}\")\n",
    "print(f\"Val files: {count_files(OUTPUT_DIRS['val'])}\")\n",
    "print(f\"Test files: {count_files(OUTPUT_DIRS['test'])}\")\n",
    "\n",
    "# Show label distribution in each set\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Val:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"Test:\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import librosa\n",
    "\n",
    "# Preprocess raw audio\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction (With saving feature to save time for future tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main extraction + saving\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0, apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"val\",\"test\"]:  \n",
    "        input_folder = input_root / split\n",
    "        output_base = output_root / f\"preprocessed_{split}\"\n",
    "\n",
    "        print(f\"Looking in: {input_folder}\")\n",
    "        wav_files = [f for f in input_folder.glob(\"*.wav\")]\n",
    "        print(f\"Found {len(wav_files)} files in '{split}'\")\n",
    "\n",
    "        for wav_file in tqdm(wav_files):\n",
    "            try:\n",
    "                y, _ = librosa.load(wav_file, sr=sr)\n",
    "                y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                base_name = wav_file.stem + \".npy\"\n",
    "\n",
    "                feature_dict = {\n",
    "                    \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                    \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                    \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                    \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                    \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                    \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                    \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                    \"energy\": librosa.feature.rms(y=y),\n",
    "                    \"zcr\": librosa.feature.zero_crossing_rate(y),\n",
    "                    \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                }\n",
    "\n",
    "                for feature_name, data in feature_dict.items():\n",
    "                    out_path = output_base / feature_name / base_name\n",
    "                    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "#Run the feature extraction\n",
    "#extract_and_save_all(\"datasets/release_in_the_wild\", \"datasets/release_in_the_wild\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_audio(path, sr=22050):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "def show_waveform(y, sr, title=\"Waveform\"):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_mel_spectrogram(y, sr, title=\"Mel Spectrogram\"):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_audio(original_path, fake_path, apply_preprocessing=False, sr=22050):\n",
    "    print(f\" Original: {original_path}\")\n",
    "    y_real = load_audio(original_path, sr=sr)\n",
    "    y_fake = load_audio(fake_path, sr=sr)\n",
    "\n",
    "    if apply_preprocessing:\n",
    "        y_real = preprocess_audio(y_real, sr, apply_preemphasis=True, normalise='rms')\n",
    "        y_fake = preprocess_audio(y_fake, sr, apply_preemphasis=True, normalise='rms')\n",
    "\n",
    "    # Waveform comparison\n",
    "    show_waveform(y_real, sr, title=\"Real - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "\n",
    "    show_waveform(y_fake, sr, title=\"Fake - Waveform\")\n",
    "    ipd.display(ipd.Audio(y_fake, rate=sr))\n",
    "\n",
    "    # Mel spectrogram comparison\n",
    "    show_mel_spectrogram(y_real, sr, title=\"Real - Mel Spectrogram\")\n",
    "    show_mel_spectrogram(y_fake, sr, title=\"Fake - Mel Spectrogram\")\n",
    "\n",
    "real_audio_path = \"datasets/release_in_the_wild/remaining_files/7.wav\"\n",
    "\n",
    "fake_audio_path = \"datasets/release_in_the_wild/remaining_files/5.wav\"\n",
    "\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=False)\n",
    "#compare_audio(real_audio_path, fake_audio_path, apply_preprocessing=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE THIS VERSION TO TRAIN,TEST,VAL FOR RELEASE_IN_THE_WILD DATASET\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):  \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].str.strip().str.lower()\n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {'bona-fide': 1, 'spoof': 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            tensor = tensor[:target_shape[0], :target_shape[1]] \n",
    "        else:\n",
    "            tensor = F.pad(tensor, (0, pad_w, 0, pad_h))  \n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, feat, file_id) \n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "            feature = np.load(path)\n",
    "            tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self.pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture (DenseNN + Siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=128):\n",
    "        super(DenseNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SiameseMFCCBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMFCCBranch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.flattened_size = 128 * 32 * 64  \n",
    "        self.fc = nn.Linear(self.flattened_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))       \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.fc(x)                   \n",
    "        return x\n",
    "\n",
    "\n",
    "# Final Fusion Model\n",
    "class AudioDeepfakeFusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioDeepfakeFusionModel, self).__init__()\n",
    "\n",
    "        self.mfcc_branch = SiameseMFCCBranch()\n",
    "\n",
    "        self.chroma_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.tonnetz_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.contrast_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.pitch_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.energy_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.zcr_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.onset_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.centroid_branch = DenseNeuralNetwork(input_dim=128)\n",
    "        self.mel_spec_branch = DenseNeuralNetwork(input_dim=128)\n",
    "\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(10 * 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, mfcc, chroma, tonnetz, contrast, pitch, energy, zcr, onset, centroid, mel_spec):\n",
    "        mfcc = mfcc.unsqueeze(1)  \n",
    "\n",
    "        def pool(x): return x.mean(dim=-1)  \n",
    "\n",
    "        mfcc_out = self.mfcc_branch(mfcc)\n",
    "        chroma_out = self.chroma_branch(pool(chroma))\n",
    "        tonnetz_out = self.tonnetz_branch(pool(tonnetz))\n",
    "        contrast_out = self.contrast_branch(pool(contrast))\n",
    "        pitch_out = self.pitch_branch(pool(pitch))\n",
    "        energy_out = self.energy_branch(pool(energy))\n",
    "        zcr_out = self.zcr_branch(pool(zcr))\n",
    "        onset_out = self.onset_branch(pool(onset))\n",
    "        centroid_out = self.centroid_branch(pool(centroid))\n",
    "        mel_spec_out = self.mel_spec_branch(pool(mel_spec))\n",
    "\n",
    "        fusion = torch.cat([\n",
    "            mfcc_out, chroma_out, tonnetz_out, contrast_out,\n",
    "            pitch_out, energy_out, zcr_out, onset_out, centroid_out, mel_spec_out\n",
    "        ], dim=1)\n",
    "\n",
    "        x = self.fusion_layer(fusion)\n",
    "        return torch.sigmoid(self.output_layer(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, device):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device).unsqueeze(1)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                all_preds += outputs.cpu().numpy().flatten().tolist()\n",
    "                all_labels += labels.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        all_preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        acc = np.mean(np.array(all_preds_bin) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds_bin)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "       # history['val_loss'].append(val_loss / len(val_loader))\n",
    "        #history['val_acc'].append(acc)\n",
    "        #history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"[{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, #Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return model, history, all_labels, all_preds_bin\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    #plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Accuracy')\n",
    "    #plt.plot(history['val_f1'], label='F1 Score')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1 Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conf_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Spoof', 'Bona-fide'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_binary_classification_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=20, batch_size=32, learning_rate=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_prec': [],\n",
    "        'val_rec': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        train_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            train_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            train_trues.extend(labels.cpu().numpy())\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = [b.to(device) for b in batch[:-1]]\n",
    "                labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                val_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "                val_trues.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        acc = accuracy_score(val_trues, val_preds)\n",
    "        prec = precision_score(val_trues, val_preds)\n",
    "        rec = recall_score(val_trues, val_preds)\n",
    "        f1 = f1_score(val_trues, val_preds)\n",
    "\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(acc)\n",
    "        history['val_prec'].append(prec)\n",
    "        history['val_rec'].append(rec)\n",
    "        history['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"\\n[Validation] Loss: {avg_val_loss:.4f} | Acc: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"df_model.pth\")\n",
    "    print(\"\\n Model saved to 'df_model.pth'\")\n",
    "\n",
    "    # Plot metric graphs\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Show confusion matrix for both splits\n",
    "    show_confusion_matrix(train_trues, train_preds, title=\"Train Set\")\n",
    "    show_confusion_matrix(val_trues, val_preds, title=\"Validation Set\")\n",
    "\n",
    "\n",
    "def show_confusion_matrix(true_labels, pred_labels, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(history['val_loss'], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "\n",
    "    # Accuracy & F1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_acc'], label=\"Accuracy\")\n",
    "    plt.plot(history['val_f1'], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy & F1\")\n",
    "\n",
    "    # Precision & Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['val_prec'], label=\"Precision\")\n",
    "    plt.plot(history['val_rec'], label=\"Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Precision & Recall\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaner Training and Classification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BinaryClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1e-4,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "        \n",
    "        # History including AUC & EER for both train and validation\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_prec': [], 'train_rec': [], 'train_f1': [], 'train_auc': [], 'train_eer': [],\n",
    "            'val_loss':   [], 'val_acc':   [], 'val_prec':   [], 'val_rec':   [], 'val_f1':   [], 'val_auc':   [], 'val_eer':   []\n",
    "        }\n",
    "\n",
    "    def _run_epoch(self, loader, train: bool):\n",
    "        epoch_loss = 0.0\n",
    "        all_probs, all_preds, all_labels = [], [], []\n",
    "\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        iterable = tqdm(loader, desc='Training') if train else loader\n",
    "        for batch in iterable:\n",
    "            *inputs, labels = batch\n",
    "            labels = labels.float().to(self.device).unsqueeze(1)\n",
    "            inputs = [x.to(self.device) for x in inputs]\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                outputs = self.model(*inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                if train:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * labels.size(0)\n",
    "            probs = outputs.detach().cpu().numpy().flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            truths = labels.detach().cpu().numpy().flatten()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(truths.tolist())\n",
    "\n",
    "        # Compute metrics\n",
    "        avg_loss = epoch_loss / len(loader.dataset)\n",
    "        acc  = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        auc  = roc_auc_score(all_labels, all_probs)\n",
    "        \n",
    "        # Equal Error Rate (EER)\n",
    "        fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "        fnr = 1 - tpr\n",
    "        idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "        eer = fpr[idx]\n",
    "\n",
    "        return avg_loss, acc, prec, rec, f1, auc, eer, all_labels, all_preds\n",
    "\n",
    "    def fit(self, epochs: int = 20):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Training epoch\n",
    "            train_loss, train_acc, train_prec, train_rec, train_f1, train_auc, train_eer, _, _ = \\\n",
    "                self._run_epoch(self.train_loader, train=True)\n",
    "            # Validation epoch\n",
    "            val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, val_eer, _, _ = \\\n",
    "                self._run_epoch(self.val_loader,   train=False)\n",
    "\n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['train_prec'].append(train_prec)\n",
    "            self.history['train_rec'].append(train_rec)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['train_auc'].append(train_auc)\n",
    "            self.history['train_eer'].append(train_eer)\n",
    "\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_prec'].append(val_prec)\n",
    "            self.history['val_rec'].append(val_rec)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            self.history['val_auc'].append(val_auc)\n",
    "            self.history['val_eer'].append(val_eer)\n",
    "\n",
    "            # Print summary\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | \"\n",
    "                f\"F1: {train_f1:.4f} | AUC: {train_auc:.4f} | EER: {train_eer:.4f} || \"\n",
    "                f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | \"\n",
    "                f\"F1: {val_f1:.4f} | AUC: {val_auc:.4f} | EER: {val_eer:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(self.model.state_dict(), \"df_model.pth\")\n",
    "        print(\"Model saved to 'df_model.pth'\")\n",
    "\n",
    "    def plot_history(self):\n",
    "        # You can extend this method to include plots for AUC and EER if desired\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'],   label='Val Loss')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')\n",
    "\n",
    "        # Accuracy & F1\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.history['train_acc'], label='Train Acc')\n",
    "        plt.plot(self.history['val_acc'],   label='Val Acc')\n",
    "        plt.plot(self.history['train_f1'],  label='Train F1')\n",
    "        plt.plot(self.history['val_f1'],    label='Val F1')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Score'); plt.legend(); plt.title('Acc & F1')\n",
    "\n",
    "        # Precision, Recall, AUC\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.history['train_prec'], label='Train Prec')\n",
    "        plt.plot(self.history['val_prec'],   label='Val Prec')\n",
    "        plt.plot(self.history['train_rec'],  label='Train Rec')\n",
    "        plt.plot(self.history['val_rec'],    label='Val Rec')\n",
    "        plt.plot(self.history['train_auc'],  label='Train AUC')\n",
    "        plt.plot(self.history['val_auc'],    label='Val AUC')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Score'); plt.legend(); plt.title('Prec, Rec & AUC')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion(self, true_labels, pred_labels, title='Confusion Matrix'):\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=['Spoof', 'Bona‑fide'])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength',\n",
    "        'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength',\n",
    "        'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "trainer = BinaryClassifierTrainer(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    device='cuda'  \n",
    ")\n",
    "\n",
    "#comment out if you dont want to run \n",
    "\"\"\"\n",
    "trainer.fit(epochs=50)\n",
    "trainer.plot_history()\n",
    "\n",
    "_, _, _, _, _, _, _, val_labels, val_preds = \\\n",
    "    trainer._run_epoch(trainer.val_loader, train=False)\n",
    "\n",
    "trainer.plot_confusion(val_labels, val_preds, title='Validation set Confusion')\n",
    "\n",
    "_, _, _, _, _, _, _, train_labels, train_preds = trainer._run_epoch(trainer.train_loader, train=False)\n",
    "\n",
    "trainer.plot_confusion(train_labels, train_preds, title='train Set Confusion Matrix')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Train + Val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/train_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_train\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "val_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/val_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_val\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid','mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "model = AudioDeepfakeFusionModel()\n",
    "\n",
    "\"\"\"\n",
    "train_binary_classification_model(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001\n",
    " \n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = [b.to(device) for b in batch[:-1]]\n",
    "            labels = batch[-1].float().to(device).unsqueeze(1)\n",
    "\n",
    "            outputs = model(*inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_labels = np.array(all_labels).astype(int)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Metrics\n",
    "\n",
    "    acc  = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec  = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1   = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc  = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "\n",
    "    # Compute EER\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Spoof\", \"Bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Load saved weights\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(torch.load(\"df_model.pth\", map_location=torch.device('cpu'), weights_only=True))\n",
    "\n",
    "#RELEASE IN THE WILD TEST\n",
    "release_in_wild_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/release_in_the_wild/test_meta.csv\",\n",
    "    feature_root=\"datasets/release_in_the_wild/preprocessed_test\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "evaluate_on_test_set(model, release_in_wild_test_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model\n",
    "For 2-sec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#USE THIS TO PREPROCESS,EXTRACT AND GENERATE CSV \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_all(input_root, output_root, sr=22050, target_duration=6.0,\n",
    "                         apply_preemphasis=False, coef=0.5, normalise='rms'):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"testing\"]:\n",
    "        for label_dir in [\"real\", \"fake\"]:\n",
    "            input_folder = input_root / split / label_dir\n",
    "            output_base = output_root / f\"preprocessed_{split}\" / label_dir\n",
    "\n",
    "            print(f\"Looking in: {input_folder}\")\n",
    "            wav_files = sorted(list(input_folder.glob(\"*.wav\")))\n",
    "            print(f\"Found {len(wav_files)} files in '{split}/{label_dir}'\")\n",
    "\n",
    "            for wav_file in tqdm(wav_files, desc=f\"Processing {label_dir}\"):\n",
    "                try:\n",
    "                    y, _ = librosa.load(wav_file, sr=sr)\n",
    "                    y = preprocess_audio(y, sr, target_duration, apply_preemphasis, coef, normalise)\n",
    "\n",
    "                    clean_name = wav_file.stem.split(\".\")[0] + \".npy\"\n",
    "\n",
    "                    feature_dict = {\n",
    "                        \"mel_spectrogram\": librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "                        \"mfcc\": librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "                        \"chroma\": librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "                        \"tonnetz\": librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "                        \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "                        \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "                        \"pitch\": librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "                        \"energy\": librosa.feature.rms(y=y),\n",
    "                        \"zcr\": librosa.feature.zero_crossing_rate(y=y),\n",
    "                        \"onset_strength\": librosa.onset.onset_strength(y=y, sr=sr)\n",
    "                    }\n",
    "\n",
    "                    for feature_name, data in feature_dict.items():\n",
    "                        out_path = output_base / feature_name / clean_name\n",
    "                        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        np.save(out_path, data.astype(np.float32))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {wav_file.name}: {e}\")\n",
    "\n",
    "\"\"\"\n",
    "extract_and_save_all(\n",
    "    input_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    output_root=\"datasets/evaluation/for-2sec/for-2seconds\",\n",
    "    sr=16000, target_duration=6.0\n",
    ")\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def generate_test_meta_csv(real_dir, fake_dir, save_path):\n",
    "    entries = []\n",
    "    for path in sorted(Path(real_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]  \n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 1})\n",
    "    for path in sorted(Path(fake_dir).glob(\"*.wav\")):\n",
    "        base = path.name.split(\".\")[0]\n",
    "        entries.append({\"file\": base + \".npy\", \"label\": 0})\n",
    "    pd.DataFrame(entries).to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "generate_test_meta_csv(\n",
    "    real_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/real\",\n",
    "    fake_dir=\"datasets/evaluation/for-2sec/for-2seconds/testing/fake\",\n",
    "    save_path=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\"\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=['chroma', 'energy', 'mel_spectrogram', 'mfcc',\n",
    "                           'onset_strength', 'pitch', 'spectral_centroid',\n",
    "                           'spectral_contrast', 'tonnetz', 'zcr'],\n",
    "                 target_shape=(128, 259)):\n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int) \n",
    "        self.feature_root = feature_root\n",
    "        self.features = features\n",
    "        self.label_map = {1: 1, 0: 0}\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        pad_h = target_shape[0] - h\n",
    "        pad_w = target_shape[1] - w\n",
    "\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:target_shape[0], :target_shape[1]]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label_raw = row[\"label\"]\n",
    "        if label_raw not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label: '{label_raw}' at idx {idx}\")\n",
    "        label = self.label_map[label_raw]\n",
    "\n",
    "        label_dir = \"real\" if label == 1 else \"fake\"\n",
    "\n",
    "        # Construct the file path\n",
    "        file_id = row[\"file\"]\n",
    "        feature_arrays = []\n",
    "        for feat in self.features:\n",
    "            path = os.path.join(self.feature_root, label_dir, feat, file_id)\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "            arr = np.load(path)\n",
    "            tensor = torch.tensor(arr, dtype=torch.float32)\n",
    "            if tensor.dim() == 1:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = self._pad_or_resize(tensor, self.target_shape)\n",
    "            feature_arrays.append(tensor)\n",
    "\n",
    "        return (*feature_arrays, torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset,\n",
    "                         batch_size=32,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs, all_preds_05, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            *features, labels = batch\n",
    "            features = [f.to(device) for f in features]\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(*features)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "            preds_05 = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds_05.extend(preds_05.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs  = np.array(all_probs)\n",
    "    all_preds_05 = np.array(all_preds_05)\n",
    "\n",
    "    # ROC and EER\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Optimal threshold based on max(TPR - FPR)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_optimal = (all_probs > optimal_threshold).astype(int)\n",
    "\n",
    "    print(\"\\n--- Evaluation at Threshold = 0.5 ---\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, all_preds_05):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Evaluation at Optimal Threshold ---\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, preds_optimal):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "\n",
    "    print(f\"\\nAUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # Confusion matrix for optimal threshold\n",
    "    cm = confusion_matrix(all_labels, preds_optimal)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"spoof\", \"bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix @ Optimal Threshold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    eer_x = eer\n",
    "    eer_y = 1 - eer\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label=\"Random Guess\")\n",
    "    plt.plot(eer_x, eer_y, 'ro', label=f\"EER = {eer:.4f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for_2sec_test_dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/for-2sec/for-2seconds/test_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/for-2sec/for-2seconds/preprocessed_testing\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluate_on_test_set(model, for_2sec_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of ASVspoof2019 LA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_la_eval_metadata(\n",
    "    protocol_path: str,\n",
    "    audio_dir: str,\n",
    "    output_csv: str\n",
    "):\n",
    "    protocol = Path(protocol_path)\n",
    "    if not protocol.exists():\n",
    "        raise FileNotFoundError(f\"Cannot find protocol file: {protocol}\")\n",
    "\n",
    "    entries = []\n",
    "    with protocol.open('r') as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split()\n",
    "            if len(cols) != 5:\n",
    "                continue\n",
    "            speaker, audio_id, key = cols\n",
    "            filename = audio_id + \".flac\"\n",
    "            audio_path = Path(audio_dir) / filename\n",
    "            if not audio_path.exists():\n",
    "                print(f\"[WARNING] Audio missing: {audio_path}\")\n",
    "                continue\n",
    "            label = 1 if key.lower() == \"bonafide\" else 0\n",
    "            entries.append({\n",
    "                \"file\":    filename,\n",
    "                \"speaker\": speaker,\n",
    "                \"label\":   label\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(entries, columns=[\"file\",\"speaker\",\"label\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_csv}\")\n",
    "\n",
    "\n",
    "make_la_eval_metadata(\n",
    "    protocol_path=\"datasets/evaluation/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "    audio_dir=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",          \n",
    "    output_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_la_features_with_eta(\n",
    "    meta_csv: str,\n",
    "    audio_root: str,\n",
    "    output_root: str,\n",
    "    sr: int = 16000,\n",
    "    target_duration: float = 6.0,\n",
    "    apply_preemphasis: bool = False,\n",
    "    coef: float = 0.5,\n",
    "    normalise: str = \"rms\",\n",
    "):\n",
    "    df = pd.read_csv(meta_csv)     \n",
    "    audio_root  = Path(audio_root)\n",
    "    output_base = Path(output_root) / \"preprocessed_eval\"\n",
    "\n",
    "    total_files = len(df)\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm(df.iterrows(),\n",
    "                total=total_files,\n",
    "                desc=\"Extract LA features\",\n",
    "                unit=\"file\")\n",
    "\n",
    "    for idx, (_, row) in enumerate(pbar):\n",
    "        filename = row[\"file\"]\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "        in_path   = audio_root / filename\n",
    "\n",
    "        y, _ = librosa.load(str(in_path), sr=sr)\n",
    "        y = preprocess_audio(y, sr, target_duration,\n",
    "                             apply_preemphasis, coef, normalise)\n",
    "\n",
    "        feats = {\n",
    "            \"mel_spectrogram\":   librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128),\n",
    "            \"mfcc\":              librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "            \"chroma\":            librosa.feature.chroma_stft(y=y, sr=sr),\n",
    "            \"tonnetz\":           librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr),\n",
    "            \"spectral_contrast\": librosa.feature.spectral_contrast(y=y, sr=sr),\n",
    "            \"spectral_centroid\": librosa.feature.spectral_centroid(y=y, sr=sr),\n",
    "            \"pitch\":             librosa.yin(y, fmin=50, fmax=300, sr=sr),\n",
    "            \"energy\":            librosa.feature.rms(y=y),\n",
    "            \"zcr\":               librosa.feature.zero_crossing_rate(y=y),\n",
    "            \"onset_strength\":    librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        }\n",
    "\n",
    "        npy_name = Path(filename).stem + \".npy\"\n",
    "        for feat_name, arr in feats.items():\n",
    "            out_dir = output_base / label_dir / feat_name\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(out_dir / npy_name, arr.astype(np.float32))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_per_file = elapsed / (idx + 1)\n",
    "        remaining = total_files - (idx + 1)\n",
    "        eta = remaining * avg_per_file\n",
    "        pbar.set_postfix_str(f\"ETA {eta:.0f}s\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "\"\"\"\n",
    "extract_and_save_la_features_with_eta(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    audio_root=\"datasets/evaluation/LA/ASVspoof2019_LA_eval/flac\",\n",
    "    output_root=\"datasets/evaluation/LA\",\n",
    "    sr=22050,\n",
    "    target_duration=6.0,\n",
    "    apply_preemphasis=False,\n",
    "    coef=0.5,\n",
    "    normalise=\"rms\",\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, meta_csv, feature_root,\n",
    "                 features=[\n",
    "                     'mel_spectrogram','mfcc','chroma','tonnetz',\n",
    "                     'spectral_contrast','spectral_centroid',\n",
    "                     'pitch','energy','zcr','onset_strength'\n",
    "                 ],\n",
    "                 target_shape=(128, 259)):\n",
    "                \n",
    "        self.df = pd.read_csv(meta_csv)\n",
    "        self.feature_root = Path(feature_root)\n",
    "        self.features = features\n",
    "        self.target_shape = target_shape\n",
    "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_or_resize(self, tensor, target_shape):\n",
    "        h, w = tensor.shape\n",
    "        th, tw = target_shape\n",
    "        if th is None: th = h\n",
    "        if tw is None: tw = w\n",
    "        pad_h = th - h\n",
    "        pad_w = tw - w\n",
    "        if pad_h < 0 or pad_w < 0:\n",
    "            return tensor[:th, :tw]\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = Path(row[\"file\"]).stem + \".npy\"\n",
    "        label_dir = \"real\" if row[\"label\"] == 1 else \"fake\"\n",
    "\n",
    "        feature_tensors = []\n",
    "        for feat in self.features:\n",
    "            path = self.feature_root / \"preprocessed_eval\" / label_dir / feat / file_id\n",
    "            if not path.exists():\n",
    "                raise FileNotFoundError(f\"Missing {path}\")\n",
    "            arr = np.load(path)\n",
    "            t = torch.tensor(arr, dtype=torch.float32)\n",
    "            if t.dim() == 1:\n",
    "                t = t.unsqueeze(0)\n",
    "            t = self._pad_or_resize(t, self.target_shape)\n",
    "            feature_tensors.append(t)\n",
    "\n",
    "        return (*feature_tensors, torch.tensor(row[\"label\"], dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, ConfusionMatrixDisplay\n",
    ")\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_on_test_set(model, test_dataset,\n",
    "                         batch_size=32,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_probs, all_preds_05, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Testing\"):\n",
    "            *features, labels = batch\n",
    "            features = [f.to(device) for f in features]\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(*features)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "            preds_05 = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_preds_05.extend(preds_05.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs  = np.array(all_probs)\n",
    "    all_preds_05 = np.array(all_preds_05)\n",
    "\n",
    "    # ROC and EER\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    # Optimal threshold based on max(TPR - FPR)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_optimal = (all_probs > optimal_threshold).astype(int)\n",
    "\n",
    "    print(\"\\n--- Evaluation at Threshold = 0.5 ---\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, all_preds_05):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, all_preds_05, zero_division=0):.4f}\")\n",
    "\n",
    "    print(\"\\n--- Evaluation at Optimal Threshold ---\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    print(f\"Accuracy:  {accuracy_score(all_labels, preds_optimal):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(all_labels, preds_optimal, zero_division=0):.4f}\")\n",
    "\n",
    "    print(f\"\\nAUC:       {auc:.4f}\")\n",
    "    print(f\"EER:       {eer:.4f}\")\n",
    "\n",
    "    # Confusion matrix for optimal threshold\n",
    "    cm = confusion_matrix(all_labels, preds_optimal)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"spoof\", \"bona-fide\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix @ Optimal Threshold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    eer_x = eer\n",
    "    eer_y = 1 - eer\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label=\"Random Guess\")\n",
    "    plt.plot(eer_x, eer_y, 'ro', label=f\"EER = {eer:.4f}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AudioDeepfakeFusionModel()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"df_model.pth\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "dataset = AudioFeatureDataset(\n",
    "    meta_csv=\"datasets/evaluation/LA/LA_eval_meta.csv\",\n",
    "    feature_root=\"datasets/evaluation/LA\",\n",
    "    features=[\n",
    "        'mfcc', 'chroma', 'tonnetz', 'spectral_contrast',\n",
    "        'pitch', 'energy', 'zcr', 'onset_strength', 'spectral_centroid', 'mel_spectrogram'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "debug_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "batch = next(iter(debug_loader))\n",
    "\n",
    "evaluate_on_test_set(model, dataset, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
