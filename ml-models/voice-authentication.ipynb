{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())         # should return True\n",
    "print(torch.cuda.get_device_name(0))     # should say \"NVIDIA GeForce RTX 3060\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For data manipulation and analysis \n",
    "import pandas as pd\n",
    "\n",
    "# For numerical operations and array handling \n",
    "import numpy as np\n",
    "\n",
    "# For creating plots and visualisations\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# For advanced data visualisations \n",
    "#import seaborn as sns\n",
    "# For file pattern matching \n",
    "import glob as glob\n",
    "\n",
    "# For audio processing \n",
    "import librosa\n",
    "\n",
    "# For displaying audio data visually \n",
    "import librosa.display\n",
    "\n",
    "# For playing audio directly in notebooks \n",
    "import IPython.display as ipd\n",
    "\n",
    "import wave\n",
    "\n",
    "#for reproduability \n",
    "import random\n",
    "\n",
    "#for the fodler re-structure 150 sample \n",
    "import shutil\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.getcwd(), 'datasets', 'voice-based-id-recognition')\n",
    "diffPhrase = os.path.join(base_dir,'differentPhrase')\n",
    "samePhrase = os.path.join(base_dir,'samePhrase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Dataset Location existance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differentPhrase Directory: \n",
      "['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n",
      "samePhrase Directory: \n",
      "['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "# Check if the training directory exists\n",
    "if os.path.exists(diffPhrase):\n",
    "    print(\"differentPhrase Directory: \")\n",
    "    print(os.listdir(diffPhrase))\n",
    "else:\n",
    "    print(\"Directory does not exist:\", diffPhrase) \n",
    "   \n",
    "\n",
    "# Check if the testing directory exists\n",
    "if os.path.exists(samePhrase):\n",
    "    \n",
    "    print(\"samePhrase Directory: \")\n",
    "    print(os.listdir(samePhrase))\n",
    "else:\n",
    "    print(\"Directory does not exist:\", samePhrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 18-19: truncated \\UXXXXXXXX escape (3963867248.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 18-19: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "file_path = r\"C:\\Users\\Aadil\\Desktop\\Voice-Auth-DF-System\\ml-models\\datasets\\voice-based-id-recognition\\differentPhrase\\1\\1-11.wav\"\n",
    "\n",
    "with wave.open(file_path, 'rb') as wav_file:\n",
    "    num_frames = wav_file.getnframes()\n",
    "    sample_rate = wav_file.getframerate()\n",
    "    sample_width = wav_file.getsampwidth()\n",
    "    channels = wav_file.getnchannels()\n",
    "    audio_data = wav_file.readframes(num_frames)\n",
    "\n",
    "# Convert raw bytes to numpy array\n",
    "audio_np = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "# If stereo, reshape\n",
    "if channels == 2:\n",
    "    audio_np = audio_np.reshape(-1, 2)\n",
    "\n",
    "ipd.Audio(audio_np, rate=sample_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset split\n",
    "base_path = os.path.join(os.getcwd(), 'datasets', 'vox1_dev')\n",
    "print(base_path)\n",
    "\n",
    "#Each folder contains a different speaker\n",
    "\n",
    "speaker_count = sum(os.path.isdir(os.path.join(base_path, entry)) for entry in os.listdir(base_path))\n",
    "print(\"Number of original Speakers : \",speaker_count)\n",
    "#Select a random selection of speakers, as inital dataset of 1211 speakers is roughly 30GB. will take a sample of 150 speakers(12.3%), aroudn 3.7GB\n",
    "\n",
    "\"\"\"\"\n",
    "# Destination path for subset\n",
    "train_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Set seed and select random 150 speakers\n",
    "random.seed(42)\n",
    "all_speakers = [entry for entry in os.listdir(base_path)\n",
    "                if os.path.isdir(os.path.join(base_path, entry))]\n",
    "selected_speakers = random.sample(all_speakers, 150)\n",
    "\n",
    "# Copy selected speakers and their full contents to the subset folder\n",
    "for speaker in selected_speakers:\n",
    "    src_speaker_path = os.path.join(base_path, speaker)\n",
    "    dest_speaker_path = os.path.join(train_path, speaker)\n",
    "    shutil.copytree(src_speaker_path, dest_speaker_path)\n",
    "\n",
    "#print(f\"Subset dataset created at: {train_path}\")\n",
    "\"\"\"\n",
    "\n",
    "# Print the selected speaker folders\n",
    "count = 0\n",
    "#print(\"Selected speakers:\")\n",
    "for speaker in selected_speakers:\n",
    "    count +=1\n",
    "    #print(speaker)\n",
    "\n",
    "print(\"Number of train Speakers : \",count)\n",
    "\n",
    "\n",
    "\n",
    "test_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_test')\n",
    "\n",
    "test_speaker_count = sum(os.path.isdir(os.path.join(test_path, entry)) for entry in os.listdir(test_path))\n",
    "print(\"Number of test Speakers : \",test_speaker_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing to see if files are copied in both train and test, tested using hash of the files to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "def file_hash(filepath):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# paths\n",
    "test_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_test')\n",
    "train_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "\n",
    "# Map filenames to full paths\n",
    "def get_file_map(base_path):\n",
    "    file_map = {}\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_map[file] = os.path.join(root, file)\n",
    "    return file_map\n",
    "\n",
    "test_files = get_file_map(test_path)\n",
    "train_files = get_file_map(train_path)\n",
    "\n",
    "# Compare common files\n",
    "common = set(test_files).intersection(train_files)\n",
    "print(f\"Found {len(common)} common files.\\n\")\n",
    "\n",
    "for filename in sorted(common):\n",
    "    test_file = test_files[filename]\n",
    "    train_file = train_files[filename]\n",
    "\n",
    "    same = file_hash(test_file) == file_hash(train_file)\n",
    "    print(f\"{filename} - {'Same' if same else 'Different'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting train and validaion(80/20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset with 150 speaker folders\n",
    "source_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "\n",
    "# Output subfolders for train and val\n",
    "train_folder = os.path.join(source_path, 'train_data')\n",
    "val_folder = os.path.join(source_path, 'val_data')\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "# Get all speaker folders \n",
    "speaker_folders = [folder for folder in os.listdir(source_path)\n",
    "                   if os.path.isdir(os.path.join(source_path, folder)) and folder not in ['train_data', 'val_data']]\n",
    "\n",
    "# Shuffle and split 80/20\n",
    "random.seed(42)\n",
    "random.shuffle(speaker_folders)\n",
    "split_index = int(0.8 * len(speaker_folders))\n",
    "train_speakers = speaker_folders[:split_index]\n",
    "val_speakers = speaker_folders[split_index:]\n",
    "\n",
    "# Copy folders to train_data\n",
    "for speaker in train_speakers:\n",
    "    src = os.path.join(source_path, speaker)\n",
    "    dst = os.path.join(train_folder, speaker)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "# Copy folders to val_data\n",
    "for speaker in val_speakers:\n",
    "    src = os.path.join(source_path, speaker)\n",
    "    dst = os.path.join(val_folder, speaker)\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "print(f\"{len(train_speakers)} speakers moved to train_data, {len(val_speakers)} to val_data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking number of files, i.e the size to see how long it may take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train .wav files: 14308\n",
      "Val .wav files: 3353\n",
      "Total .wav files: 17661\n",
      "Scanning audio files in datasets\\vox1_subset_150\\train_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function ExecutionEngine._raw_object_cache_notify at 0x000001FB4716A680>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aadil\\anaconda3\\envs\\voice-auth-env\\lib\\site-packages\\llvmlite\\binding\\executionengine.py\", line 178, in _raw_object_cache_notify\n",
      "    def _raw_object_cache_notify(self, data):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load datasets\\vox1_subset_150\\train_data\\id10002\\0_laIeN-Q44\\00001.wav: no compiled object yet for <Library '<numba.np.ufunc.wrappers._GufuncWrapper object at 0x000001FB4CA5F1F0>' at 0x1fb4ca39e10>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Root dataset path\n",
    "source_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150')\n",
    "\n",
    "# Train and validation folders\n",
    "train_folder = os.path.join(source_path, 'train_data')\n",
    "val_folder = os.path.join(source_path, 'val_data')\n",
    "\n",
    "def count_wav_files(folder_path):\n",
    "    wav_count = 0\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        wav_files = [f for f in files if f.lower().endswith('.wav')]\n",
    "        wav_count += len(wav_files)\n",
    "    return wav_count\n",
    "\n",
    "train_count = count_wav_files(train_folder)\n",
    "val_count = count_wav_files(val_folder)\n",
    "total_count = train_count + val_count\n",
    "\n",
    "print(f\"Train .wav files: {train_count}\")\n",
    "print(f\"Val .wav files: {val_count}\")\n",
    "print(f\"Total .wav files: {total_count}\")\n",
    "\n",
    "\n",
    "#find avreage audio file length to see what the set time shodul be for the pre proicessing\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def check_audio_durations(folder_path, sr=22050, plot_hist=True):\n",
    "    folder = Path(folder_path)\n",
    "    durations = []\n",
    "\n",
    "    print(f\"Scanning audio files in {folder}...\")\n",
    "\n",
    "    for wav_file in folder.rglob(\"*.wav\"):\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_file, sr=sr)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            durations.append(duration)\n",
    "        except Exception as e:\n",
    "             print(f\"Failed to load {wav_file}: {e}\")\n",
    "\n",
    "    durations = np.array(durations)\n",
    "\n",
    "    print(f\"\\n Duration Stats (in seconds):\")\n",
    "    print(f\"Average: {np.mean(durations):.2f}s\")\n",
    "    print(f\"Min:     {np.min(durations):.2f}s\")\n",
    "    print(f\"Max:     {np.max(durations):.2f}s\")\n",
    "    print(f\"Std dev: {np.std(durations):.2f}s\")\n",
    "    print(f\"Total files: {len(durations)}\")\n",
    "\n",
    "    if plot_hist:\n",
    "        plt.hist(durations, bins=50, edgecolor='black')\n",
    "        plt.title(\"Audio Duration Distribution\")\n",
    "        plt.xlabel(\"Duration (seconds)\")\n",
    "        plt.ylabel(\"Number of files\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return durations\n",
    "\n",
    "check_audio_durations('datasets/vox1_subset_150/train_data')\n",
    "check_audio_durations('datasets/vox1_subset_150/val_data')\n",
    "check_audio_durations('datasets/vox1_subset_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "def preprocess_audio(y, sr, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.6, normalise='rms'):\n",
    "    \n",
    "    # Trim leading/trailing silence\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Apply noise reduction / dereverberation\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    # Apply pre-emphasis filter\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Normalisation method\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Duration control: pad or truncate\n",
    "    target_length = int(sr * target_duration)\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "\n",
    "    return y\n",
    "\n",
    "def preprocess_folder(input_folder, output_folder, sr=22050, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.6, normalise='rms'):\n",
    "    \n",
    "    input_folder = Path(input_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    wav_files = list(input_folder.rglob(\"*.wav\"))\n",
    "    \n",
    "    print(f\"Found {len(wav_files)} audio files in {input_folder}\")\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_file, sr=sr)\n",
    "            y = preprocess_audio(\n",
    "                y, sr, \n",
    "                target_duration=target_duration,\n",
    "                apply_preemphasis=apply_preemphasis,\n",
    "                apply_reduction=apply_reduction,\n",
    "                coef=coef,\n",
    "                normalise=normalise\n",
    "            )\n",
    "\n",
    "            # Save processed file to the same structure\n",
    "            relative_path = wav_file.relative_to(input_folder)\n",
    "            out_path = output_folder / relative_path\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            sf.write(out_path, y, sr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {wav_file} → {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocess_folder(\n",
    "    input_folder='datasets/vox1_subset_150/train_data',\n",
    "    output_folder='datasets/vox1_subset_150/train_data_preprocessed',\n",
    "    sr=22050,\n",
    "    target_duration=3.0,\n",
    "    apply_preemphasis=True,\n",
    "    apply_reduction=True,\n",
    "    coef=0.6,\n",
    "    normalise='rms'\n",
    ")\n",
    "\n",
    "\n",
    "preprocess_folder(\n",
    "    input_folder='datasets/vox1_subset_150/val_data',\n",
    "    output_folder='datasets/vox1_subset_150/val_data_preprocessed',\n",
    "    sr=22050,\n",
    "    target_duration=3.0,\n",
    "    apply_preemphasis=True,\n",
    "    apply_reduction=True,\n",
    "    coef=0.6,\n",
    "    normalise='rms'\n",
    ")\n",
    "\n",
    "\n",
    "preprocess_folder(\n",
    "    input_folder='datasets/vox1_subset_test',\n",
    "    output_folder='datasets/vox1_subset_test_data_preprocessed',\n",
    "    sr=22050,\n",
    "    target_duration=3.0,\n",
    "    apply_preemphasis=True,\n",
    "    apply_reduction=True,\n",
    "    coef=0.6,\n",
    "    normalise='rms'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "def preview_preprocessing(file_path, sr=22050, target_duration=6.0, apply_preemphasis=False, apply_reduction=False, coef=0.6, normalise='rms'):\n",
    "\n",
    "    y, _ = librosa.load(file_path, sr=sr)\n",
    "    y_orig = y.copy()\n",
    "\n",
    "    # Trim silence\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    if apply_reduction:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    if apply_preemphasis:\n",
    "        y = librosa.effects.preemphasis(y, coef=coef)\n",
    "\n",
    "    # Choose normalisation method\n",
    "    if normalise == 'rms':\n",
    "        rms = np.sqrt(np.mean(y**2))\n",
    "        y = y / (rms + 1e-6)\n",
    "    elif normalise == 'peak':\n",
    "        y = y / (np.max(np.abs(y)) + 1e-6)\n",
    "\n",
    "    # Fix length\n",
    "    target_len = int(sr * target_duration)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "\n",
    "    print(\"Original:\")\n",
    "    ipd.display(ipd.Audio(y_orig, rate=sr))\n",
    "\n",
    "    print(\"Preprocessed:\")\n",
    "    ipd.display(ipd.Audio(y, rate=sr))\n",
    "\n",
    "    # Plot waveform comparison\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(y_orig, label=\"Original\", alpha=0.6)\n",
    "    plt.plot(y, label=\"Preprocessed\", alpha=0.6)\n",
    "    plt.title(\"Waveform Comparison\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "preview_preprocessing('datasets/vox1_subset_150/train_data/id10115/4cQ5Zgr-BK0/00001.wav', apply_preemphasis=True, apply_reduction=True, normalise='rms')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to extact MFCC, Chroma, Tonnetz, Mel spectrogram, spectral contrast, all can be done using librosa\n",
    "\n",
    "#first split the data itno 5 parts for each model, as each model will have one fearture\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def split_train_data_by_feature(train_data_path, output_base_path):\n",
    "    speaker_dirs = [d for d in os.listdir(train_data_path)\n",
    "                    if os.path.isdir(os.path.join(train_data_path, d))]\n",
    "\n",
    "    #  consistent randomisation with ealier seed number for reducablity \n",
    "    random.seed(42)\n",
    "    random.shuffle(speaker_dirs)\n",
    "\n",
    "    feature_folders = [\n",
    "        'denseNN_mfcc',\n",
    "        'denseNN_chroma',\n",
    "        'denseNN_tonnetz',\n",
    "        'denseNN_spectral_contrast',\n",
    "        'cnn_mel_spectrogram'\n",
    "    ]\n",
    "\n",
    "    part_size = len(speaker_dirs) // len(feature_folders)\n",
    "\n",
    "    for i, feature_folder in enumerate(feature_folders):\n",
    "        part_dir = os.path.join(output_base_path, feature_folder)\n",
    "        os.makedirs(part_dir, exist_ok=True)\n",
    "\n",
    "        start = i * part_size\n",
    "        end = (i + 1) * part_size if i < len(feature_folders) - 1 else len(speaker_dirs)\n",
    "        part_speakers = speaker_dirs[start:end]\n",
    "\n",
    "        for speaker in part_speakers:\n",
    "            src = os.path.join(train_data_path, speaker)\n",
    "            dst = os.path.join(part_dir, speaker)\n",
    "            shutil.copytree(src, dst)\n",
    "\n",
    "        print(f\"{feature_folder}: {len(part_speakers)} speakers copied.\")\n",
    "\n",
    "\n",
    "train_data_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150', 'train_data')\n",
    "output_base_path = os.path.join(os.getcwd(), 'datasets', 'vox1_subset_150/vox1_split_by_feature')\n",
    "\n",
    "\n",
    "#ACTUAL FEATURE EXTRACTING\n",
    "\n",
    "def extract_feature(y, sr, feature_type):\n",
    "    if feature_type == \"mfcc\":\n",
    "        return librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    elif feature_type == \"chroma\":\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        return librosa.feature.chroma_stft(S=stft, sr=sr)\n",
    "    elif feature_type == \"tonnetz\":\n",
    "        y_harmonic = librosa.effects.harmonic(y)\n",
    "        return librosa.feature.tonnetz(y=y_harmonic, sr=sr)\n",
    "    elif feature_type == \"spectral_contrast\":\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        return librosa.feature.spectral_contrast(S=stft, sr=sr)\n",
    "    elif feature_type == \"mel_spectrogram\":\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        return librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature type: {feature_type}\")\n",
    "\n",
    "def extract_features_from_folder(input_dir, feature_type, output_base=\"datasets/vox1_subset_150/features\", sr=22050):\n",
    "    input_dir = Path(input_dir)\n",
    "    input_name = input_dir.name\n",
    "    output_dir = Path(output_base) / input_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for wav_path in input_dir.rglob(\"*.wav\"):\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_path, sr=sr)\n",
    "            feat = extract_feature(y, sr, feature_type)\n",
    "\n",
    "            # Build output path, preserving structure\n",
    "            relative = wav_path.relative_to(input_dir).with_suffix(\".npy\")\n",
    "            out_path = output_dir / relative\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            np.save(out_path, feat)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {wav_path} → {e}\")\n",
    "    \n",
    "    print(f\"Done extracting '{feature_type}' features from {input_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "extract_features_from_folder(\"datasets/vox1_subset_150/vox1_split_by_feature/denseNN_mfcc\", \"mfcc\")\n",
    "extract_features_from_folder(\"datasets/vox1_subset_150/vox1_split_by_feature/denseNN_tonnetz\", \"tonnetz\")\n",
    "extract_features_from_folder(\"datasets/vox1_subset_150/vox1_split_by_feature/denseNN_chroma\", \"chroma\")\n",
    "extract_features_from_folder(\"datasets/vox1_subset_150/vox1_split_by_feature/denseNN_spectral_contrast\", \"spectral_contrast\")\n",
    "extract_features_from_folder(\"datasets/vox1_subset_150/vox1_split_by_feature/cnn_mel_spectrogram\", \"mel_spectrogram\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing some features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#CNN model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
